{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10.4: Prompt Optimization and A/B Testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we explored basic and advanced Prompt Engineering techniques. However, designing a perfect prompt from the start is rare. **Prompt optimization** is an iterative and continuous process. This lesson will delve into the prompt optimization process, tools and techniques for testing and comparing prompt variations, an introduction to A/B testing for prompts in production, and ethical and safety considerations in Prompt Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Iterative Prompt Optimization Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt optimization is a continuous cycle of experimentation, learning, and improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  **Define Goals and Evaluation Criteria:**\n",
    "    * What do you want the LLM to do? (e.g., summarize, classify, answer questions).\n",
    "    * How will you know the LLM is performing well? (e.g., accuracy, relevance, fluency, lack of hallucinations).\n",
    "2.  **Design Initial Prompt:**\n",
    "    * Start with a simple, clear, and specific prompt.\n",
    "    * Apply basic Prompt Engineering principles (context, formatting, etc.).\n",
    "3.  **Create Test Dataset:**\n",
    "    * Assemble a set of input examples representative of real-world use cases.\n",
    "    * Include both normal cases and edge cases or difficult scenarios.\n",
    "    * If possible, include \"ground truth\" answers for automated evaluation.\n",
    "4.  **Run Experiments and Collect Results:**\n",
    "    * Run your prompt on the test dataset.\n",
    "    * Collect the LLM's responses.\n",
    "    * Record metrics like latency and token cost.\n",
    "5.  **Evaluate Results:**\n",
    "    * **Manual:** Read and score responses based on predefined criteria.\n",
    "    * **Automated:** Use automated metrics (ROUGE, BLEU, BERTScore) or LLM-as-a-Judge.\n",
    "    * Identify common errors and cases where the prompt performs poorly.\n",
    "6.  **Analyze and Refine Prompt:**\n",
    "    * Based on evaluation results, identify the root causes of issues.\n",
    "    * Adjust the prompt:\n",
    "        * Add/remove context.\n",
    "        * Change wording, phrasing.\n",
    "        * Add few-shot examples.\n",
    "        * Modify formatting instructions.\n",
    "        * Add constraints or rules.\n",
    "        * Adjust `temperature` or other LLM parameters.\n",
    "7.  **Iterate:** Go back to step 4 and repeat the process until desired performance is achieved.\n",
    "\n",
    "![An iterative optimization loop diagram](https://placehold.co/600x400/ccddff/ffffff?text=Iterative+Optimization+Loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tools and Techniques for Testing and Comparing Prompt Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support the iterative optimization process, we need effective tools and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **LangChain Runnables and LCEL:**\n",
    "    * LangChain Expression Language (LCEL) allows you to build runnable chains in a modular and flexible way.\n",
    "    * You can easily swap out prompts, LLMs, or other components in your chain and test them.\n",
    "* **LangSmith:**\n",
    "    * As learned in Lesson 9.4, LangSmith is an indispensable tool.\n",
    "    * **Tracing:** Logs every LLM call, prompt, and response, helping you visualize and debug each step.\n",
    "    * **Experimentation:** Allows you to create \"experiments\" to compare the performance of different prompts or models on the same dataset. You can run multiple prompt variations in parallel and compare metrics (accuracy, latency, cost).\n",
    "    * **Evaluation:** Integrates automated evaluators (LLM-as-a-Judge) and manual annotation capabilities to quantify output quality.\n",
    "* **Evaluation Datasets:**\n",
    "    * Creating and managing high-quality test datasets is crucial.\n",
    "    * These datasets should include diverse input cases and, if possible, \"ground truth\" answers for automated evaluation.\n",
    "* **Unit Testing and Regression Testing:**\n",
    "    * Write automated tests for critical prompts to ensure that changes do not degrade performance on known cases.\n",
    "    * Run these tests after every prompt or model change.\n",
    "\n",
    "![Person using a dashboard to compare different prompt versions](https://placehold.co/600x400/ddeeffcc/ffffff?text=Prompt+Comparison+Dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introduction to A/B Testing for Prompts in Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have optimized your prompts in a development/testing environment, the next step to validate their effectiveness is A/B testing in a production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** Deploy two or more versions of the application (where the main difference is the prompt) to different groups of real users.\n",
    "* **Purpose:** Measure the impact of prompt changes on business metrics and user experience under real-world conditions.\n",
    "* **Steps:**\n",
    "    1.  **Define Variations:** Version A (current prompt) and Version B (newly optimized prompt).\n",
    "    2.  **Split User Groups:** Randomly assign users to groups A and B.\n",
    "    3.  **Collect Data:** Monitor user interaction metrics (e.g., satisfaction rate, conversion rate, session duration, error rate) for both groups.\n",
    "    4.  **Statistical Analysis:** Compare metrics between groups to determine if the new prompt significantly improved performance.\n",
    "    5.  **Decision Making:** Based on the results, decide whether to roll out the new prompt to all users.\n",
    "* **Benefits:** Provides empirical evidence of prompt effectiveness, minimizes risk when rolling out major changes.\n",
    "* **Challenges:** Requires A/B testing infrastructure, needs sufficient traffic for statistically significant results, can be time-consuming.\n",
    "\n",
    "![An A/B testing setup with two user groups](https://placehold.co/600x400/aaccaa/ffffff?text=A/B+Testing+Prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ethical and Safety Considerations in Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Engineering is not just about performance but also about ensuring LLM applications are used responsibly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Bias Mitigation:**\n",
    "    * **Identification:** LLMs can reflect biases from training data. Prompts and responses need to be checked for signs of bias (e.g., gender, racial, religious discrimination).\n",
    "    * **Techniques:**\n",
    "        * **Neutral Prompting:** Use neutral language, avoid words that might suggest bias.\n",
    "        * **Ethical Guidelines:** Add instructions to the prompt requiring the LLM to provide fair, objective responses.\n",
    "        * **Robustness Testing:** Test responses with input variations (e.g., changing names, genders) to see if the LLM responds unfairly.\n",
    "* **Harmful Content Prevention:**\n",
    "    * **Identification:** LLMs can generate harmful content (hate speech, violence, sexual content, self-harm, misinformation).\n",
    "    * **Techniques:**\n",
    "        * **Moderation Systems:** Use content moderation models or APIs to filter inputs and outputs.\n",
    "        * **Safety Instructions in Prompt:** Instruct the LLM not to generate harmful, unethical content.\n",
    "        * **Scope Limitation:** Restrict the LLM to respond only within a safe topic range.\n",
    "* **Privacy and Sensitive Data Protection:**\n",
    "    * **Identification:** LLMs can inadvertently reveal personal or sensitive information if they were trained on such data or if the prompt contains such information.\n",
    "    * **Techniques:**\n",
    "        * **Anonymization/Redaction:** Remove or mask sensitive information from input data.\n",
    "        * **Privacy Instructions:** Instruct the LLM not to store or disclose personal information.\n",
    "        * **Secure Architecture:** Ensure data is processed and stored securely.\n",
    "* **Transparency and Explainability:**\n",
    "    * **Purpose:** Helps users understand why the LLM produced a particular response, especially in critical applications.\n",
    "    * **Techniques:**\n",
    "        * **Chain-of-Thought:** Encourage the LLM to articulate its reasoning steps.\n",
    "        * **Source Citation:** Ask the LLM to cite the information sources it used (especially in RAG).\n",
    "\n",
    "![Ethical guidelines for AI](https://placehold.co/600x400/ccffcc/ffffff?text=AI+Ethics+Guidelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical: Refining an Existing Prompt to Improve Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a simple prompt and try to refine it to improve its accuracy and output format for an information extraction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** Extract product name, quantity, and price from an order description sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have the `langchain-openai` library installed.\n",
    "* Set the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) # Use low temperature for consistent results\n",
    "\n",
    "# Test data\n",
    "order_descriptions = [\n",
    "    \"Tôi muốn mua 2 cái áo thun màu xanh với giá 150.000 VNĐ mỗi cái.\", # I want to buy 2 blue t-shirts for 150,000 VND each.\n",
    "    \"Đơn hàng gồm 1 chiếc quần jean giá 300.000 VNĐ.\", # The order includes 1 pair of jeans for 300,000 VND.\n",
    "    \"Vui lòng thêm 3 đôi tất đen, giá 50.000 VNĐ/đôi và 1 áo khoác da giá 1.200.000 VNĐ.\", # Please add 3 pairs of black socks, priced at 50,000 VND/pair, and 1 leather jacket for 1,200,000 VND.\n",
    "    \"Mua 5 quyển sách 'Lập trình Python' với giá 250.000 VNĐ.\" # Buy 5 books 'Python Programming' for 250,000 VND. # Can be confusing about quantity\n",
    "]\n",
    "\n",
    "# --- 1. Initial Prompt (Baseline) ---\n",
    "print(\"--- 1. Initial Prompt (Baseline) ---\")\n",
    "initial_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an information extraction assistant. Extract product name, quantity, and price from the order description.\"),\n",
    "    (\"user\", \"Order description: {description}\")\n",
    "])\n",
    "initial_chain = initial_prompt_template | llm | StrOutputParser()\n",
    "\n",
    "for i, desc in enumerate(order_descriptions):\n",
    "    print(f\"\\n--- Order {i+1} ---\")\n",
    "    print(f\"Description: {desc}\")\n",
    "    response = initial_chain.invoke({\"description\": desc})\n",
    "    print(f\"Initial response:\\n{response}\")\n",
    "\n",
    "# --- 2. Refined Prompt (Add JSON format and examples) ---\n",
    "print(\"\\n--- 2. Refined Prompt (Add JSON format and examples) ---\")\n",
    "refined_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a professional information extraction assistant.\n",
    "    From the order description, extract PRODUCT NAME, QUANTITY, and PRICE.\n",
    "    If there are multiple products, list all of them.\n",
    "    Respond in JSON format. Here is an example:\n",
    "\n",
    "    Description: Tôi muốn mua 2 cái áo thun màu xanh với giá 150.000 VNĐ mỗi cái.\n",
    "    JSON:\n",
    "    ```json\n",
    "    [\n",
    "      {\n",
    "        \"ten_san_pham\": \"áo thun màu xanh\",\n",
    "        \"so_luong\": 2,\n",
    "        \"gia\": 150000,\n",
    "        \"don_vi_gia\": \"VNĐ\"\n",
    "      }\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    Description: Vui lòng thêm 3 đôi tất đen, giá 50.000 VNĐ/đôi và 1 áo khoác da giá 1.200.000 VNĐ.\n",
    "    JSON:\n",
    "    ```json\n",
    "    [\n",
    "      {\n",
    "        \"ten_san_pham\": \"tất đen\",\n",
    "        \"so_luong\": 3,\n",
    "        \"gia\": 50000,\n",
    "        \"don_vi_gia\": \"VNĐ\"\n",
    "      },\n",
    "      {\n",
    "        \"ten_san_pham\": \"áo khoác da\",\n",
    "        \"so_luong\": 1,\n",
    "        \"gia\": 1200000,\n",
    "        \"don_vi_gia\": \"VNĐ\"\n",
    "      }\n",
    "    ]\n",
    "    ```\n",
    "    \"\"\"),\n",
    "    (\"user\", \"Order description: {description}\\nJSON:\") # Ask LLM to start with JSON\n",
    "])\n",
    "refined_chain = refined_prompt_template | llm | StrOutputParser()\n",
    "\n",
    "for i, desc in enumerate(order_descriptions):\n",
    "    print(f\"\\n--- Order {i+1} ---\")\n",
    "    print(f\"Description: {desc}\")\n",
    "    response_raw = refined_chain.invoke({\"description\": desc})\n",
    "    print(f\"Refined response (raw):\\n{response_raw}\")\n",
    "    \n",
    "    # Try to parse JSON to check format\n",
    "    try:\n",
    "        # Extract JSON part from response if it's wrapped by ```json\n",
    "        json_str = response_raw.split(\"```json\")[1].split(\"```\")[0].strip() if \"```json\" in response_raw else response_raw.strip()\n",
    "        parsed_json = json.loads(json_str)\n",
    "        print(f\"Refined response (parsed JSON):\\n{json.dumps(parsed_json, indent=2, ensure_ascii=False)}\")\n",
    "    except (json.JSONDecodeError, IndexError) as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        print(\"Response is not in valid JSON format.\")\n",
    "\n",
    "print(\"\\n--- End of Practical ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
