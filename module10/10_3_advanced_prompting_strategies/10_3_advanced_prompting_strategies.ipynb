{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10.3: Other Advanced Prompting Strategies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lesson 10.2, we explored **Chain-of-Thought (CoT) Prompting**, a powerful technique that helps LLMs reason step-by-step. However, the field of Prompt Engineering is constantly evolving, and there are many other advanced strategies that can significantly improve LLM performance on complex tasks. This lesson will introduce strategies such as **Tree-of-Thought (ToT)**, **Self-Consistency**, **Retrieval-Augmented Prompting**, and **Prompt Chaining**, along with a practical exercise to compare their effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tree-of-Thought (ToT): Exploring Multiple Thought Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree-of-Thought (ToT)** is a Prompting strategy that extends CoT, allowing the LLM to explore and evaluate multiple possible thought paths, rather than just a single linear chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** Instead of generating a single thought chain, ToT encourages the LLM to generate a \"tree\" of thoughts. Each \"node\" in the tree is an intermediate thought state, and the LLM can branch out to explore different lines of reasoning.\n",
    "* **Process:**\n",
    "    1.  **Problem Decomposition:** The LLM breaks down the problem into smaller steps.\n",
    "    2.  **Generate Multiple Options:** At each step, the LLM generates multiple possible next thoughts or actions.\n",
    "    3.  **Evaluation:** The LLM (or another model/evaluation function) evaluates the \"quality\" or \"promise\" of each thought path.\n",
    "    4.  **Search:** Uses search algorithms (e.g., Breadth-First Search - BFS, Depth-First Search - DFS, or Beam Search) to explore the thought tree and select the best path leading to a solution.\n",
    "* **Pros:**\n",
    "    * Significantly improves the ability to solve complex problems, especially those with multiple steps and requiring exploration of the solution space.\n",
    "    * Reduces the risk of getting stuck in a flawed thought chain.\n",
    "* **Cons:**\n",
    "    * More complex to implement and manage.\n",
    "    * More computationally expensive and costly (more LLM calls).\n",
    "* **Applications:** Solving advanced mathematical problems, complex planning, multi-step logical reasoning.\n",
    "\n",
    "![A tree diagram with multiple branches, each representing a thought path](https://placehold.co/600x400/aaddcc/ffffff?text=Tree-of-Thought+Diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self-Consistency: Generating Multiple Answers and Choosing the Most Popular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-Consistency** is a simple yet effective technique to improve the reliability of LLM responses by leveraging their stochastic nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** Instead of just asking the LLM to generate a single answer, you ask it to generate **multiple independent answers** to the same question (typically with `temperature > 0`). Then, you aggregate these answers and choose the **most popular (majority vote)** or the **best** answer (e.g., by another LLM).\n",
    "* **Process:**\n",
    "    1.  **Generate Multiple Thought Chains/Answers:** Run the same prompt (often a CoT prompt) multiple times to have the LLM generate different thought chains and answers.\n",
    "    2.  **Extract Final Answer:** From each thought chain, extract the final answer.\n",
    "    3.  **Aggregate/Select:** Use a voting mechanism (e.g., counting the frequency of answers) or a judge LLM to select the final answer.\n",
    "* **Pros:**\n",
    "    * Improves accuracy and reliability, especially for problems with a single correct answer.\n",
    "    * Simpler to implement than ToT.\n",
    "* **Cons:**\n",
    "    * Increases computational cost and API cost (more LLM calls).\n",
    "* **Applications:** Mathematical problems, logical puzzles, high-accuracy Q&A.\n",
    "\n",
    "![Multiple LLM responses converging to a single, consistent answer](https://placehold.co/600x400/ccddeeff/ffffff?text=Self-Consistency+Flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieval-Augmented Prompting: Combining RAG with Advanced Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've learned about **Retrieval-Augmented Generation (RAG)** as a way to provide external context to LLMs. **Retrieval-Augmented Prompting** is the act of combining RAG with advanced Prompting techniques (like CoT, ToT) to optimize the use of retrieved information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** Instead of simply inserting retrieved context into the prompt, you use Prompting techniques to guide the LLM on how to interact with that context more effectively.\n",
    "* **Techniques:**\n",
    "    * **CoT with Context:** Ask the LLM to think step-by-step about how to use the context to answer the question. Example: \"Based on the following context, think step-by-step to answer the question. Context: [Text]. Question: [Question]\"\n",
    "    * **Context Analysis:** Ask the LLM to first summarize or extract key points from the context before answering the question.\n",
    "    * **Factual Consistency Check:** After the LLM generates a response, ask it to cross-reference that response with the original context to ensure factual consistency.\n",
    "* **Pros:**\n",
    "    * Reduces hallucinations by forcing the LLM to stick to the provided information.\n",
    "    * Improves accuracy and relevance of responses.\n",
    "    * Enhances explainability by showing how the LLM used the context.\n",
    "* **Cons:**\n",
    "    * Can increase prompt complexity and the number of LLM calls.\n",
    "* **Applications:** Q&A systems, knowledge chatbots, document summarization.\n",
    "\n",
    "![A RAG pipeline with CoT steps integrated](https://placehold.co/600x400/ddeeff/ffffff?text=RAG+with+CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Chaining: Building Complex Prompt Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt Chaining** (also known as Chain of Prompts) is an organizational technique where a large task is broken down into multiple smaller tasks, and each smaller task is handled by a separate prompt. The output of one prompt becomes the input for the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** Instead of trying to solve the entire problem in a single prompt, you create a sequence of prompts, each performing a specific logical step.\n",
    "* **Process:**\n",
    "    1.  **Task Decomposition:** Break down a complex task into sequential steps.\n",
    "    2.  **Design Prompt for Each Step:** Create a separate prompt for each step, instructing the LLM to perform that specific task.\n",
    "    3.  **Connect Output/Input:** The LLM's output from one prompt is passed as input to the next prompt.\n",
    "* **Pros:**\n",
    "    * **Improved Accuracy:** The LLM can focus on a smaller task at a time, reducing the chance of errors.\n",
    "    * **Debuggability:** Easier to pinpoint which step caused an error.\n",
    "    * **Modularity:** Individual prompts can be reused in other chains.\n",
    "    * **Flow Control:** Allows for tighter control over the processing flow.\n",
    "* **Cons:**\n",
    "    * Increased latency (multiple sequential LLM calls).\n",
    "    * Increased cost (each LLM call costs money).\n",
    "* **Applications:** Multi-step text processing (e.g., extraction -> classification -> summarization), Agents (each step in a LangGraph graph can be a prompt chain).\n",
    "\n",
    "![A sequence of prompts, each feeding into the next](https://placehold.co/600x400/aaccaa/ffffff?text=Prompt+Chaining+Flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical: Comparing the Effectiveness of Different Prompting Strategies on the Same Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a problem that requires logical/mathematical reasoning and compare the effectiveness of:\n",
    "1.  Basic Prompt (no CoT)\n",
    "2.  Zero-shot CoT\n",
    "3.  Self-Consistency (simple)\n",
    "4.  Prompt Chaining (to decompose the problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have the `langchain-openai` library installed.\n",
    "* Set the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from collections import Counter # To count frequencies for Self-Consistency\n",
    "import json # For JSON parsing in Prompt Chaining\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7) # Use temperature > 0 for Self-Consistency\n",
    "\n",
    "# Example problem\n",
    "problem = \"If today is Tuesday, what day of the week will it be in 75 days?\"\n",
    "\n",
    "# --- 1. Basic Prompt (Baseline) ---\n",
    "print(\"--- Strategy: Basic Prompt ---\")\n",
    "basic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an intelligent assistant. Answer the following question.\"),\n",
    "    (\"user\", problem)\n",
    "])\n",
    "basic_chain = basic_prompt | llm | StrOutputParser()\n",
    "response_basic = basic_chain.invoke({})\n",
    "print(f\"Basic Response: {response_basic}\\n\")\n",
    "\n",
    "# --- 2. Zero-shot CoT ---\n",
    "print(\"--- Strategy: Zero-shot CoT ---\")\n",
    "zero_shot_cot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an intelligent assistant. Think step by step before providing the final answer.\"),\n",
    "    (\"user\", problem)\n",
    "])\n",
    "zero_shot_cot_chain = zero_shot_cot_prompt | llm | StrOutputParser()\n",
    "response_zero_shot_cot = zero_shot_cot_chain.invoke({})\n",
    "print(f\"Zero-shot CoT Response: {response_zero_shot_cot}\\n\")\n",
    "\n",
    "# --- 3. Self-Consistency (Simple) ---\n",
    "# Run Zero-shot CoT multiple times and pick the most frequent final answer\n",
    "print(\"--- Strategy: Self-Consistency ---\")\n",
    "num_samples = 5 # Number of runs to generate different responses\n",
    "final_answers = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    print(f\"  Running Self-Consistency iteration {i+1}...\")\n",
    "    # Get CoT response and try to extract the final answer\n",
    "    cot_response = zero_shot_cot_chain.invoke({})\n",
    "    # Assume the final answer is after \"Answer:\" or the last non-empty line\n",
    "    answer_lines = cot_response.split('\\n')\n",
    "    extracted_answer = \"\"\n",
    "    for line in reversed(answer_lines):\n",
    "        if \"Answer:\" in line:\n",
    "            extracted_answer = line.split(\"Answer:\", 1)[1].strip()\n",
    "            break\n",
    "        elif line.strip(): # If no \"Answer:\", take the last non-empty line\n",
    "            extracted_answer = line.strip()\n",
    "            break\n",
    "    \n",
    "    if extracted_answer:\n",
    "        final_answers.append(extracted_answer)\n",
    "    print(f\"    Extracted response: {extracted_answer}\")\n",
    "\n",
    "if final_answers:\n",
    "    most_common_answer = Counter(final_answers).most_common(1)[0][0]\n",
    "    print(f\"Self-Consistency Response (most common from {num_samples} samples): {most_common_answer}\\n\")\n",
    "else:\n",
    "    print(\"No answers extracted for Self-Consistency.\\n\")\n",
    "\n",
    "# --- 4. Prompt Chaining ---\n",
    "print(\"--- Strategy: Prompt Chaining ---\")\n",
    "\n",
    "# Step 1: Extract necessary information\n",
    "extract_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a question analysis assistant. From the following question, extract: 1. The current day mentioned. 2. The number of days in the future. Respond in JSON.\"),\n",
    "    (\"user\", problem)\n",
    "])\n",
    "extract_chain = extract_prompt | llm | StrOutputParser()\n",
    "extracted_info_raw = extract_chain.invoke({})\n",
    "\n",
    "try:\n",
    "    extracted_info = json.loads(extracted_info_raw)\n",
    "    current_day = extracted_info.get(\"The current day mentioned\") # Adjust key to match LLM output if different\n",
    "    days_ahead = extracted_info.get(\"The number of days in the future\") # Adjust key to match LLM output if different\n",
    "    print(f\"  Step 1 (Extraction): Current Day: {current_day}, Days Ahead: {days_ahead}\")\n",
    "\n",
    "    # Step 2: Calculate the day of the week\n",
    "    if current_day and days_ahead is not None:\n",
    "        calculate_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a day calculation assistant. If today is {current_day}, what day of the week will it be in {days_ahead} days? Only respond with the day name.\"),\n",
    "            (\"user\", \"Calculate and provide the result.\")\n",
    "        ])\n",
    "        calculate_chain = calculate_prompt | llm | StrOutputParser()\n",
    "        final_response_chained = calculate_chain.invoke({\"current_day\": current_day, \"days_ahead\": days_ahead})\n",
    "        print(f\"  Step 2 (Calculation): {final_response_chained}\")\n",
    "        print(f\"Prompt Chaining Response: {final_response_chained}\\n\")\n",
    "    else:\n",
    "        print(f\"  Error extracting information: {extracted_info_raw}\")\n",
    "        print(\"Prompt Chaining Response: Could not complete due to extraction error.\\n\")\n",
    "\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"  Error parsing JSON from extraction step: {extracted_info_raw}\")\n",
    "    print(\"Prompt Chaining Response: Could not complete due to format error.\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error during Prompt Chaining: {e}\")\n",
    "    print(\"Prompt Chaining Response: An error occurred in the chain.\\n\")\n",
    "\n",
    "# --- 5. (Conceptual) Tree-of-Thought (ToT) ---\n",
    "print(\"--- Strategy: Tree-of-Thought (ToT) (Conceptual) ---\")\n",
    "print(\"ToT is more complex to implement directly in a simple example.\")\n",
    "print(\"It would involve the LLM generating multiple 'thoughts' or 'actions' at each step,\")\n",
    "print(\"then an evaluation function scoring these thoughts,\")\n",
    "print(\"and a search algorithm traversing the tree to find the best path.\")\n",
    "print(\"Example: The LLM might try different calculations or different ways to analyze the problem,\")\n",
    "print(\"then self-evaluate or be evaluated by another LLM to choose the most correct path.\\n\")\n",
    "\n",
    "# --- 6. (Conceptual) Retrieval-Augmented Prompting ---\n",
    "print(\"--- Strategy: Retrieval-Augmented Prompting (Conceptual) ---\")\n",
    "print(\"This strategy combines RAG with advanced Prompting techniques.\")\n",
    "print(\"Example: To solve the problem above, if we had a database of 'Calendars' or 'How to calculate days of the week,'\")\n",
    "print(\"we would:\")\n",
    "print(\"1. Query the database to retrieve relevant information (e.g., 'number of days in a week').\")\n",
    "print(\"2. Inject this information into the prompt along with the question and ask the LLM to think step-by-step (CoT) based on the retrieved information.\")\n",
    "print(\"This helps the LLM have more reliable data for reasoning, reducing 'hallucinations'.\\n\")\n",
    "\n",
    "print(\"--- End of Practical ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
