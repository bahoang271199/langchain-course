{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10.2: Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we explored the basic and advanced principles of Prompt Engineering. One of the most groundbreaking techniques to emerge in recent years is **Chain-of-Thought (CoT) Prompting**. CoT encourages Large Language Models (LLMs) to articulate their intermediate reasoning steps, leading to improved performance on complex problem-solving tasks, especially those requiring multi-step reasoning.\n",
    "\n",
    "This lesson will delve into the concept of CoT, its variations, applications, and practical implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chain-of-Thought Concept: Encouraging LLMs to Think Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What is CoT?\n",
    "\n",
    "**Chain-of-Thought (CoT) Prompting** is a Prompt Engineering technique where you instruct an LLM to think step-by-step, articulating its reasoning process, before providing a final answer. Instead of simply asking the LLM for a direct response, you ask it to \"show your work\" or \"explain your reasoning step-by-step.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Why is CoT Effective?\n",
    "\n",
    "* **Problem Decomposition:** CoT helps the LLM break down a complex problem into smaller, more manageable steps. This is particularly useful for problems requiring multiple reasoning steps.\n",
    "* **Enhanced Reasoning Capabilities:** By forcing the LLM to generate intermediate steps, it can self-correct and refine its reasoning, similar to how humans solve a problem.\n",
    "* **Improved Accuracy:** For complex tasks, CoT has been shown to significantly improve the accuracy of LLMs.\n",
    "* **Explainability:** The LLM's thought process becomes more transparent, helping you understand why it arrived at a particular answer and making debugging easier.\n",
    "\n",
    "### 1.3. Basic CoT Application\n",
    "\n",
    "The simplest way to apply CoT is to add a phrase like \"Let's think step by step.\" or \"Explain your reasoning step by step.\" to your prompt.\n",
    "\n",
    "![A thought process diagram with steps](https://placehold.co/600x400/ccddeeff/ffffff?text=Chain-of-Thought+Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CoT Variations: Zero-shot CoT, Few-shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main variations of CoT Prompting, differing in whether you provide examples of the thought process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Zero-shot CoT\n",
    "\n",
    "* **Concept:** You simply add a trigger phrase to the prompt without providing any examples of a thought chain. The LLM will automatically generate its own reasoning steps.\n",
    "* **Example:**\n",
    "    ```\n",
    "    \"Question: A person has 5 apples. He buys 3 more and eats 2. How many apples does he have left?\n",
    "    Let's think step by step.\"\n",
    "    ```\n",
    "* **Pros:** Easy to implement, requires no example data.\n",
    "* **Cons:** Effectiveness can vary depending on the model and task complexity. Some smaller models might not automatically generate high-quality thought chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Few-shot CoT\n",
    "\n",
    "* **Concept:** You provide a few examples of (question, thought chain, answer) pairs within the prompt. The LLM learns to generate similar thought chains based on these examples.\n",
    "* **Example:**\n",
    "    ```\n",
    "    \"Question: A person has 2 oranges. He buys 4 more and gives away 1. How many oranges does he have left?\n",
    "    Thought:\n",
    "    1. Initially, the person had 2 oranges.\n",
    "    2. He bought 4 more, so the total is 2 + 4 = 6 oranges.\n",
    "    3. He gave away 1, so the remaining oranges are 6 - 1 = 5 oranges.\n",
    "    Answer: 5\n",
    "\n",
    "    Question: A store has 100 cakes. In the morning, 35 are sold. In the afternoon, 25 are sold. How many cakes are left?\n",
    "    Thought:\"\n",
    "    ```\n",
    "* **Pros:** Often yields better performance than Zero-shot CoT, especially for complex tasks or when you want to shape the LLM's thinking style.\n",
    "* **Cons:** Requires creating high-quality examples, which can be time-consuming and labor-intensive.\n",
    "\n",
    "![A prompt with multiple examples of questions and step-by-step answers](https://placehold.co/600x400/ddeeffcc/ffffff?text=Few-shot+CoT+Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applications of CoT in Complex Problem Solving, Mathematics, Logical Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoT Prompting has proven remarkably effective in various domains:\n",
    "\n",
    "* **Mathematical and Arithmetic Problems:**\n",
    "    * Solving word problems by breaking down each calculation.\n",
    "    * Performing complex computations.\n",
    "* **Logical Reasoning and Common Sense:**\n",
    "    * Solving logic puzzles, reasoning about real-world scenarios.\n",
    "    * Example: \"If A is greater than B, and B is greater than C, is A greater than C?\"\n",
    "* **Planning and Action Sequencing:**\n",
    "    * Instructing Agents to perform a series of steps to achieve a goal.\n",
    "    * Example: \"To book a flight, what steps do you need to take?\"\n",
    "* **Text Summarization and Analysis:**\n",
    "    * Analyzing text into smaller parts before summarizing or extracting information.\n",
    "* **Code Debugging:**\n",
    "    * Asking the LLM to think about each line of code to find errors.\n",
    "\n",
    "![A complex math problem being solved step-by-step](https://placehold.co/600x400/ffccaa/ffffff?text=CoT+for+Math+Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical: Applying CoT to Reasoning Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will practice applying Zero-shot CoT and Few-shot CoT to reasoning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have the `langchain-openai` library installed.\n",
    "* Set the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# --- 1. Zero-shot CoT ---\n",
    "print(\"--- Practical: Zero-shot CoT ---\")\n",
    "\n",
    "zero_shot_cot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a problem-solving assistant. Think step by step before providing the final answer.\"),\n",
    "    (\"user\", \"A store has 100 cakes. In the morning, 35 are sold. In the afternoon, 25 are sold. How many cakes are left?\")\n",
    "])\n",
    "\n",
    "zero_shot_cot_chain = zero_shot_cot_prompt | llm | StrOutputParser()\n",
    "response_zero_shot = zero_shot_cot_chain.invoke({})\n",
    "print(f\"Zero-shot CoT Response:\\n{response_zero_shot}\\n\")\n",
    "\n",
    "# --- 2. Few-shot CoT ---\n",
    "print(\"--- Practical: Few-shot CoT ---\")\n",
    "\n",
    "few_shot_cot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a problem-solving assistant. Think step by step as in the following examples before providing the final answer.\"),\n",
    "    (\"user\", \"\"\"\n",
    "Question: A person has 2 oranges. He buys 4 more and gives away 1. How many oranges does he have left?\n",
    "Thought:\n",
    "1. Initially, the person had 2 oranges.\n",
    "2. He bought 4 more, so the total is 2 + 4 = 6 oranges.\n",
    "3. He gave away 1, so the remaining oranges are 6 - 1 = 5 oranges.\n",
    "Answer: 5\n",
    "\n",
    "Question: A store has 100 cakes. In the morning, 35 are sold. In the afternoon, 25 are sold. How many cakes are left?\n",
    "Thought:\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "few_shot_cot_chain = few_shot_cot_prompt | llm | StrOutputParser()\n",
    "response_few_shot = few_shot_cot_chain.invoke({})\n",
    "print(f\"Few-shot CoT Response:\\n{response_few_shot}\\n\")\n",
    "\n",
    "# --- 3. Compare with Regular Prompt (no CoT) ---\n",
    "print(\"--- Practical: Regular Prompt (no CoT) ---\")\n",
    "\n",
    "normal_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a problem-solving assistant.\"),\n",
    "    (\"user\", \"A store has 100 cakes. In the morning, 35 are sold. In the afternoon, 25 are sold. How many cakes are left?\")\n",
    "])\n",
    "\n",
    "normal_chain = normal_prompt | llm | StrOutputParser()\n",
    "response_normal = normal_chain.invoke({})\n",
    "print(f\"Regular Response:\\n{response_normal}\\n\")\n",
    "\n",
    "print(\"--- End of Practical ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
