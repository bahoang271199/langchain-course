{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6.1: Customizing Chains and Prompts with LCEL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous Modules, we became familiar with **Chains** as a way to connect LangChain components in a fixed processing flow. However, when building more complex applications, you'll need the ability to customize and combine components more flexibly. This is where **LangChain Expression Language (LCEL)** comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lesson will delve into LCEL, how to build complex and flexible chains, and how to customize the behavior of components within the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Diving Deeper into LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What is LCEL?\n",
    "\n",
    "**LangChain Expression Language (LCEL)** is a declarative language that allows you to easily connect LangChain components (such as Prompts, LLMs, Parsers, Retrievers, and custom functions) into processing chains that can pass data back and forth.\n",
    "\n",
    "* **Declarative:** You describe how components should be connected, rather than writing sequential execution steps.\n",
    "* **Composable:** Components built with LCEL can be easily combined using the pipe (`|`) operator.\n",
    "* **Streamable:** LCEL supports streaming output, allowing applications to respond faster.\n",
    "* **Debuggable:** Easy to trace data flow and intermediate steps.\n",
    "* **Runnable in parallel:** Some parts of the chain can run in parallel to optimize performance.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Why Use LCEL?\n",
    "\n",
    "* **Flexibility:** Build more complex processing flows than traditional `SequentialChain`.\n",
    "* **Modularity:** Each component is an independent `Runnable`, easy to reuse and test.\n",
    "* **Performance:** Supports asynchronous (async) and parallel execution, improving speed.\n",
    "* **Extensibility:** Easily add custom processing steps or integrate external systems.\n",
    "* **Transparency:** The declarative structure makes it easy to read and understand the data flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Complex and Flexible Chains with Runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LCEL, every component is a `Runnable`. A `Runnable` is a standard interface that allows components to be invoked, streamed, or run asynchronously (`ainvoke`, `astream`).\n",
    "\n",
    "You can combine `Runnable`s using the pipe (`|`) operator to form processing chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. `RunnableSequence` (Sequential Connection)\n",
    "\n",
    "* **Concept:** Connects `Runnable`s in sequence, where the output of one is the input of the next. This is the most common way to build a chain.\n",
    "* **Usage:** Use the pipe (`|`) operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt thư viện nếu chưa có\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Thiết lập biến môi trường cho khóa API của OpenAI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Định nghĩa Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Kể một câu chuyện ngắn về {topic} trong khoảng 50 từ.\") # Tell a short story about {topic} in about 50 words.\n",
    "\n",
    "# Định nghĩa Parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Xây dựng chuỗi tuần tự với LCEL\n",
    "# Đầu vào của chuỗi là 'topic'\n",
    "# Prompt nhận 'topic' và tạo tin nhắn\n",
    "# LLM nhận tin nhắn và tạo phản hồi\n",
    "# Parser nhận phản hồi và chuyển thành chuỗi\n",
    "story_chain = prompt | llm | parser\n",
    "\n",
    "print(\"--- Ví dụ RunnableSequence ---\") # Example RunnableSequence\n",
    "print(story_chain.invoke({\"topic\": \"một con mèo phiêu lưu\"})) # an adventurous cat\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. `RunnableParallel` (Run in Parallel)\n",
    "\n",
    "* **Concept:** Allows you to run multiple `Runnable`s simultaneously and combine their results into a dictionary. Very useful when you need to prepare multiple inputs for a subsequent step.\n",
    "* **Usage:** Use a dictionary literal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Chuỗi để tạo câu chuyện\n",
    "story_chain = ChatPromptTemplate.from_template(\"Kể một câu chuyện ngắn về {topic} trong khoảng 50 từ.\") | llm | StrOutputParser() # Tell a short story about {topic} in about 50 words.\n",
    "\n",
    "# Chuỗi để tạo một câu thơ\n",
    "poem_chain = ChatPromptTemplate.from_template(\"Viết một bài thơ ngắn về {topic} trong khoảng 4 dòng.\") | llm | StrOutputParser() # Write a short poem about {topic} in about 4 lines.\n",
    "\n",
    "# Kết hợp hai chuỗi trên để chạy song song\n",
    "# Đầu vào của parallel_chain là 'topic'\n",
    "# 'story' sẽ chạy story_chain với 'topic'\n",
    "# 'poem' sẽ chạy poem_chain với 'topic'\n",
    "parallel_chain = RunnableParallel(\n",
    "    story=story_chain,\n",
    "    poem=poem_chain\n",
    ")\n",
    "\n",
    "print(\"--- Ví dụ RunnableParallel ---\") # Example RunnableParallel\n",
    "result = parallel_chain.invoke({\"topic\": \"mùa thu\"}) # autumn\n",
    "print(\"Câu chuyện:\\n\", result[\"story\"]) # Story:\n",
    "print(\"\\nBài thơ:\\n\", result[\"poem\"]) # Poem:\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. `RunnablePassthrough` (Pass Input Through)\n",
    "\n",
    "* **Concept:** A simple `Runnable` that passes its input directly as its output. Very useful when you need to pass a portion of the original input to a later step in the chain, or when you need to combine inputs from multiple sources.\n",
    "* **Usage:** Often used within `RunnableParallel` to retain an input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# Ví dụ: Truyền câu hỏi của người dùng qua đồng thời với kết quả tìm kiếm\n",
    "# Giả sử có một retriever (từ Module 3)\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_core.documents import Document\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "# vector_store = Chroma.from_documents([Document(page_content=\"LangChain là một framework để xây dựng ứng dụng LLM.\"), Document(page_content=\"RAG giúp giảm ảo giác.\")], embeddings)\n",
    "# retriever = vector_store.as_retriever()\n",
    "\n",
    "# Để ví dụ đơn giản, chúng ta sẽ giả lập retriever\n",
    "def mock_retriever(query):\n",
    "    if \"LangChain\" in query:\n",
    "        return \"LangChain là một framework để xây dựng ứng dụng LLM.\" # LangChain is a framework for building LLM applications.\n",
    "    if \"RAG\" in query:\n",
    "        return \"RAG giúp giảm ảo giác và tăng độ chính xác.\" # RAG helps reduce hallucinations and increase accuracy.\n",
    "    return \"Không tìm thấy thông tin liên quan.\" # No relevant information found.\n",
    "\n",
    "# Định nghĩa Prompt cho RAG\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Trả lời câu hỏi dựa trên ngữ cảnh: {context}\"), # Answer the question based on the context:\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "# Xây dựng chuỗi RAG với RunnablePassthrough\n",
    "# Đầu vào của chuỗi là {\"question\": \"...\"}\n",
    "rag_chain_with_passthrough = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(mock_retriever), # Retriever receives question from input\n",
    "        \"question\": RunnablePassthrough() # Pass the original question through\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- Ví dụ RunnablePassthrough ---\") # Example RunnablePassthrough\n",
    "print(rag_chain_with_passthrough.invoke({\"question\": \"LangChain là gì?\"})) # What is LangChain?\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Customizing Component Behavior in the Chain\n",
    "\n",
    "LCEL allows you to customize the behavior of individual components or the entire chain:\n",
    "\n",
    "* **`.bind()`**: Attaches fixed parameters to a `Runnable`. Example: `llm.bind(stop=[\"\\nObservation\"])`.\n",
    "* **`.with_config()`**: Configures runtime settings like `callbacks`, `tags`, `metadata`.\n",
    "* **`.with_retry()`**: Adds retry logic for a `Runnable` if it fails.\n",
    "* **`.with_types()`**: Adds type information for inputs/outputs, useful for validation and auto-completion.\n",
    "* **`.map()`**: Applies a `Runnable` to each element in a list input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Ví dụ về .bind() và .with_config()\n",
    "# Giả sử bạn muốn một LLM luôn dừng lại khi thấy \"END\"\n",
    "bound_llm = llm.bind(stop=[\"END\"]) # Example of .bind() and .with_config() / Assume you want an LLM to always stop when it sees \"END\"\n",
    "\n",
    "# Một chuỗi đơn giản với LLM đã bind\n",
    "bound_chain = ChatPromptTemplate.from_template(\"Viết một đoạn văn ngắn về {topic}. Kết thúc bằng từ END.\") | bound_llm | StrOutputParser() # A simple chain with the bound LLM / Write a short paragraph about {topic}. End with the word END.\n",
    "\n",
    "print(\"--- Ví dụ .bind() ---\") # Example .bind()\n",
    "print(bound_chain.invoke({\"topic\": \"mùa đông\"})) # winter\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Ví dụ về .with_config()\n",
    "# Thêm metadata cho chuỗi để theo dõi\n",
    "configured_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Nói xin chào.\") # Say hello.\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ").with_config(tags=[\"greeting_chain\", \"test_run\"]) # Add metadata to the chain for tracking\n",
    "\n",
    "print(\"--- Ví dụ .with_config() ---\") # Example .with_config()\n",
    "print(configured_chain.invoke({})) # No input needed as prompt is fixed\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Practical Example: Building a Custom RAG Chain with LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a complete and custom RAG chain using LCEL, combining components learned from Module 3 and Module 5 (Memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have the necessary libraries installed: `langchain-openai`, `chromadb`, `pypdf`.\n",
    "* Set the `OPENAI_API_KEY` environment variable.\n",
    "* Create a sample PDF file (e.g., `lcel_rag_document.pdf`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt thư viện nếu chưa có\n",
    "# pip install langchain-openai openai chromadb pypdf\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.memory import ConversationBufferMemory # Để duy trì lịch sử trò chuyện\n",
    "\n",
    "# Thiết lập biến môi trường cho khóa API của OpenAI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Khởi tạo LLM và Embeddings Model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Thư mục lưu trữ ChromaDB\n",
    "persist_directory = \"./chroma_lcel_rag_demo_db\"\n",
    "# Xóa thư mục cũ nếu tồn tại để đảm bảo sạch sẽ\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "    print(f\"Đã xóa thư mục Chroma cũ: {persist_directory}\") # Old Chroma directory deleted:\n",
    "\n",
    "# Tạo một file PDF mẫu\n",
    "pdf_file_path = \"lcel_rag_document.pdf\"\n",
    "try:\n",
    "    from reportlab.pdfgen import canvas\n",
    "    c = canvas.Canvas(pdf_file_path)\n",
    "    c.drawString(100, 750, \"LangChain Expression Language (LCEL) là một cách mạnh mẽ để xây dựng các chuỗi.\") # LangChain Expression Language (LCEL) is a powerful way to build chains.\n",
    "    c.drawString(100, 730, \"LCEL cho phép kết hợp các Runnable bằng toán tử pipe (|).\") # LCEL allows combining Runnables using the pipe operator (|).\n",
    "    c.drawString(100, 710, \"Các loại Runnable bao gồm RunnableSequence, RunnableParallel và RunnablePassthrough.\") # Runnable types include RunnableSequence, RunnableParallel, and RunnablePassthrough.\n",
    "    c.drawString(100, 690, \"Bạn có thể tùy chỉnh Runnable bằng .bind(), .with_config(), v.v.\") # You can customize Runnables using .bind(), .with_config(), etc.\n",
    "    c.drawString(100, 670, \"RAG (Retrieval-Augmented Generation) kết hợp truy xuất thông tin với LLM.\") # RAG (Retrieval-Augmented Generation) combines information retrieval with LLM.\n",
    "    c.drawString(100, 650, \"Chroma là một Vector Store phổ biến cho việc lưu trữ embeddings.\") # Chroma is a popular Vector Store for storing embeddings.\n",
    "    c.save()\n",
    "    print(f\"Đã tạo file PDF mẫu: {pdf_file_path}\") # Sample PDF file created:\n",
    "except ImportError:\n",
    "    with open(pdf_file_path, \"w\") as f:\n",
    "        f.write(\"Đây là file giả lập PDF cho LCEL RAG. Vui lòng thay bằng PDF thật.\") # This is a dummy PDF file for LCEL RAG. Please replace with a real PDF.\n",
    "    print(\"Không thể tạo PDF thật bằng reportlab. Sử dụng file giả lập.\") # Could not create real PDF with reportlab. Using dummy file.\n",
    "    print(\"Vui lòng đảm bảo bạn có file PDF thật 'lcel_rag_document.pdf' để ví dụ hoạt động tốt nhất.\") # Please ensure you have a real PDF file 'lcel_rag_document.pdf' for the example to work best.\n",
    "\n",
    "# --- Giai đoạn Indexing (Tải, Chia nhỏ, Nhúng, Lưu trữ) ---\n",
    "print(\"\\n--- Bắt đầu giai đoạn Indexing ---\") # --- Starting Indexing phase ---\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "print(\"Đã hoàn thành giai đoạn Indexing và tạo Retriever.\") # Indexing phase completed and Retriever created.\n",
    "\n",
    "# --- Giai đoạn Runtime (Xây dựng chuỗi RAG tùy chỉnh với LCEL và Memory) ---\n",
    "\n",
    "# 1. Khởi tạo Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 2. Định nghĩa Prompt cho RAG\n",
    "# Prompt này sẽ bao gồm lịch sử trò chuyện, ngữ cảnh và câu hỏi hiện tại\n",
    "rag_prompt_with_history = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Bạn là một trợ lý Q&A hữu ích. Trả lời câu hỏi dựa trên ngữ cảnh được cung cấp. Nếu không tìm thấy, hãy nói không biết. Duy trì ngữ cảnh cuộc trò chuyện.\"), # You are a helpful Q&A assistant. Answer the question based on the provided context. If not found, say you don't know. Maintain conversation context.\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # Lịch sử trò chuyện\n",
    "    (\"human\", \"Ngữ cảnh: {context}\\n\\nCâu hỏi: {question}\"), # Context: {context}\\n\\nQuestion: {question}\n",
    "])\n",
    "\n",
    "# 3. Xây dựng chuỗi RAG tùy chỉnh với LCEL\n",
    "# Chuỗi này sẽ:\n",
    "# - Lấy câu hỏi của người dùng (input)\n",
    "# - Lấy lịch sử trò chuyện từ memory\n",
    "# - Truy xuất ngữ cảnh từ retriever\n",
    "# - Kết hợp lịch sử, ngữ cảnh và câu hỏi vào prompt\n",
    "# - Gọi LLM để tạo câu trả lời\n",
    "# - Lưu ngữ cảnh mới vào memory\n",
    "# - Trả về câu trả lời\n",
    "\n",
    "# Hàm để định dạng tài liệu được truy xuất thành một chuỗi\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chuỗi chính\n",
    "# Đầu vào của chuỗi là {'input': user_question}\n",
    "# RunnableParallel để lấy cả lịch sử và ngữ cảnh song song\n",
    "# Sau đó, kết hợp chúng vào prompt và gọi LLM\n",
    "\n",
    "# Chuỗi để lấy ngữ cảnh và lịch sử\n",
    "context_and_history = RunnableParallel(\n",
    "    context=RunnableLambda(lambda x: x[\"input\"]) | retriever | format_docs, # Truy xuất ngữ cảnh\n",
    "    chat_history=RunnableLambda(lambda x: memory.load_memory_variables({})[\"chat_history\"]), # Lấy lịch sử từ memory\n",
    "    question=RunnablePassthrough() # Truyền câu hỏi gốc qua\n",
    ")\n",
    "\n",
    "# Chuỗi RAG cuối cùng\n",
    "full_rag_chain = (\n",
    "    context_and_history\n",
    "    | rag_prompt_with_history\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Hàm để gọi chuỗi và lưu ngữ cảnh vào memory\n",
    "def invoke_and_save(chain, user_input):\n",
    "    print(f\"\\nNgười dùng: {user_input}\") # User:\n",
    "    # Lấy lịch sử hiện tại trước khi invoke\n",
    "    current_chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "    \n",
    "    # Invoke chuỗi với input và lịch sử\n",
    "    response = chain.invoke({\"input\": user_input, \"chat_history\": current_chat_history})\n",
    "    \n",
    "    # Lưu ngữ cảnh mới vào memory\n",
    "    memory.save_context(\n",
    "        {\"input\": user_input},\n",
    "        {\"output\": response}\n",
    "    )\n",
    "    print(f\"AI: {response}\")\n",
    "    return response\n",
    "\n",
    "# --- Thực thi hệ thống Q&A tùy chỉnh ---\n",
    "print(\"\\n--- Bắt đầu hệ thống Q&A RAG tùy chỉnh với LCEL và Memory ---\") # Starting custom RAG Q&A system with LCEL and Memory\n",
    "\n",
    "# Câu hỏi 1: Về LCEL\n",
    "invoke_and_save(full_rag_chain, \"LCEL là gì?\") # What is LCEL?\n",
    "\n",
    "# Câu hỏi 2: Về RAG (sẽ sử dụng ngữ cảnh từ PDF)\n",
    "invoke_and_save(full_rag_chain, \"RAG giúp ích gì?\") # What does RAG help with?\n",
    "\n",
    "# Câu hỏi 3: Hỏi về một khái niệm đã nói trước đó (kiểm tra memory)\n",
    "invoke_and_save(full_rag_chain, \"Nó có những thành phần nào?\") # What are its components?\n",
    "\n",
    "# Câu hỏi 4: Hỏi về một khái niệm không có trong tài liệu (LLM sẽ nói không biết)\n",
    "invoke_and_save(full_rag_chain, \"Thủ đô của Úc là gì?\") # What is the capital of Australia?\n",
    "\n",
    "print(\"\\n--- Lịch sử cuối cùng trong Memory ---\") # Final history in Memory\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "# Dọn dẹp file PDF mẫu và thư mục Chroma\n",
    "os.remove(pdf_file_path)\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "print(f\"\\nĐã xóa file PDF mẫu và thư mục Chroma '{persist_directory}'.\") # Sample PDF file and Chroma directory '{persist_directory}' deleted.\n",
    "\n",
    "print(\"\\n--- Kết thúc hệ thống Q&A RAG tùy chỉnh ---\") # Ending custom RAG Q&A system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of data flow in `full_rag_chain`:**\n",
    "\n",
    "1.  `invoke_and_save(full_rag_chain, user_input)`: This function takes the user's question and calls `full_rag_chain`.\n",
    "2.  `context_and_history`:\n",
    "    * `context=RunnableLambda(lambda x: x[\"input\"]) | retriever | format_docs`: This part takes the `input` (user's question) from the input dictionary, passes it to the `retriever` to retrieve documents, and then formats those documents into a string. The result is assigned to the `context` key.\n",
    "    * `chat_history=RunnableLambda(lambda x: memory.load_memory_variables({})[\"chat_history\"])`: This part retrieves the current conversation history from the initialized `memory` object. The result is assigned to the `chat_history` key.\n",
    "    * `question=RunnablePassthrough()`: This part passes the original user's question directly to the `question` key.\n",
    "    * The output of `context_and_history` is a dictionary: `{\"context\": \"...\", \"chat_history\": [...], \"question\": \"...\"}`.\n",
    "3.  `| rag_prompt_with_history`: The prompt receives this dictionary and uses the `context`, `chat_history`, and `question` keys to construct the final prompt sent to the LLM.\n",
    "4.  `| llm`: The LLM receives the constructed prompt and generates a response.\n",
    "5.  `| StrOutputParser()`: The parser converts the LLM's response into a text string.\n",
    "6.  `memory.save_context(...)`: After the LLM generates the response, the `invoke_and_save` function saves this (input, output) pair to `memory` to maintain context for subsequent turns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson delved into **LangChain Expression Language (LCEL)**, a powerful tool for building complex and flexible processing chains in LangChain. You learned how to combine `Runnable`s like `RunnableSequence`, `RunnableParallel`, and `RunnablePassthrough` to create custom data flows. We also explored how to customize the behavior of components using methods like `.bind()` and `.with_config()`. Finally, you applied all this knowledge to **practice building a custom RAG chain with LCEL and integrated Memory**, illustrating how different LangChain components can be seamlessly connected to create intelligent and context-aware LLM applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
