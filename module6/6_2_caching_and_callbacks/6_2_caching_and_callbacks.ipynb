{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6.2: Caching and Callbacks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we learned how to build complex chains and Agents with LangChain Expression Language (LCEL). As LLM applications grow larger and are used more frequently, two important aspects to consider are **performance** and **observability**. This lesson will introduce **Caching** for speed and cost reduction, along with **Callbacks** for monitoring and controlling execution flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Why is Caching Needed?\n",
    "\n",
    "**Caching** is a technique of storing the results of expensive computations (like LLM calls) so that when the same computation is requested again, the result can be returned immediately from the cache instead of being re-executed.\n",
    "\n",
    "* **Reduce API Costs:** Calls to LLMs often incur costs (per token). If the same question is asked multiple times, caching helps avoid paying for each call.\n",
    "* **Increase Response Speed:** Retrieving data from cache is significantly faster than calling the LLM from scratch, improving user experience.\n",
    "* **Avoid Redundant LLM Calls for Duplicate Questions:** Especially useful in Question Answering (Q&A) applications or chatbots where users might repeat questions.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Types of Cache in LangChain\n",
    "\n",
    "LangChain supports various cache types, suitable for different environments and storage requirements:\n",
    "\n",
    "* **`InMemoryCache`:**\n",
    "    * **Concept:** The simplest cache, storing data in the application's RAM.\n",
    "    * **Pros:** Very fast, easy to set up.\n",
    "    * **Cons:** Data is lost when the application shuts down, cannot be shared between application instances.\n",
    "    * **When to use:** Development, testing, or small applications that don't require data persistence.\n",
    "* **`SQLiteCache`:**\n",
    "    * **Concept:** Stores data in a SQLite database file.\n",
    "    * **Pros:** Data is persistent (not lost when the application shuts down), easier to set up than dedicated DB servers.\n",
    "    * **Cons:** Performance might not be as high as specialized DB servers for heavy loads.\n",
    "    * **When to use:** Local applications requiring data persistence without complex DB server setup.\n",
    "* **`RedisCache`:**\n",
    "    * **Concept:** Uses Redis (an in-memory data structure store) as the cache backend.\n",
    "    * **Pros:** Very fast, can be shared across multiple applications/servers, supports TTL (Time To Live) for data.\n",
    "    * **Cons:** Requires Redis server installation and management.\n",
    "    * **When to use:** Production applications requiring high performance, shared cache, and scalability.\n",
    "* **Other types:** LangChain also supports other cache types like `GPTCache`, `UpstashRedisCache`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. How to Set Up and Use Caching\n",
    "\n",
    "To use caching in LangChain, you need to set up a cache backend and then enable it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.cache import InMemoryCache\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Thiết lập biến môi trường cho khóa API của OpenAI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 1. Thiết lập Caching Backend\n",
    "# Kích hoạt InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "print(\"Đã thiết lập InMemoryCache.\") # InMemoryCache has been set up.\n",
    "\n",
    "# Để sử dụng SQLiteCache:\n",
    "# from langchain.cache import SQLiteCache\n",
    "# langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\n",
    "# print(\"Đã thiết lập SQLiteCache.\") # SQLiteCache has been set up.\n",
    "\n",
    "# Để sử dụng RedisCache: (cần cài đặt redis và redis-py)\n",
    "# pip install redis\n",
    "# from langchain.cache import RedisCache\n",
    "# import redis\n",
    "# langchain.llm_cache = RedisCache(redis.Redis(host=\"localhost\", port=6379, db=0))\n",
    "# print(\"Đã thiết lập RedisCache.\") # RedisCache has been set up.\n",
    "\n",
    "# 2. Khởi tạo LLM (hoặc Chain/Agent)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# 3. Sử dụng LLM như bình thường\n",
    "print(\"\\n--- Thực hành sử dụng Caching ---\") # --- Practical Caching Example ---\n",
    "\n",
    "# Lần gọi đầu tiên (sẽ gọi LLM thực sự)\n",
    "print(\"Lần gọi 1:\") # Call 1:\n",
    "start_time = time.time()\n",
    "response1 = llm.invoke(\"Kể một câu chuyện ngắn về một con chó phiêu lưu.\") # Tell a short story about an adventurous dog.\n",
    "end_time = time.time()\n",
    "print(f\"Thời gian phản hồi: {end_time - start_time:.2f} giây\") # Response time: {end_time - start_time:.2f} seconds\n",
    "print(f\"Phản hồi: {response1.content[:50]}...\") # Response: {response1.content[:50]}...\n",
    "\n",
    "# Lần gọi thứ hai với cùng một prompt (sẽ lấy từ cache)\n",
    "print(\"\\nLần gọi 2 (cùng prompt):\") # Call 2 (same prompt):\n",
    "start_time = time.time()\n",
    "response2 = llm.invoke(\"Kể một câu chuyện ngắn về một con chó phiêu lưu.\") # Tell a short story about an adventurous dog.\n",
    "end_time = time.time()\n",
    "print(f\"Thời gian phản hồi: {end_time - start_time:.2f} giây\") # Response time: {end_time - start_time:.2f} seconds\n",
    "print(f\"Phản hồi: {response2.content[:50]}...\") # Response: {response2.content[:50]}...\n",
    "\n",
    "# Lần gọi thứ ba với prompt khác (sẽ gọi LLM thực sự)\n",
    "print(\"\\nLần gọi 3 (prompt khác):\") # Call 3 (different prompt):\n",
    "start_time = time.time()\n",
    "response3 = llm.invoke(\"Kể một câu chuyện ngắn về một con mèo dễ thương.\") # Tell a short story about a cute cat.\n",
    "end_time = time.time()\n",
    "print(f\"Thời gian phản hồi: {end_time - start_time:.2f} giây\") # Response time: {end_time - start_time:.2f} seconds\n",
    "print(f\"Phản hồi: {response3.content[:50]}...\") # Response: {response3.content[:50]}...\n",
    "\n",
    "# Xóa cache (nếu cần)\n",
    "langchain.llm_cache.clear()\n",
    "print(\"\\nĐã xóa cache.\") # Cache cleared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "* You set up the cache by assigning a cache object (e.g., `InMemoryCache()`) to `langchain.llm_cache`.\n",
    "* Once the cache is set up, any call to the LLM (or via Chain/Agent) with the same prompt will automatically be served from the cache if the result is already available.\n",
    "* You will notice a significant difference in response time between the first call (slower) and the second call (much faster) with the same prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Concept of Callbacks\n",
    "\n",
    "**Callbacks** in LangChain are a powerful mechanism that allows you to **monitor and control the execution flow** of LangChain components (LLMs, Chains, Tools, Agents). They provide \"hooks\" where you can attach your custom functions to perform actions when a specific event occurs.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Callback Events\n",
    "\n",
    "LangChain provides various callback events, allowing you to monitor application activity in detail:\n",
    "\n",
    "* `on_llm_start`, `on_llm_end`, `on_llm_error`: When an LLM starts, ends, or encounters an error.\n",
    "* `on_chain_start`, `on_chain_end`, `on_chain_error`: When a Chain starts, ends, or encounters an error.\n",
    "* `on_tool_start`, `on_tool_end`, `on_tool_error`: When a Tool starts, ends, or encounters an error (applies only to Agents).\n",
    "* `on_agent_action`, `on_agent_finish`: When an Agent takes an action or finishes.\n",
    "* `on_text`: When a piece of text is generated (useful for streaming).\n",
    "* `on_retriever_start`, `on_retriever_end`, `on_retriever_error`: When a Retriever starts, ends, or encounters an error.\n",
    "* And many other events.\n",
    "\n",
    "You can create a **Custom Callback Handler** by inheriting from `BaseCallbackHandler` and overriding the corresponding methods for the events you want to monitor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Applications of Callbacks\n",
    "\n",
    "Callbacks have countless applications in developing and deploying LLM applications:\n",
    "\n",
    "* **Logging:** Record detailed activities of LLMs, Chains, Tools for debugging or later analysis.\n",
    "* **Monitoring:** Track performance (response time, token count), costs, and errors in real-time.\n",
    "* **Performance Analysis:** Collect data to analyze and optimize components.\n",
    "* **Custom Error Handling:** Perform specific actions when errors occur (e.g., send notifications, retry).\n",
    "* **Progress Tracking:** Provide feedback to the user about the progress of a long task.\n",
    "* **Flow Control:** Change the behavior of a Chain/Agent based on events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Practical Example: Using Callbacks to Monitor Chain or Agent Activity\n",
    "\n",
    "We will create a simple Custom Callback Handler to log key events when a Chain or Agent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "# Thiết lập biến môi trường cho khóa API của OpenAI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 1. Tạo Custom Callback Handler\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Chạy khi LLM bắt đầu.\"\"\"\n",
    "        print(f\"\\n--- LLM BẮT ĐẦU ---\") # --- LLM START ---\n",
    "        print(f\"  Mô hình: {serialized['name']}\") #   Model:\n",
    "        print(f\"  Prompts: {prompts}\") #   Prompts:\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        \"\"\"Chạy khi LLM kết thúc thành công.\"\"\"\n",
    "        print(f\"--- LLM KẾT THÚC ---\") # --- LLM END ---\n",
    "        print(f\"  Đầu ra: {response.generations[0][0].text[:50]}...\") #   Output:\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Chạy khi LLM gặp lỗi.\"\"\"\n",
    "        print(f\"--- LLM LỖI ---\") # --- LLM ERROR ---\n",
    "        print(f\"  Lỗi: {error}\") #   Error:\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Chạy khi một Chain bắt đầu.\"\"\"\n",
    "        print(f\"\\n=== CHAIN BẮT ĐẦU: {serialized['name']} ===\") # === CHAIN START: {serialized['name']} ===\n",
    "        print(f\"  Đầu vào Chain: {inputs}\") #   Chain Input:\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\n",
    "        \"\"\"Chạy khi một Chain kết thúc thành công.\"\"\"\n",
    "        print(f\"=== CHAIN KẾT THÚC ===\") # === CHAIN END ===\n",
    "        print(f\"  Đầu ra Chain: {outputs}\") #   Chain Output:\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Chạy khi một Tool bắt đầu (chỉ với Agents).\"\"\"\n",
    "        print(f\"\\n--- TOOL BẮT ĐẦU: {serialized['name']} ---\") # --- TOOL START: {serialized['name']} ---\n",
    "        print(f\"  Đầu vào Tool: {input_str}\") #   Tool Input:\n",
    "\n",
    "    def on_tool_end(self, output: str, **kwargs: Any) -> None:\n",
    "        \"\"\"Chạy khi một Tool kết thúc thành công (chỉ với Agents).\"\"\"\n",
    "        print(f\"--- TOOL KẾT THÚC ---\") # --- TOOL END ---\n",
    "        print(f\"  Đầu ra Tool: {output[:50]}...\") #   Tool Output:\n",
    "\n",
    "    def on_agent_action(self, action: Any, **kwargs: Any) -> Any:\n",
    "        \"\"\"Chạy khi Agent đưa ra một hành động.\"\"\"\n",
    "        print(f\"\\n--- AGENT HÀNH ĐỘNG ---\") # --- AGENT ACTION ---\n",
    "        print(f\"  Hành động: {action.tool} với đầu vào: {action.tool_input}\") #   Action: {action.tool} with input: {action.tool_input}\n",
    "\n",
    "    def on_agent_finish(self, finish: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"Chạy khi Agent kết thúc.\"\"\"\n",
    "        print(f\"\\n--- AGENT KẾT THÚC ---\") # --- AGENT FINISH ---\n",
    "        print(f\"  Kết quả cuối cùng: {finish.return_values['output'][:50]}...\") #   Final result:\n",
    "\n",
    "# 2. Khởi tạo LLM và Chain\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "prompt_template = PromptTemplate.from_template(\"Trả lời câu hỏi sau: {question}\") # Answer the following question:\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# 3. Sử dụng Callbacks\n",
    "print(\"--- Thực hành sử dụng Callbacks với Chain ---\") # --- Practical Callbacks with Chain ---\n",
    "\n",
    "# Cách 1: Truyền callback handler trực tiếp vào .invoke()\n",
    "response_chain = llm_chain.invoke(\n",
    "    {\"question\": \"LangChain là gì?\"}, # What is LangChain?\n",
    "    config={\"callbacks\": [MyCustomHandler()]} # Truyền callback handler ở đây\n",
    ")\n",
    "print(f\"\\nPhản hồi cuối cùng từ Chain: {response_chain['text'][:100]}...\") # Final response from Chain:\n",
    "\n",
    "# Cách 2: Gắn callback handler vào LLM (sẽ áp dụng cho tất cả các cuộc gọi LLM qua LLM này)\n",
    "# llm_with_callback = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, callbacks=[MyCustomHandler()])\n",
    "# llm_chain_with_callback = LLMChain(llm=llm_with_callback, prompt=prompt_template)\n",
    "# print(\"\\n--- Thực hành sử dụng Callbacks gắn vào LLM ---\")\n",
    "# response_chain_attached = llm_chain_with_callback.invoke({\"question\": \"RAG là gì?\"})\n",
    "# print(f\"\\nPhản hồi cuối cùng từ Chain (gắn vào LLM): {response_chain_attached['text'][:100]}...\")\n",
    "\n",
    "# 4. Thực hành Callbacks với Agent (cần cài đặt thêm thư viện cho Tools)\n",
    "# pip install google-search-results numexpr\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_community.tools import Tool\n",
    "from langchain_community.tools.calculator.tool import Calculator\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Khởi tạo Tools\n",
    "search_tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    func=SerpAPIWrapper().run,\n",
    "    description=\"Hữu ích khi bạn cần tìm kiếm thông tin trên Google về các sự kiện hiện tại hoặc dữ liệu thực tế.\" # Useful when you need to search for information on Google about current events or factual data.\n",
    ")\n",
    "calculator_tool = Calculator()\n",
    "tools = [search_tool, calculator_tool]\n",
    "\n",
    "# Định nghĩa Prompt cho Agent\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Bạn là một trợ lý hữu ích. Bạn có quyền truy cập vào các công cụ sau: {tools}. Sử dụng chúng để trả lời các câu hỏi.\"), # You are a helpful assistant. You have access to the following tools: {tools}. Use them to answer questions.\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Tạo Agent\n",
    "agent = create_react_agent(llm, tools, agent_prompt)\n",
    "\n",
    "# Tạo Agent Executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False) # verbose=False để callback handler kiểm soát output\n",
    "\n",
    "print(\"\\n--- Thực hành sử dụng Callbacks với Agent ---\") # --- Practical Callbacks with Agent ---\n",
    "response_agent = agent_executor.invoke(\n",
    "    {\"input\": \"Thời tiết hôm nay ở Đà Nẵng thế nào?\"}, # What's the weather like today in Da Nang?\n",
    "    config={\"callbacks\": [MyCustomHandler()]} # Truyền callback handler vào Agent Executor\n",
    ")\n",
    "print(f\"\\nPhản hồi cuối cùng từ Agent: {response_agent['output'][:100]}...\") # Final response from Agent:\n",
    "\n",
    "print(\"\\n--- Kết thúc Callbacks thực hành ---\") # --- End Callbacks practical example ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "* We create a `MyCustomHandler` class inheriting from `BaseCallbackHandler` and override methods like `on_llm_start`, `on_llm_end`, `on_chain_start`, `on_chain_end`, `on_tool_start`, `on_tool_end`, `on_agent_action`, `on_agent_finish`.\n",
    "* When you run `llm_chain.invoke` or `agent_executor.invoke` and pass `config={\"callbacks\": [MyCustomHandler()]}` to it, the corresponding methods in `MyCustomHandler` will automatically be called when events occur during the Chain or Agent execution.\n",
    "* You will see `print` statements from `MyCustomHandler` appear, showing you exactly when the LLM, Chain, or Tool starts/ends, and what their inputs/outputs are. This is extremely useful for debugging and monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson introduced two important concepts for optimizing and monitoring LangChain applications: **Caching** and **Callbacks**. You understood **why caching is needed** to reduce API costs and increase response speed, as well as common cache types like `InMemoryCache`, `SQLiteCache`, and `RedisCache`. Regarding **Callbacks**, you grasped the **concept** of the execution flow monitoring mechanism, key **callback events**, and their **applications** in logging, monitoring, and performance analysis. Finally, you practiced **using Callbacks** to closely observe the activity of a Chain and an Agent, giving you deeper insights into how LangChain components interact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
