{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6.3: Error Handling and Debugging\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process of developing any software application, especially with complex systems like LLM applications using LangChain, encountering errors is inevitable. Understanding how these errors arise and knowing how to handle and debug them are crucial skills. This lesson will focus on common errors, error handling techniques, and effective debugging tools in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Common Errors in LLM and LangChain Application Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. API Rate Limits\n",
    "\n",
    "* **Concept:** LLM providers (like OpenAI, Google) often impose limits on the number of requests or tokens you can send within a certain time frame (e.g., requests per minute, tokens per minute).\n",
    "* **Causes:** Sending too many requests in a short period, or sending excessively large requests.\n",
    "* **Consequences:** Requests are rejected with an HTTP 429 (Too Many Requests) error.\n",
    "* **Solutions:**\n",
    "    * **Exponential Backoff:** Automatically retry the request after increasing delays.\n",
    "    * **Rate Limiting:** Limit the number of requests from your application's side.\n",
    "    * **Upgrade API Plan:** If you frequently hit limits, consider upgrading your service plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Invalid Prompts\n",
    "\n",
    "* **Concept:** Errors occur when the prompt sent to the LLM does not conform to the model's expected format or contains invalid characters/structures.\n",
    "* **Causes:**\n",
    "    * Syntax errors in `f-string` or `template`.\n",
    "    * Missing variables in the prompt.\n",
    "    * Prompt is too long, exceeding the LLM's context window.\n",
    "    * Using disallowed special characters.\n",
    "* **Consequences:** LLM returns an error, or a irrelevant/nonsensical response.\n",
    "* **Solutions:** Carefully check prompt syntax, ensure all variables are populated, check prompt length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Parsing Errors\n",
    "\n",
    "* **Concept:** Occurs when the LLM's output cannot be parsed into the desired format by an `OutputParser` or by the `AgentExecutor` (when an Agent attempts to call a Tool).\n",
    "* **Causes:**\n",
    "    * LLM generates a response that is not valid JSON, XML, or the Agent's action format.\n",
    "    * LLM `temperature` is too high, causing it to generate creative but unstructured output.\n",
    "    * The prompt is not clear enough in instructing the LLM about the output format.\n",
    "* **Consequences:** Application crashes with a parsing error, or the Agent cannot perform the next action.\n",
    "* **Solutions:**\n",
    "    * **Prompt Engineering:** Ask the LLM to output a specific format, provide few-shot examples.\n",
    "    * **Lower `temperature`:** When structured output is required.\n",
    "    * **`handle_parsing_errors=True`:** In `AgentExecutor`, allows the Agent to attempt self-correction.\n",
    "    * **Custom Output Parser:** Write a custom parser to handle variations in output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Connection Errors\n",
    "\n",
    "* **Concept:** Network errors that occur when the application cannot connect to the LLM's API or external Tool APIs.\n",
    "* **Causes:** Loss of internet connection, firewall blocking, API server downtime, incorrect API URL.\n",
    "* **Consequences:** Request fails with a connection error.\n",
    "* **Solutions:** Check network connectivity, ensure correct API URLs, check API server status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Common Error Handling Technique: try-except blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic and effective way to handle errors in Python is by using `try-except` blocks. This allows you to \"catch\" exceptions and perform graceful recovery actions or error notifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from openai import OpenAIError # To catch specific OpenAI errors\n",
    "\n",
    "# Thiết lập biến môi trường cho khóa API của OpenAI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "prompt_template = PromptTemplate.from_template(\"Viết một câu nói truyền cảm hứng về {topic}.\") # Write an inspiring quote about {topic}.\n",
    "\n",
    "def generate_inspiration(topic: str):\n",
    "    try:\n",
    "        # Giả lập một lỗi API (ví dụ: do sai khóa API hoặc rate limit)\n",
    "        # Trong thực tế, lỗi này sẽ do thư viện OpenAI ném ra\n",
    "        if topic == \"lỗi\": # error\n",
    "            raise OpenAIError(\"Simulated OpenAI API error: Invalid API key or Rate Limit Exceeded.\")\n",
    "\n",
    "        response = llm.invoke(prompt_template.format(topic=topic))\n",
    "        print(f\"Thành công: {response.content}\") # Success:\n",
    "    except OpenAIError as e:\n",
    "        print(f\"Lỗi API OpenAI: {e}. Vui lòng kiểm tra khóa API hoặc giới hạn tốc độ.\") # OpenAI API Error: {e}. Please check API key or rate limit.\n",
    "    except Exception as e:\n",
    "        print(f\"Một lỗi không xác định đã xảy ra: {e}\") # An unknown error occurred:\n",
    "\n",
    "print(\"--- Thực hành try-except blocks ---\") # --- Practical try-except blocks ---\n",
    "generate_inspiration(\"hạnh phúc\") # happiness\n",
    "generate_inspiration(\"lỗi\") # error (will trigger simulated error)\n",
    "generate_inspiration(\"thành công\") # success\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Debugging Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Print Statements\n",
    "\n",
    "A classic yet still very effective debugging method. By adding `print()` statements at various points in your code, you can track variable values, execution flow, and pinpoint where errors occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example already illustrated in previous lessons and will continue in the practical section.\n",
    "# print(f\"Giá trị biến X: {X}\") # Value of variable X:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Using `verbose=True` in Agent Executor (and Chains)\n",
    "\n",
    "LangChain provides a `verbose=True` parameter for most Chains and especially for `AgentExecutor`. When `verbose` is enabled, LangChain will print detailed intermediate steps of the execution process, including:\n",
    "\n",
    "* **Thoughts:** What the LLM is thinking.\n",
    "* **Actions:** Which tool the LLM decided to call with what input.\n",
    "* **Observations:** The result returned from the tool execution.\n",
    "* **Final Answer:** When the Agent concludes.\n",
    "\n",
    "This is incredibly useful for understanding why an Agent made a particular decision or why it failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example will be illustrated in the practical section.\n",
    "# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. LangSmith (Overview)\n",
    "\n",
    "**LangSmith** is a platform developed by LangChain, providing powerful tools for **monitoring, debugging, testing, and evaluating** LLM applications.\n",
    "\n",
    "* **Key Features:**\n",
    "    * **Trace Visualization:** Visually displays the entire execution flow of a Chain/Agent, including all LLM calls, Tool calls, and intermediate steps. Helps you easily identify bottlenecks or errors.\n",
    "    * **Debugging:** Provides detailed information about inputs, outputs, and errors for each step.\n",
    "    * **Monitoring:** Tracks application performance, costs, latency, and error rates in real-time.\n",
    "    * **Evaluation:** Supports automated testing and evaluation of different application versions.\n",
    "    * **Dataset Management:** Manages datasets for testing and fine-tuning.\n",
    "* **Usage:** You need to sign up for a LangSmith account and set the environment variables `LANGCHAIN_TRACING_V2=true`, `LANGCHAIN_API_KEY`, `LANGCHAIN_PROJECT`. After that, LangChain will automatically send traces to LangSmith.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical Exercise: Handling Error Scenarios and Debugging Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will practice common error scenarios and how to debug them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have `langchain-openai`, `google-search-results`, `numexpr` installed.\n",
    "* Set the `OPENAI_API_KEY` and `SERPAPI_API_KEY` environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai google-search-results numexpr\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_community.tools import Tool\n",
    "from langchain_community.tools.calculator.tool import Calculator\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.exceptions import OutputParserException # To catch parsing errors\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "# Thiết lập biến môi trường cho khóa API của OpenAI và SerpAPI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"YOUR_SERPAPI_API_KEY\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Khởi tạo Tools\n",
    "search_tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    func=SerpAPIWrapper().run,\n",
    "    description=\"Hữu ích khi bạn cần tìm kiếm thông tin trên Google về các sự kiện hiện tại hoặc dữ liệu thực tế.\" # Useful when you need to search for information on Google about current events or factual data.\n",
    ")\n",
    "calculator_tool = Calculator()\n",
    "tools = [search_tool, calculator_tool]\n",
    "\n",
    "# Định nghĩa Prompt cho Agent\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Bạn là một trợ lý hữu ích. Bạn có quyền truy cập vào các công cụ sau: {tools}. Sử dụng chúng để trả lời các câu hỏi.\"), # You are a helpful assistant. You have access to the following tools: {tools}. Use them to answer questions.\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Tạo Agent\n",
    "agent = create_react_agent(llm, tools, agent_prompt)\n",
    "\n",
    "# --- 4.1. Thực hành xử lý lỗi API (giả lập) ---\n",
    "print(\"--- Thực hành xử lý lỗi API (giả lập) ---\") # --- Practical API Error Handling (simulated) ---\n",
    "\n",
    "# Giả lập một LLM có thể gây lỗi\n",
    "class MockErrorLLM(ChatOpenAI):\n",
    "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, **kwargs: Any) -> LLMResult:\n",
    "        # Giả lập lỗi API sau một số lần gọi\n",
    "        if getattr(self, '_call_count', 0) < 1: # Lần gọi đầu tiên thành công\n",
    "            setattr(self, '_call_count', getattr(self, '_call_count', 0) + 1)\n",
    "            return super()._generate(messages, stop, **kwargs)\n",
    "        else:\n",
    "            setattr(self, '_call_count', getattr(self, '_call_count', 0) + 1)\n",
    "            # Giả lập lỗi API\n",
    "            raise Exception(\"Simulated API connection error or rate limit exceeded.\")\n",
    "\n",
    "mock_llm = MockErrorLLM(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "mock_llm_chain = LLMChain(llm=mock_llm, prompt=PromptTemplate.from_template(\"Nói xin chào.\")) # Say hello.\n",
    "\n",
    "try:\n",
    "    print(\"Thử gọi LLM lần 1:\") # Attempting LLM call 1:\n",
    "    mock_llm_chain.invoke({})\n",
    "    print(\"Gọi LLM lần 1 thành công.\") # LLM call 1 successful.\n",
    "    \n",
    "    print(\"\\nThử gọi LLM lần 2 (sẽ lỗi):\") # Attempting LLM call 2 (will fail):\n",
    "    mock_llm_chain.invoke({})\n",
    "    print(\"Gọi LLM lần 2 thành công (không mong đợi).\") # LLM call 2 successful (unexpected).\n",
    "except Exception as e:\n",
    "    print(f\"Đã bắt được lỗi khi gọi LLM: {e}\") # Caught error when calling LLM:\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 4.2. Thực hành xử lý lỗi phân tích cú pháp của Agent ---\n",
    "print(\"\\n--- Thực hành xử lý lỗi phân tích cú pháp của Agent ---\") # --- Practical Agent Parsing Error Handling ---\n",
    "\n",
    "# Tạo một Agent Executor với handle_parsing_errors=True\n",
    "# Để minh họa lỗi, chúng ta sẽ tạo một LLM giả lập đôi khi trả về định dạng sai\n",
    "class MalformedOutputLLM(ChatOpenAI):\n",
    "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, **kwargs: Any) -> LLMResult:\n",
    "        # Sau một số lần gọi, trả về output không đúng định dạng tool call\n",
    "        if getattr(self, '_malform_count', 0) < 1:\n",
    "            setattr(self, '_malform_count', getattr(self, '_malform_count', 0) + 1)\n",
    "            return super()._generate(messages, stop, **kwargs)\n",
    "        else:\n",
    "            setattr(self, '_malform_count', getattr(self, '_malform_count', 0) + 1)\n",
    "            # Trả về một chuỗi không phải JSON hoặc không đúng định dạng Action\n",
    "            # Điều này sẽ gây ra lỗi phân tích cú pháp trong AgentExecutor\n",
    "            malformed_response = \"Đây là một phản hồi không đúng định dạng mà Agent không thể hiểu.\" # This is a malformed response that the Agent cannot understand.\n",
    "            return LLMResult(generations=[[BaseMessage(content=malformed_response)]])\n",
    "\n",
    "malformed_llm = MalformedOutputLLM(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "malformed_agent = create_react_agent(malformed_llm, tools, agent_prompt)\n",
    "\n",
    "# Agent Executor với handle_parsing_errors=True\n",
    "malformed_agent_executor = AgentExecutor(\n",
    "    agent=malformed_agent,\n",
    "    tools=tools,\n",
    "    verbose=True, # Bật verbose để xem Agent cố gắng tự sửa lỗi\n",
    "    handle_parsing_errors=True, # Cho phép Agent xử lý lỗi phân tích cú pháp\n",
    "    max_iterations=3 # Giới hạn số lần lặp để tránh vòng lặp vô hạn\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"Thử chạy Agent với đầu ra LLM không đúng định dạng:\") # Attempting to run Agent with malformed LLM output:\n",
    "    response_malformed = malformed_agent_executor.invoke({\"input\": \"Tìm kiếm thời tiết ở New York.\"}) # Search for weather in New York.\n",
    "    print(f\"Phản hồi từ Agent: {response_malformed['output']}\") # Response from Agent:\n",
    "except OutputParserException as e:\n",
    "    print(f\"Đã bắt được lỗi phân tích cú pháp (nhưng AgentExecutor đã cố gắng xử lý): {e}\") # Caught parsing error (but AgentExecutor attempted to handle):\n",
    "except Exception as e:\n",
    "    print(f\"Một lỗi khác đã xảy ra: {e}\") # Another error occurred:\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 4.3. Debugging luồng Agent với `verbose=True` ---\n",
    "print(\"\\n--- Debugging luồng Agent với verbose=True ---\") # --- Debugging Agent flow with verbose=True ---\n",
    "\n",
    "# Tạo Agent Executor với verbose=True\n",
    "debug_agent_executor = AgentExecutor(\n",
    "    agent=agent, # Sử dụng agent bình thường\n",
    "    tools=tools,\n",
    "    verbose=True, # BẬT VERBOSE\n",
    "    max_iterations=5 # Giới hạn số lần lặp\n",
    ")\n",
    "\n",
    "print(\"Yêu cầu: Tính tổng 123 và 456, sau đó tìm kiếm thông tin về LangChain.\") # Request: Calculate the sum of 123 and 456, then search for information about LangChain.\n",
    "response_debug = debug_agent_executor.invoke({\"input\": \"Tính tổng 123 và 456, sau đó tìm kiếm thông tin về LangChain.\"}) # Calculate the sum of 123 and 456, then search for information about LangChain.\n",
    "print(f\"\\nPhản hồi cuối cùng từ Agent: {response_debug['output']}\") # Final response from Agent:\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Dọn dẹp (nếu có file cache SQLite)\n",
    "# if os.path.exists(\".langchain.db\"):\n",
    "#     os.remove(\".langchain.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of practical sections:**\n",
    "\n",
    "* **Simulated API Error Handling:** We define `MockErrorLLM` to simulate an LLM that occasionally throws an error. The `try-except` block surrounding the LLM call catches this error and prints a friendly message instead of crashing the application.\n",
    "* **Agent Parsing Error Handling:**\n",
    "    * `MalformedOutputLLM` is created to simulate an LLM that sometimes generates malformed output that the `AgentExecutor` cannot parse into an `AgentAction` or `AgentFinish`.\n",
    "    * When `AgentExecutor` is initialized with `handle_parsing_errors=True`, instead of stopping immediately, it passes the parsing error back to the LLM as an \"Observation.\" The LLM then attempts to self-correct in the next turn.\n",
    "    * You will see in the `verbose=True` output that the Agent receives an \"Observation\" about the parsing error and tries to re-think to generate a valid action.\n",
    "* **Debugging Agent Flow with `verbose=True`:**\n",
    "    * When you run `debug_agent_executor.invoke` with `verbose=True`, you will see each \"Thought,\" \"Action,\" and \"Observation\" of the Agent.\n",
    "    * This allows you to trace the LLM's reasoning step-by-step, which tool was called, and the result of that tool. It's crucial for understanding why the Agent behaves the way it does and for identifying issues in your logic or prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson equipped you with essential skills for error handling and debugging in LLM application development with LangChain. You learned about **common errors** such as API rate limits, invalid prompts, parsing errors, and connection errors. We explored the **`try-except`** technique for graceful error handling. For **debugging**, you learned how to use tools like **print statements** and especially the **`verbose=True`** parameter in `AgentExecutor` to trace the detailed flow of Agent activity. Finally, you were introduced to **LangSmith**, a powerful platform for in-depth monitoring and debugging. Through practical examples, you applied these techniques to handle error scenarios and gain a better understanding of how to debug your LangChain applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
