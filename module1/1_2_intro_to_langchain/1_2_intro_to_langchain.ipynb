{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1.2: Introduction to LangChain\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Challenges of Working Directly with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Large Language Models (LLMs) like GPT or Gemini are incredibly powerful, building practical applications based on them often presents several challenges when working directly with LLM APIs. The main issues include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Complex Prompt Management\n",
    "\n",
    "* **Manual Prompt Creation:** As applications become more complex, manually creating and managing prompts (e.g., as plain Python strings) becomes cumbersome and error-prone.\n",
    "* **Prompt Reusability:** It's difficult to reuse parts of prompts or apply the same prompt structure to different tasks.\n",
    "* **Prompt Engineering:** Experimenting with and optimizing prompts to achieve desired results requires many iterations and tracking, which is inefficient without a clear structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Chaining Operations\n",
    "\n",
    "* **Multi-step Tasks:** Most real-world applications are not just a single LLM call. They often require a sequence of tasks: data fetching, preprocessing, LLM invocation, post-processing, another LLM call, and so on.\n",
    "* **Input/Output Connection:** Manually connecting the output of one step as the input for the next step is complex and prone to errors.\n",
    "* **Error Handling:** When an error occurs in the chain, identifying the cause and handling the error becomes difficult.\n",
    "\n",
    "*Example:* To build a question-answering system over your documents, you would need the following steps:\n",
    "1.  Load documents.\n",
    "2.  Split documents into smaller chunks.\n",
    "3.  Create embeddings for those chunks and store them in a vector database.\n",
    "4.  When a question is asked, search for relevant chunks in the vector database.\n",
    "5.  Pass the relevant chunks and the question to the LLM as a prompt.\n",
    "6.  The LLM generates an answer.\n",
    "\n",
    "If you had to write code for each of these steps and connect them manually, the code would be long, hard to read, and difficult to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. External Data Integration\n",
    "\n",
    "* **Unstructured Data:** LLMs work well with text, but our data is often scattered across various formats (PDFs, CSVs, web pages, databases).\n",
    "* **Information Retrieval:** LLMs have knowledge up to a certain point in time. For them to answer questions about new or your specific data (e.g., internal company documents), there needs to be a mechanism for the LLM to access and use that information.\n",
    "* **API Connection:** For LLMs to perform real-world actions (e.g., searching the web, calling external APIs), they need the ability to use tools. Managing these tools and allowing the LLM to decide when to use them is a significant challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is LangChain? Why is it Necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. What is LangChain?\n",
    "\n",
    "**LangChain** is an open-source framework designed to help developers build applications powered by Large Language Models (LLMs) more easily and efficiently. It provides a set of tools, components, and abstractions to simplify complex processes related to LLMs, from prompt management to connecting LLMs with external data sources and tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Why is LangChain Necessary for LLM Application Development?\n",
    "\n",
    "LangChain addresses the aforementioned challenges by providing:\n",
    "\n",
    "* **Structuring and Modularity:** Breaking down complex tasks into small, reusable components (models, prompts, chains, agents, tools, memory).\n",
    "* **Composability:** Easily connecting these components to create complex processing flows.\n",
    "* **Data Integration:** Offering tools to load, process, and integrate data from various sources with LLMs (e.g., RAG - Retrieval Augmented Generation).\n",
    "* **Reasoning and Action Capabilities:** Enabling LLMs not just to generate text but also to plan and execute actions through tools.\n",
    "* **Reduced Boilerplate Code:** Helping developers focus on business logic instead of rewriting basic code sections.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LangChain's Design Principles and Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is designed based on several core principles to maximize efficiency and flexibility in building LLM applications:\n",
    "\n",
    "* **Modularity:** LangChain components are designed to be independent, allowing users to combine and swap them flexibly. This promotes reusability and ease of maintenance.\n",
    "* **Abstraction:** LangChain abstracts away the complexity of interacting with different LLMs and data sources, providing a unified interface. For example, you can switch between OpenAI GPT and Google Gemini without significant code changes.\n",
    "* **Composability:** Components can be combined like building blocks to create more complex logical sequences. This is the foundation of the \"Chains\" and \"Agents\" concepts.\n",
    "* **Data-awareness:** LangChain is designed to easily connect LLMs with external data sources, enabling LLMs to access information not present in their initial training data.\n",
    "* **Agent-centricity:** LangChain focuses on enabling LLMs not only to generate text but also to act, plan, and interact with the external environment through tools.\n",
    "\n",
    "**Main Goals of LangChain:**\n",
    "\n",
    "* **Simplify Development:** Reduce the complexity and time required to build LLM applications.\n",
    "* **Enhance LLM Capabilities:** Extend the capabilities of LLMs by allowing them to interact with external data and tools.\n",
    "* **Foster New Use Cases:** Encourage the creation of more complex and innovative LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LangChain's Overview Architecture: Key Components and Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain's architecture is built around several core components, each with a distinct role, and they can be combined with each other.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Key Components\n",
    "\n",
    "1.  **Models:**\n",
    "    * **LLMs:** A generic interface to interact with large language models (e.g., `OpenAI`, `GooglePalm`, `HuggingFaceHub`). They take text strings and return text strings.\n",
    "    * **Chat Models:** An optimized interface for conversational interactions, taking a list of messages and returning a message (e.g., `ChatOpenAI`, `ChatGoogleGenerativeAI`).\n",
    "    * **Embeddings:** An interface to generate embeddings from text, representing the semantic meaning of text as numerical vectors (e.g., `OpenAIEmbeddings`, `HuggingFaceEmbeddings`).\n",
    "\n",
    "2.  **Prompts:**\n",
    "    * **Prompt Templates:** Objects that help construct and manage prompts in a structured way, allowing dynamic variables to be inserted into the prompt.\n",
    "    * **Output Parsers:** Help extract and format the LLM's output into desired data structures (e.g., JSON, lists, Pydantic objects).\n",
    "\n",
    "3.  **Chains:**\n",
    "    * Are sequences of components linked together to perform a specific processing flow.\n",
    "    * Each Chain performs a certain task, taking input and producing output.\n",
    "    * Examples: `LLMChain` (connects Prompt and LLM), `SequentialChain` (connects multiple Chains).\n",
    "\n",
    "4.  **Retrieval:**\n",
    "    * **Document Loaders:** Load data from various sources (PDFs, web, CSVs, etc.).\n",
    "    * **Text Splitters:** Split large texts into smaller chunks for efficient processing.\n",
    "    * **Vector Stores:** Specialized databases to store and search embeddings (e.g., FAISS, Chroma, Pinecone).\n",
    "    * **Retrievers:** Objects that help query relevant text chunks from a Vector Store based on a question.\n",
    "\n",
    "5.  **Agents:**\n",
    "    * Are LLMs capable of making decisions and performing actions.\n",
    "    * An Agent uses an LLM as its \"brain\" to plan and decide when and which tool to use.\n",
    "    * **Tools:** Functions or APIs that an Agent can call to interact with the external world (e.g., web search, calculations, custom API calls).\n",
    "    * **Agent Executor:** Executes the actions decided by the Agent.\n",
    "\n",
    "6.  **Memory:**\n",
    "    * Helps maintain the state and context of a conversation across multiple turns.\n",
    "    * Different types of Memory (e.g., `ConversationBufferMemory`, `ConversationSummaryMemory`) store conversation history in various ways.\n",
    "\n",
    "7.  **Callbacks:**\n",
    "    * Provide a mechanism to monitor and control events occurring during LangChain's execution (e.g., start/end of LLM call, Chain, Tool).\n",
    "    * Useful for logging, monitoring, and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Use Cases\n",
    "\n",
    "LangChain enables the construction of many powerful LLM applications:\n",
    "\n",
    "* **Question Answering (QA) Systems:** Answering questions based on your specific documents (RAG).\n",
    "* **Chatbots and Virtual Assistants:** Building chatbots capable of maintaining conversation context and performing actions.\n",
    "* **Document Summarization:** Summarizing long documents or multiple documents.\n",
    "* **Content Generation:** Generating various types of content (articles, emails, scripts).\n",
    "* **Integration with External APIs:** Allowing LLMs to interact with external systems (e.g., scheduling appointments, fetching weather data).\n",
    "* **Structured Data Analysis:** Extracting information from text to populate databases or forms.\n",
    "* **Automated Decision Systems:** Building agents capable of planning and executing complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson delved into the challenges of developing applications directly with LLMs, including complex prompt management, chaining operations, and external data integration. We learned **what LangChain is** and why it is an essential framework for addressing these issues, by providing tools to structure, connect, and extend LLM capabilities. The lesson also presented LangChain's core **design principles** and its **key components** (Models, Prompts, Chains, Retrieval, Agents, Memory, Callbacks), along with the diverse practical **use cases** that LangChain supports. Understanding this architecture and its components is key to starting to build effective LLM applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
