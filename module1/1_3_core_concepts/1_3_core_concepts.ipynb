{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1.3: Core Concepts in LangChain\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lesson will introduce the core components of LangChain, the fundamental \"building blocks\" you'll use to create powerful LLM applications. Understanding each of these concepts is key to grasping how LangChain works and how to combine them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LangChain, \"Models\" are the interfaces for interacting with various AI models, especially Large Language Models (LLMs) and embedding models. LangChain provides a unified interface so you can easily switch between different model providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. LLMs (Large Language Models)\n",
    "\n",
    "* **Concept:** This is the most basic interface for interacting with large language models. They take a text string as input and return a text string as output. LLMs are typically used for text generation, summarization, translation, etc., where both input and output are plain text.\n",
    "* **Relationship:** LLMs are the foundation for many other components in LangChain, such as Chains and Agents.\n",
    "* **Examples:** `OpenAI`, `GooglePalm`, `HuggingFaceHub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Initialize the LLM model\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.7)\n",
    "\n",
    "# Invoke the model\n",
    "text = \"Write a short slogan for an AI startup company.\"\n",
    "response = llm.invoke(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Chat Models\n",
    "\n",
    "* **Concept:** Chat Models are a type of LLM specifically optimized for conversational interactions. Instead of taking a single text string, they take a list of \"messages,\" each with a role (e.g., `system`, `human`, `ai`), and return a message. This helps the model maintain conversation context better.\n",
    "* **Relationship:** Often used in chatbot applications where conversation history needs to be maintained. They work well with LangChain's Memory components.\n",
    "* **Examples:** `ChatOpenAI`, `ChatGoogleGenerativeAI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Initialize Chat Model\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Invoke the model with a list of messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful and friendly AI assistant.\"),\n",
    "    HumanMessage(content=\"Hello, how can I help you?\"),\n",
    "    HumanMessage(content=\"Tell me a short story about a flying cat.\")\n",
    "]\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Embeddings\n",
    "\n",
    "* **Concept:** Embeddings are numerical representations (vectors) of text. Each vector captures the semantic meaning of that text. Texts with similar meanings will have their embedding vectors close to each other in a multi-dimensional space. Embeddings are crucial for semantic search, recommendation, and clustering tasks.\n",
    "* **Relationship:** A core component in information retrieval systems, especially in RAG (Retrieval-Augmented Generation) and Vector Stores.\n",
    "* **Examples:** `OpenAIEmbeddings`, `HuggingFaceEmbeddings`, `GoogleGenerativeAIEmbeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Initialize Embeddings model\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# Create embeddings for text snippets\n",
    "text1 = \"The dog is running in the field.\"\n",
    "text2 = \"A canine is jogging outdoors.\"\n",
    "text3 = \"The sky today is very clear.\"\n",
    "\n",
    "embedding1 = embeddings_model.embed_query(text1)\n",
    "embedding2 = embeddings_model.embed_query(text2)\n",
    "embedding3 = embeddings_model.embed_query(text3)\n",
    "\n",
    "print(f\"Embedding of '{text1}' has length: {len(embedding1)}\")\n",
    "# To check similarity, you can calculate cosine similarity between vectors\n",
    "# (Not directly in LangChain Embeddings, requires a math library like numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts are the input instructions you provide to an LLM to guide its behavior. LangChain offers tools to build and manage Prompts effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Prompt Templates\n",
    "\n",
    "* **Concept:** Prompt Templates are objects that help you construct prompts in a structured and flexible way. They allow you to define a prompt string with \"placeholders\" (variables) that you can fill in later. This aids in prompt reusability and easy dynamic content changes.\n",
    "* **Relationship:** The first component in most Chains, taking input from the user or previous steps and formatting it for the LLM.\n",
    "* **Examples:** `PromptTemplate`, `ChatPromptTemplate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if not already installed\n",
    "# pip install langchain\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "# Example of a basic PromptTemplate\n",
    "string_prompt = PromptTemplate.from_template(\"Tell a short story about {character} in {setting}.\")\n",
    "print(string_prompt.format(character=\"a dragon\", setting=\"an ancient castle\"))\n",
    "\n",
    "# Example of a ChatPromptTemplate (for Chat Models)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a creative assistant, specialized in writing fairy tales.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"Write a story about {character} and {pet}.\"),\n",
    "])\n",
    "print(chat_prompt.format_messages(character=\"Cinderella\", pet=\"a smart mouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Output Parsers\n",
    "\n",
    "* **Concept:** Output Parsers help extract and format the LLM's string output into desired data structures (e.g., JSON, lists, Python objects). LLMs often return free-form text, but in many applications, we need structured data for further processing.\n",
    "* **Relationship:** Often the last component in a Chain, transforming the LLM's output into a usable format for subsequent steps or for the user.\n",
    "* **Examples:** `StrOutputParser`, `CommaSeparatedListOutputParser`, `PydanticOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if not already installed\n",
    "# pip install langchain langchain-openai pydantic\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Example 1: StrOutputParser (default if no other parser is specified)\n",
    "prompt_str = ChatPromptTemplate.from_template(\"Say 'Hello World!'\")\n",
    "chain_str = prompt_str | chat_model | StrOutputParser()\n",
    "print(f\"StrOutputParser: {chain_str.invoke({})}\")\n",
    "\n",
    "# Example 2: CommaSeparatedListOutputParser\n",
    "prompt_list = ChatPromptTemplate.from_template(\"List 3 common fruits, separated by commas.\")\n",
    "chain_list = prompt_list | chat_model | CommaSeparatedListOutputParser()\n",
    "print(f\"CommaSeparatedListOutputParser: {chain_list.invoke({})})\")\n",
    "\n",
    "# Example 3: PydanticOutputParser\n",
    "class BookInfo(BaseModel):\n",
    "    title: str = Field(description=\"The title of the book\")\n",
    "    author: str = Field(description=\"The author's name of the book\")\n",
    "    publication_year: int = Field(description=\"The publication year of the book\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=BookInfo)\n",
    "\n",
    "prompt_pydantic = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an information extraction assistant. Extract the following book information based on the format:\\n{format_instructions}\"),\n",
    "    (\"human\", \"Extract book information from the following text: 'The book 'How to Win Friends and Influence People' by Dale Carnegie, published in 1936, is a classic work on communication skills.'\")\n",
    "]).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain_pydantic = prompt_pydantic | chat_model | parser\n",
    "book_info = chain_pydantic.invoke({})\n",
    "print(f\"PydanticOutputParser: {book_info.title}, {book_info.author}, {book_info.publication_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** Chains are sequences of components linked together to perform a specific processing flow. Each component in the chain (e.g., Prompt Template, LLM, Output Parser) processes the output of the previous component as its input. Chains help you build complex workflows in an organized and manageable way.\n",
    "* **Relationship:** Chains are the primary way to combine Models, Prompts, Parsers, and other components. They are the backbone of most LangChain applications.\n",
    "* **Examples:** `LLMChain`, `SequentialChain`, and chains built using LCEL (LangChain Expression Language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Define Prompt Template\n",
    "prompt = ChatPromptTemplate.from_template(\"Summarize the following paragraph into one sentence: {text}\")\n",
    "\n",
    "# Build the Chain using LCEL (LangChain Expression Language)\n",
    "# This is the modern and flexible way to create chains in LangChain\n",
    "summarize_chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# Execute the Chain\n",
    "long_text = \"LangChain is an open-source framework designed to help developers build applications powered by Large Language Models (LLMs) more easily and efficiently. It provides a set of tools, components, and abstractions to simplify complex processes related to LLMs, from prompt management to connecting LLMs with external data sources and tools.\"\n",
    "summary = summarize_chain.invoke({\"text\": long_text})\n",
    "print(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** Agents are LLMs capable of making decisions and performing actions. Unlike Chains which have a fixed processing flow, Agents use an LLM as their \"brain\" to reason about what action to take next, which tool to use, and when to stop. They can iterate this process until a goal is achieved.\n",
    "* **Relationship:** Agents extend the capabilities of LLMs by allowing them to interact with the external world through Tools. They often use Memory to maintain context in complex conversations.\n",
    "* **Example:** An Agent might search for information on Google, then use a calculator to perform a calculation, and finally answer the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if not already installed\n",
    "# pip install langchain-openai openai wikipedia\n",
    "# Note: To run this example, you need to install SerpAPI or use another search tool\n",
    "# pip install google-search-results # If using SerpAPI\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain import hub\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"YOUR_SERPAPI_API_KEY\" # If using SerpAPI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Define Tools that the Agent can use\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "tools = [wikipedia] # Add other tools if needed, e.g., SearchTool (SerpAPI)\n",
    "\n",
    "# Load prompt for ReAct Agent from LangChain Hub\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# Create Agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# Create Agent Executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Execute Agent\n",
    "print(\"Agent is processing the question...\")\n",
    "response = agent_executor.invoke({\"input\": \"Who is the current president of the United States? What year was he born?\"})\n",
    "print(f\"Agent's answer: {response['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** Memory is the component that helps maintain the state and context of a conversation across multiple turns. LLMs are inherently \"stateless,\" meaning they don't remember previous interactions. Memory solves this by storing conversation history and feeding it back to the LLM in subsequent calls.\n",
    "* **Relationship:** Memory is crucial for chatbot applications and conversational tasks where context is key. It is often integrated into Chains and Agents.\n",
    "* **Examples:** `ConversationBufferMemory`, `ConversationBufferWindowMemory`, `ConversationSummaryMemory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Initialize Chat Model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Initialize Memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create Conversation Chain\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True # To see the conversation history passed to the LLM\n",
    ")\n",
    "\n",
    "# Conduct the conversation\n",
    "print(\"Starting conversation:\")\n",
    "response1 = conversation.predict(input=\"Hello, what is your name?\")\n",
    "print(f\"AI: {response1}\")\n",
    "\n",
    "response2 = conversation.predict(input=\"My name is An. Can you tell me about LangChain?\")\n",
    "print(f\"AI: {response2}\")\n",
    "\n",
    "response3 = conversation.predict(input=\"So what kind of applications can it help me build?\")\n",
    "print(f\"AI: {response3}\")\n",
    "\n",
    "# View conversation history in Memory\n",
    "print(\"\\nConversation history in Memory:\")\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson introduced the core concepts in LangChain, which are the foundation for building complex LLM applications. We learned about **Models** (LLMs, Chat Models, Embeddings) - interfaces for interacting with AI models. **Prompts** (Prompt Templates, Output Parsers) help guide and format LLM input/output. **Chains** are how we connect components to create processing flows. **Agents** enable LLMs to make decisions and perform actions through **Tools**. Finally, **Memory** helps maintain context in long conversations. Mastering these concepts will make it easier for you to design and deploy LangChain-based solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
