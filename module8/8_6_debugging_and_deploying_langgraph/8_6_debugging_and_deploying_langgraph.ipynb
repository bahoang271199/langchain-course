{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8.6: Debugging and Deploying LangGraph\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we built complex Agents and stateful workflows using **LangGraph**. As these systems become more sophisticated, **debugging** and effectively **deploying** them become critically important skills. This lesson will focus on debugging techniques for LangGraph graphs, how to leverage **LangSmith** for visualization, and key considerations when deploying LangGraph applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Debugging Techniques for LangGraph Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging stateful and complex control flow graphs like those in LangGraph can be challenging. Here are some useful techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Tracing State and Execution Flow:**\n",
    "    * **`print` statements:** The simplest way is to add `print` statements at the beginning and end of each Node, as well as printing important values within the graph's **State**. This helps you track how data changes through each step.\n",
    "    * **`verbose=True`:** When initializing `AgentExecutor` (or `Graph` if you're calling it directly), setting `verbose=True` will print out the Agent's reasoning steps, actions, and tool results, providing an overview of the execution flow.\n",
    "    * **Inspecting `state` at each Node:** Within each Node function, you can print the entire `state` object to see exactly what data that Node received and returned. This is especially useful when debugging state merging issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Using Python Debugging Tools:**\n",
    "    * **`pdb` (Python Debugger):** You can insert `import pdb; pdb.set_trace()` anywhere in your Node code to pause execution and inspect variables, stepping through the code.\n",
    "    * **IDE Debugger:** IDEs like VS Code or PyCharm have powerful integrated debuggers that allow you to set breakpoints, inspect variables, and step through the LangGraph graph.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using LangSmith for Visualization and Tracing LangGraph Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangSmith** is a platform developed by LangChain to help developers debug, test, evaluate, and monitor LLM applications. For LangGraph, LangSmith is particularly useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Understanding Agent Steps:**\n",
    "    * LangSmith provides an intuitive user interface to view each execution step of your LangGraph graph. You can clearly see which Nodes were activated, the input/output data of each Node, and the edges that were followed.\n",
    "    * It displays conversation history, LLM calls, tool calls, and intermediate steps (`agent_scratchpad`), making it easy to trace the Agent's reasoning process.\n",
    "* **Performance and Error Analysis:**\n",
    "    * You can view the execution time of each Node and the overall graph, helping to identify performance bottlenecks.\n",
    "    * LangSmith logs errors and exceptions, allowing you to quickly pinpoint the root cause of issues.\n",
    "    * You can compare different runs to observe changes in Agent behavior.\n",
    "* **LangSmith Setup:**\n",
    "    1.  **Install:** `pip install langsmith`\n",
    "    2.  **Set Environment Variables:**\n",
    "        ```bash\n",
    "        export LANGCHAIN_TRACING_V2=\"true\"\n",
    "        export LANGCHAIN_API_KEY=\"<your-langsmith-api-key>\"\n",
    "        export LANGCHAIN_PROJECT=\"<your-project-name>\" # Example: \"LangGraph Debugging\"\n",
    "        ```\n",
    "        Or in Python code:\n",
    "        ```python\n",
    "        import os\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "        os.environ[\"LANGCHAIN_API_KEY\"] = \"<your-langsmith-api-key>\"\n",
    "        os.environ[\"LANGCHAIN_PROJECT\"] = \"<your-project-name>\"\n",
    "        ```\n",
    "    3.  **View Results:** After running your LangGraph application, visit the LangSmith dashboard (app.langsmith.com) to view the traces.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Storing and Loading LangGraph Graphs for Reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've built and tested a LangGraph graph, you might want to save it for reuse without recompiling it from scratch, or for sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Serialization:**\n",
    "    * A LangGraph graph after being `compile()`d is a Python object. You can use the `pickle` or `dill` library to serialize it to a file.\n",
    "    * **Note:** Serializing objects containing LLMs or tools with API connections can be complex. Ensure that API keys or other sensitive information are not stored directly in the serialized file. It's best to store the graph structure and parameters, then re-initialize LLMs/Tools upon loading.\n",
    "    * **Example (conceptual):**\n",
    "        ```python\n",
    "        import pickle\n",
    "        # Assuming 'app' is your compiled graph\n",
    "        with open(\"my_langgraph_app.pkl\", \"wb\") as f:\n",
    "            pickle.dump(app, f)\n",
    "        print(\"Graph saved to my_langgraph_app.pkl\")\n",
    "        ```\n",
    "* **Deserialization:**\n",
    "    * To load the graph, you simply read the serialized file.\n",
    "    * **Example (conceptual):**\n",
    "        ```python\n",
    "        import pickle\n",
    "        with open(\"my_langgraph_app.pkl\", \"rb\") as f:\n",
    "            loaded_app = pickle.load(f)\n",
    "        print(\"Graph loaded from my_langgraph_app.pkl\")\n",
    "        # You can then use loaded_app.invoke(...)\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Considerations for Deploying LangGraph Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying LangGraph applications has its own considerations, especially due to their stateful nature and complex reasoning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Performance:**\n",
    "    * **Latency:** Agentic loops can increase overall latency. Optimize prompts, choose faster LLMs, and reduce unnecessary iterative steps.\n",
    "    * **Throughput:** The number of requests that can be processed per second. Use scalable LLM APIs, optimize Node code, and parallelize when possible.\n",
    "* **Scalability:**\n",
    "    * **Horizontal Scaling:** Deploy multiple instances of your LangGraph application.\n",
    "    * **Distributed State Management:** If you use in-memory state, horizontal scaling will be difficult as each instance will have its own state. A distributed state backend is needed.\n",
    "* **State Management:**\n",
    "    * **In-memory State:** Simple for development and testing, but not suitable for multi-instance production environments.\n",
    "    * **External State Backends:** For production applications, you need to store LangGraph's state in an external system:\n",
    "        * **Redis:** Good for low-latency and ephemeral state.\n",
    "        * **Firestore / DynamoDB:** NoSQL databases suitable for persistent and structured state.\n",
    "        * **PostgreSQL / MongoDB:** Can also be used.\n",
    "    * LangGraph provides integrations for various state backends (e.g., `RedisSaver`).\n",
    "* **Monitoring:**\n",
    "    * In addition to LangSmith, integrate application performance monitoring (APM) tools like Prometheus, Grafana, Datadog to track resource usage, errors, and latency.\n",
    "* **Secrets Management:** Always use cloud provider's dedicated secrets management services (AWS Secrets Manager, GCP Secret Manager, Azure Key Vault) to store API keys.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Debugging a Complex Graph Using Learned Techniques and Understanding How LangSmith Helps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the self-correcting Agent from Lesson 8.5 and add debugging techniques, while discussing how LangSmith visualizes this flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have the necessary libraries installed: `langchain-openai`, `google-search-results`, `numexpr`, `langgraph`, `langsmith`.\n",
    "* Set the `OPENAI_API_KEY`, `SERPAPI_API_KEY`, `LANGCHAIN_API_KEY`, `LANGCHAIN_TRACING_V2=\"true\"`, `LANGCHAIN_PROJECT=\"LangGraph Debugging Example\"` environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai google-search-results numexpr langgraph langsmith\n",
    "\n",
    "import os\n",
    "from typing import TypedDict, Annotated, List, Union, Dict, Any\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "import operator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.tools.calculator.tool import Calculator\n",
    "from langchain_core.agents import AgentFinish, AgentAction # To parse LLM output\n",
    "\n",
    "# Set environment variables for OpenAI and SerpAPI keys\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"YOUR_SERPAPI_API_KEY\"\n",
    "\n",
    "# Set up LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGSMITH_API_KEY\" # Replace with your LangSmith API key\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph Debugging Example\" # Set your project name\n",
    "\n",
    "# --- 1. Define the Agent Graph's State Type ---\n",
    "class AgentState(TypedDict):\n",
    "    chat_history: Annotated[List[BaseMessage], operator.add]\n",
    "    intermediate_steps: Annotated[List[Union[AgentAction, ToolMessage]], operator.add]\n",
    "    retry_count: int\n",
    "\n",
    "# --- 2. Initialize LLM and Tools ---\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    func=SerpAPIWrapper().run,\n",
    "    description=\"Useful when you need to search for information on Google about current events or factual data.\"\n",
    ")\n",
    "calculator_tool = Calculator()\n",
    "tools = [search_tool, calculator_tool]\n",
    "\n",
    "# --- 3. Define the Agent Prompt ---\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. You have access to the following tools: {tools}. Use them to answer user questions. If you have a final answer, respond directly. If you encounter an error with a tool, try again or try a different approach.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# --- 4. Define the Graph Nodes ---\n",
    "\n",
    "def call_llm_node(state: AgentState) -> Dict[str, Any]:\n",
    "    print(\"\\n--- Node: Call LLM (Reasoning/Acting) ---\")\n",
    "    # Print the input state of the Node for debugging\n",
    "    print(f\"  LLM Input State: {state}\")\n",
    "    \n",
    "    messages = agent_prompt.format_messages(\n",
    "        tools=tools,\n",
    "        chat_history=state[\"chat_history\"],\n",
    "        agent_scratchpad=state[\"intermediate_steps\"]\n",
    "    )\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    if \"Final Answer:\" in response.content:\n",
    "        final_answer_content = response.content.split(\"Final Answer:\", 1)[1].strip()\n",
    "        print(f\"  LLM provides final answer: {final_answer_content}\")\n",
    "        return {\"chat_history\": [AIMessage(content=final_answer_content)], \"intermediate_steps\": [AgentFinish(return_values={\"output\": final_answer_content}, log=response.content)]}\n",
    "    \n",
    "    try:\n",
    "        thought_part = \"\"\n",
    "        action_part = \"\"\n",
    "        action_input_part = \"\"\n",
    "\n",
    "        if \"Thought:\" in response.content:\n",
    "            parts = response.content.split(\"Thought:\", 1)\n",
    "            thought_part = parts[1].split(\"Action:\", 1)[0].strip() if \"Action:\" in parts[1] else parts[1].strip()\n",
    "        \n",
    "        if \"Action:\" in response.content:\n",
    "            parts = response.content.split(\"Action:\", 1)[1].split(\"Action Input:\", 1)\n",
    "            action_part = parts[0].strip()\n",
    "            action_input_part = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "\n",
    "        action = AgentAction(tool=action_part, tool_input=action_input_part, log=response.content)\n",
    "        print(f\"  LLM decides to act: Tool='{action.tool}', Input='{action.tool_input}'\")\n",
    "        return {\"intermediate_steps\": [action]}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error parsing LLM output to Action: {e}. LLM Response: {response.content}\")\n",
    "        return {\n",
    "            \"chat_history\": [AIMessage(content=f\"Error parsing my response. Please try again.\")],\n",
    "            \"intermediate_steps\": [AgentFinish(return_values={\"output\": \"LLM parsing error\"}, log=response.content)],\n",
    "            \"retry_count\": state.get(\"retry_count\", 0) + 1\n",
    "        }\n",
    "\n",
    "\n",
    "def call_tool_node(state: AgentState) -> Dict[str, Any]:\n",
    "    print(\"--- Node: Call Tool (Execute tool) ---\")\n",
    "    # Print the input state of the Node for debugging\n",
    "    print(f\"  Tool Input State: {state}\")\n",
    "\n",
    "    last_action = state[\"intermediate_steps\"][-1]\n",
    "    \n",
    "    tool_name = last_action.tool\n",
    "    tool_input = last_action.tool_input\n",
    "\n",
    "    selected_tool = next((t for t in tools if t.name == tool_name), None)\n",
    "    if selected_tool:\n",
    "        try:\n",
    "            tool_output = selected_tool.run(tool_input)\n",
    "            print(f\"  Tool '{tool_name}' returns: {tool_output[:100]}...\")\n",
    "            return {\"intermediate_steps\": [ToolMessage(content=tool_output, tool_call_id=last_action.tool)], \"retry_count\": 0}\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error executing Tool '{tool_name}' with input '{tool_input}': {e}\"\n",
    "            print(f\"  {error_message}\")\n",
    "            return {\"intermediate_steps\": [ToolMessage(content=error_message, tool_call_id=last_action.tool)], \"retry_count\": state.get(\"retry_count\", 0) + 1}\n",
    "    else:\n",
    "        error_message = f\"Error: Tool with name '{tool_name}' not found. Please check tool name again.\"\n",
    "        print(f\"  {error_message}\")\n",
    "        return {\"intermediate_steps\": [ToolMessage(content=error_message, tool_call_id=\"unknown_tool\")], \"retry_count\": state.get(\"retry_count\", 0) + 1}\n",
    "\n",
    "# --- 5. Define the Conditional Edge function ---\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    print(\"--- Node: Should Continue (Decide flow) ---\")\n",
    "    # Print the input state of the Node for debugging\n",
    "    print(f\"  Should Continue Input State: {state}\")\n",
    "\n",
    "    last_step = state[\"intermediate_steps\"][-1]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    MAX_RETRIES = 2\n",
    "\n",
    "    if isinstance(last_step, AgentFinish):\n",
    "        print(\"--- Decision: END (AgentFinish) ---\")\n",
    "        return \"end\"\n",
    "    elif isinstance(last_step, AgentAction):\n",
    "        print(\"--- Decision: CONTINUE (AgentAction) ---\")\n",
    "        return \"continue\"\n",
    "    elif isinstance(last_step, ToolMessage):\n",
    "        if \"Error\" in last_step.content or retry_count > 0:\n",
    "            print(f\"--- Decision: SELF-CORRECT (ToolMessage error, retry attempt {retry_count}) ---\")\n",
    "            if retry_count >= MAX_RETRIES:\n",
    "                print(\"--- Max retries reached. END ---\")\n",
    "                return \"end\"\n",
    "            return \"call_llm\"\n",
    "        else:\n",
    "            print(\"--- Decision: CONTINUE (ToolMessage successful) ---\")\n",
    "            return \"call_llm\"\n",
    "    else:\n",
    "        print(f\"--- Decision: ERROR/UNKNOWN (Unexpected type: {type(last_step)}) ---\")\n",
    "        return \"end\"\n",
    "\n",
    "# --- 6. Build the Agent Graph ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"call_llm\", call_llm_node)\n",
    "workflow.add_node(\"call_tool\", call_tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"call_llm\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_llm\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"call_tool\",\n",
    "        \"end\": END,\n",
    "        \"call_llm\": \"call_llm\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"call_tool\", \"call_llm\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"\\n--- Starting LangGraph Debugging and Deployment Practical ---\")\n",
    "\n",
    "# --- Scenario 1: Question requires search and calculation (normal) ---\n",
    "print(\"\\n--- Scenario 1: Question requires search and calculation (normal) ---\")\n",
    "initial_state_1 = {\"chat_history\": [HumanMessage(content=\"What is the weather like today in London in Celsius? Then multiply the result by 2.\")], \"intermediate_steps\": [], \"retry_count\": 0}\n",
    "final_state_1 = app.invoke(initial_state_1)\n",
    "print(f\"\\nFinal response:\")\n",
    "for message in final_state_1[\"chat_history\"]:\n",
    "    print(f\"{message.type.capitalize()}: {message.content}\")\n",
    "\n",
    "# --- Scenario 2: LLM attempts to call a non-existent tool (self-correction) ---\n",
    "print(\"\\n--- Scenario 2: LLM attempts to call a non-existent tool (self-correction) ---\")\n",
    "# To simulate, we will temporarily change the prompt to make the LLM intentionally try a wrong tool\n",
    "original_prompt = agent_prompt\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. You have access to the following tools: {tools}. Use them to answer user questions. If you have a final answer, respond directly. Try to use the tool 'NonExistentTool' if you are unsure.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "# Need to recompile the app after changing the prompt\n",
    "agent = create_react_agent(llm, tools, agent_prompt)\n",
    "app = StateGraph(AgentState)\n",
    "app.add_node(\"call_llm\", call_llm_node)\n",
    "app.add_node(\"call_tool\", call_tool_node)\n",
    "app.set_entry_point(\"call_llm\")\n",
    "app.add_conditional_edges(\"call_llm\", should_continue, {\"continue\": \"call_tool\", \"end\": END, \"call_llm\": \"call_llm\"})\n",
    "app.add_edge(\"call_tool\", \"call_llm\")\n",
    "app = app.compile()\n",
    "\n",
    "initial_state_2 = {\"chat_history\": [HumanMessage(content=\"Tell me about LangChain.\")], \"intermediate_steps\": [], \"retry_count\": 0}\n",
    "final_state_2 = app.invoke(initial_state_2)\n",
    "print(f\"\\nFinal response:\")\n",
    "for message in final_state_2[\"chat_history\"]:\n",
    "    print(f\"{message.type.capitalize()}: {message.content}\")\n",
    "\n",
    "# Restore original prompt\n",
    "agent_prompt = original_prompt\n",
    "agent = create_react_agent(llm, tools, agent_prompt)\n",
    "app = StateGraph(AgentState)\n",
    "app.add_node(\"call_llm\", call_llm_node)\n",
    "app.add_node(\"call_tool\", call_tool_node)\n",
    "app.set_entry_point(\"call_llm\")\n",
    "app.add_conditional_edges(\"call_llm\", should_continue, {\"continue\": \"call_tool\", \"end\": END, \"call_llm\": \"call_llm\"})\n",
    "app.add_edge(\"call_tool\", \"call_llm\")\n",
    "app = app.compile()\n",
    "\n",
    "print(\"\\n--- End of LangGraph Debugging and Deployment Practical ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
  "nbformat": 4,
  "nbformat_minor": 5
}
