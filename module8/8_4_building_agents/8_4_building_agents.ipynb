{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8.4: Building Agents with LangGraph (Agentic Workflows)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we learned about **Agents** in LangChain, which enable Large Language Models (LLMs) to make decisions and use **Tools** to interact with the external world. We were also introduced to **LangGraph**, a powerful library for building stateful and conditional workflows. This lesson will combine these two concepts, guiding you on how to **build more complex Agents** by leveraging LangGraph's graph structure to implement **Agentic Workflows**, specifically the classic **ReAct (Reasoning and Acting)** loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Integrating LLM and Tools into LangGraph Nodes to Create Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build an Agent in LangGraph, we will consider the LLM and Tools as **Nodes** in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **LLM as a Reasoning Node:** The LLM is the \"brain\" of the Agent. In LangGraph, we will have one or more Nodes whose primary task is to call the LLM. This Node will receive the current state (including conversation history, intermediate steps, etc.), and the LLM will reason to decide:\n",
    "    * What action needs to be taken (which Tool to call with what input)?\n",
    "    * Or is there enough information to provide a final answer?\n",
    "* **Tools as Action Nodes:** Each Tool (e.g., web search, calculator, custom API) will be encapsulated into a separate Node. This Node will receive the necessary input from the state (as decided by the LLM), execute the Tool, and return the observation result to update the state.\n",
    "* **State:** The graph's state will store all necessary information for the Agent, including conversation history, the Agent's intermediate reasoning and action steps (agent scratchpad), and results from Tools.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Basic Agent with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic Agent in LangGraph typically follows an iterative pattern:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  **LLM Reasoning Node (`call_llm`):**\n",
    "    * Receives the current state (including conversation history and previous intermediate steps).\n",
    "    * Uses the LLM to generate a \"Thought\" and an \"Action\" or a \"Final Answer.\"\n",
    "    * Updates the state with the LLM's reasoning and action/answer.\n",
    "    * **Important:** This Node will return a value to control a Conditional Edge, indicating the next step (e.g., \"continue\" to call a Tool, or \"end\" to finish)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  **Tool Calling Node (`call_tool`):**\n",
    "    * Only activated if the LLM decides a Tool needs to be called.\n",
    "    * Receives the state, extracts information about the Tool to call and its input.\n",
    "    * Executes the chosen Tool.\n",
    "    * Updates the state with the \"Observation\" (result of the Tool execution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  **Observation Processing Node (Implicit in `call_llm` or `call_tool`):**\n",
    "    * In the ReAct model, the observation result from the Tool will be fed back to the LLM in the next turn. This is usually handled by adding the Observation to `chat_history` or `agent_scratchpad` in the state, which the LLM Node then receives in its next call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing the Classic ReAct (Reasoning and Acting) Loop with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ReAct loop** is one of the most common Agent architectures, where the LLM alternates between **Reasoning** and **Acting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Reasoning:** The LLM analyzes the request, conversation history, and previous observations to generate a \"Thought\" and decide the next \"Action\" (call a Tool or provide a final answer).\n",
    "* **Acting:** The Agent executes the Tool chosen by the LLM and receives an \"Observation\" (the result).\n",
    "* **Loop:** This Observation is then fed back to the LLM for it to continue reasoning and acting until the goal is achieved.\n",
    "\n",
    "LangGraph is the perfect tool to implement the ReAct loop due to its state management and Conditional Edges capabilities:\n",
    "\n",
    "1.  **State:** The `AgentState` will store `chat_history` (including `HumanMessage`, `AIMessage`, and `ToolMessage` - Observations).\n",
    "2.  **Nodes:**\n",
    "    * One Node for LLM reasoning and decision-making (call Tool or finish).\n",
    "    * One Node for Tool execution.\n",
    "3.  **Conditional Edge:** A conditional function will check the output of the LLM Node to decide whether the Agent should transition to the Tool execution Node or terminate. If it transitions to the Tool Node, after the Tool executes, the flow will return to the LLM Node to continue reasoning (forming a loop).\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical Example: Building an Agent Capable of Web Search and Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a simple LangGraph Agent that can use a web search tool and a calculator tool to answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have the necessary libraries installed: `langchain-openai`, `google-search-results`, `numexpr`, `langgraph`.\n",
    "* Set the `OPENAI_API_KEY` and `SERPAPI_API_KEY` environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai google-search-results numexpr langgraph\n",
    "\n",
    "import os\n",
    "from typing import TypedDict, Annotated, List, Union, Dict, Any\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "import operator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.tools.calculator.tool import Calculator\n",
    "from langchain_core.agents import AgentFinish, AgentAction # To parse LLM output\n",
    "\n",
    "# Set environment variables for OpenAI and SerpAPI keys\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"YOUR_SERPAPI_API_KEY\"\n",
    "\n",
    "# --- 1. Define the Agent Graph's State Type ---\n",
    "# chat_history: Conversation history, will be appended.\n",
    "# intermediate_steps: Agent's intermediate steps (Thought, Action, Observation), also appended.\n",
    "class AgentState(TypedDict):\n",
    "    chat_history: Annotated[List[BaseMessage], operator.add]\n",
    "    intermediate_steps: Annotated[List[Union[AgentAction, ToolMessage]], operator.add]\n",
    "\n",
    "# --- 2. Initialize LLM and Tools ---\n",
    "# Use LLM with low temperature for more consistent responses for the Agent.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Search tool\n",
    "search_tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    func=SerpAPIWrapper().run,\n",
    "    description=\"Hữu ích khi bạn cần tìm kiếm thông tin trên Google về các sự kiện hiện tại hoặc dữ liệu thực tế.\" # Useful when you need to search for information on Google about current events or factual data.\n",
    ")\n",
    "\n",
    "# Calculator tool\n",
    "calculator_tool = Calculator()\n",
    "\n",
    "# Collection of Tools\n",
    "tools = [search_tool, calculator_tool]\n",
    "\n",
    "# --- 3. Define the Agent Prompt (to be used in the LLM Node) ---\n",
    "# This prompt instructs the LLM on how to think and use tools.\n",
    "# It includes chat history and agent_scratchpad for intermediate steps.\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Bạn là một trợ lý hữu ích. Bạn có quyền truy cập vào các công cụ sau: {tools}. Sử dụng chúng để trả lời các câu hỏi của người dùng. Nếu bạn đã có câu trả lời cuối cùng, hãy trả lời trực tiếp.\"), # You are a helpful assistant. You have access to the following tools: {tools}. Use them to answer user questions. If you have a final answer, respond directly.\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # Chat history\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # Thought, Action, Observation steps\n",
    "])\n",
    "\n",
    "# --- 4. Define the Graph Nodes ---\n",
    "\n",
    "# Node 1: Call LLM (ReAct's reasoning/acting part)\n",
    "def call_llm_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    This node calls the LLM to reason about the next step.\n",
    "    The LLM can decide to call a Tool or provide a final answer.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Node: Call LLM (Reasoning/Acting) ---\") # --- Node: Call LLM (Reasoning/Acting) ---\n",
    "    messages = agent_prompt.format_messages(\n",
    "        tools=tools, # Pass the list of tools to the prompt\n",
    "        chat_history=state[\"chat_history\"],\n",
    "        agent_scratchpad=state[\"intermediate_steps\"] # Previous intermediate steps\n",
    "    )\n",
    "    \n",
    "    # Call LLM and parse output\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # LangChain has a built-in parser for Agent, but we'll do it manually\n",
    "    # to illustrate how to check output for AgentAction or AgentFinish\n",
    "    \n",
    "    # If LLM decides to give a final answer\n",
    "    if \"Final Answer:\" in response.content:\n",
    "        final_answer_content = response.content.split(\"Final Answer:\", 1)[1].strip()\n",
    "        # Return AgentFinish to end the graph\n",
    "        return {\"chat_history\": [AIMessage(content=final_answer_content)], \"intermediate_steps\": [AgentFinish(return_values={\"output\": final_answer_content}, log=response.content)]}\n",
    "    \n",
    "    # If LLM decides to call a Tool (parse AgentAction)\n",
    "    try:\n",
    "        # Try to parse the output into AgentAction\n",
    "        # This is a complex part, LLM needs to return a specific format\n",
    "        # Example: \"Thought: ...\\nAction: tool_name\\nAction Input: tool_input\"\n",
    "        # For simplicity, we will assume LLM returns a parsable string\n",
    "        # or you can use a specialized OutputParser.\n",
    "        \n",
    "        # A simple way to extract Action:\n",
    "        thought_match = \"Thought:\"\n",
    "        action_match = \"Action:\"\n",
    "        action_input_match = \"Action Input:\"\n",
    "\n",
    "        if thought_match in response.content and action_match in response.content and action_input_match in response.content:\n",
    "            thought_part = response.content.split(thought_match, 1)[1].split(action_match, 1)[0].strip()\n",
    "            action_part = response.content.split(action_match, 1)[1].split(action_input_match, 1)[0].strip()\n",
    "            action_input_part = response.content.split(action_input_match, 1)[1].strip()\n",
    "\n",
    "            action = AgentAction(tool=action_part, tool_input=action_input_part, log=response.content)\n",
    "            print(f\"  LLM decides to act: Tool='{action.tool}', Input='{action.tool_input}'\") # LLM decides to act:\n",
    "            return {\"intermediate_steps\": [action]}\n",
    "        else:\n",
    "            # If not Final Answer and cannot parse Action, consider it an error or unclear\n",
    "            print(f\"  LLM response unclear, neither Action nor Final Answer: {response.content}\") # LLM response unclear, neither Action nor Final Answer:\n",
    "            return {\"chat_history\": [AIMessage(content=f\"Xin lỗi, tôi không hiểu yêu cầu của bạn. Phản hồi của tôi: {response.content}\")], \"intermediate_steps\": [AgentFinish(return_values={\"output\": \"LLM parsing error\"}, log=response.content)]} # Sorry, I don't understand your request. My response:\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error parsing LLM output to Action: {e}. LLM Response: {response.content}\") # Error parsing LLM output to Action:\n",
    "        # Return an error or end the flow\n",
    "        return {\"chat_history\": [AIMessage(content=f\"Xin lỗi, tôi gặp vấn đề khi xử lý yêu cầu của bạn: {e}\")], \"intermediate_steps\": [AgentFinish(return_values={\"output\": \"LLM parsing error\"}, log=response.content)]} # Sorry, I encountered a problem processing your request:\n",
    "\n",
    "\n",
    "# Node 2: Call Tool (ReAct's acting part)\n",
    "def call_tool_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    This node executes the Tool that the LLM decided on.\n",
    "    \"\"\"\n",
    "    print(\"--- Node: Call Tool (Execute tool) ---\") # --- Node: Call Tool (Execute tool) ---\n",
    "    last_action = state[\"intermediate_steps\"][-1] # Get the last AgentAction\n",
    "    \n",
    "    tool_name = last_action.tool\n",
    "    tool_input = last_action.tool_input\n",
    "\n",
    "    # Find and run the Tool\n",
    "    selected_tool = next((t for t in tools if t.name == tool_name), None)\n",
    "    if selected_tool:\n",
    "        try:\n",
    "            tool_output = selected_tool.run(tool_input)\n",
    "            print(f\"  Tool '{tool_name}' returns: {tool_output[:100]}...\") # Tool '{tool_name}' returns:\n",
    "            # Return ToolMessage to add to intermediate_steps (Observation)\n",
    "            return {\"intermediate_steps\": [ToolMessage(content=tool_output, tool_call_id=last_action.tool)]}\n",
    "        except Exception as e:\n",
    "            error_message = f\"Lỗi khi thực thi Tool '{tool_name}' với đầu vào '{tool_input}': {e}\" # Error executing Tool '{tool_name}' with input '{tool_input}':\n",
    "            print(f\"  {error_message}\")\n",
    "            return {\"intermediate_steps\": [ToolMessage(content=error_message, tool_call_id=last_action.tool)]}\n",
    "    else:\n",
    "        error_message = f\"Không tìm thấy Tool: {tool_name}\" # Tool not found:\n",
    "        print(f\"  {error_message}\")\n",
    "        return {\"intermediate_steps\": [ToolMessage(content=error_message, tool_call_id=\"unknown_tool\")]}\n",
    "\n",
    "# --- 5. Define the Conditional Edge function (decide to continue loop or end) ---\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    This function checks the last intermediate step to decide the next flow.\n",
    "    If it's an AgentFinish, end. If it's an AgentAction, call the Tool.\n",
    "    \"\"\"\n",
    "    last_step = state[\"intermediate_steps\"][-1]\n",
    "    if isinstance(last_step, AgentFinish):\n",
    "        print(\"--- Decision: END (AgentFinish) ---\") # --- Decision: END (AgentFinish) ---\n",
    "        return \"end\" # End the graph\n",
    "    elif isinstance(last_step, AgentAction):\n",
    "        print(\"--- Decision: CONTINUE (AgentAction) ---\") # --- Decision: CONTINUE (AgentAction) ---\n",
    "        return \"continue\" # Continue the loop (call Tool)\n",
    "    else:\n",
    "        # Unexpected case, could be a ToolMessage without a preceding AgentAction\n",
    "        # Or an invalid state.\n",
    "        print(f\"--- Decision: ERROR/UNKNOWN (Unexpected type: {type(last_step)}) ---\") # --- Decision: ERROR/UNKNOWN (Unexpected type:\n",
    "        return \"end\" # End to prevent infinite loop\n",
    "\n",
    "# --- 6. Build the Agent Graph ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"call_llm\", call_llm_node)\n",
    "workflow.add_node(\"call_tool\", call_tool_node)\n",
    "\n",
    "# Set entry point: Always start by calling the LLM to reason\n",
    "workflow.set_entry_point(\"call_llm\")\n",
    "\n",
    "# Define conditional edge from LLM Node\n",
    "# LLM will decide whether to continue by calling a Tool or end\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_llm\", # Node from which we branch\n",
    "    should_continue, # Function that decides the path\n",
    "    {\n",
    "        \"continue\": \"call_tool\", # If 'continue', go to call_tool\n",
    "        \"end\": END # If 'end', end the graph\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define edge from Tool Node back to LLM Node\n",
    "# After the Tool is called, the flow returns to the LLM to continue reasoning with the Tool result\n",
    "workflow.add_edge(\"call_tool\", \"call_llm\")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"\\n--- Starting Agent with LangGraph Practical ---\") # --- Starting Agent with LangGraph Practical ---\n",
    "\n",
    "# --- Scenario 1: Question requires search and calculation ---\n",
    "print(\"\\n--- Scenario 1: Question requires search and calculation ---\") # --- Scenario 1: Question requires search and calculation ---\n",
    "initial_state_1 = {\"chat_history\": [HumanMessage(content=\"Thời tiết hôm nay ở London là bao nhiêu độ C? Sau đó nhân kết quả với 2.\")]} # What is the weather like today in London in Celsius? Then multiply the result by 2.\n",
    "final_state_1 = app.invoke(initial_state_1)\n",
    "print(f\"\\nFinal response:\") # Final response:\n",
    "for message in final_state_1[\"chat_history\"]:\n",
    "    print(f\"{message.type.capitalize()}: {message.content}\")\n",
    "\n",
    "# --- Scenario 2: Question can be answered directly ---\n",
    "print(\"\\n--- Scenario 2: Question can be answered directly ---\") # --- Scenario 2: Question can be answered directly ---\n",
    "initial_state_2 = {\"chat_history\": [HumanMessage(content=\"Thủ đô của Pháp là gì?\")]} # What is the capital of France?\n",
    "final_state_2 = app.invoke(initial_state_2)\n",
    "print(f\"\\nFinal response:\") # Final response:\n",
    "for message in final_state_2[\"chat_history\"]:\n",
    "    print(f\"{message.type.capitalize()}: {message.content}\")\n",
    "\n",
    "# --- Scenario 3: Simple calculation question ---\n",
    "print(\"\\n--- Scenario 3: Simple calculation question ---\") # --- Scenario 3: Simple calculation question ---\n",
    "initial_state_3 = {\"chat_history\": [HumanMessage(content=\"Tính 150 chia 3.\")]} # Calculate 150 divided by 3.\n",
    "final_state_3 = app.invoke(initial_state_3)\n",
    "print(f\"\\nFinal response:\") # Final response:\n",
    "for message in final_state_3[\"chat_history\"]:\n",
    "    print(f\"{message.type.capitalize()}: {message.content}\")\n",
    "\n",
    "print(\"\\n--- End of Agent with LangGraph Practical ---\") # --- End of Agent with LangGraph Practical ---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
  "nbformat": 4,
  "nbformat_minor": 5
}
