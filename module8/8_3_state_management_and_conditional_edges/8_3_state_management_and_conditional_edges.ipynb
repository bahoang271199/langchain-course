{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8.3: State Management and Conditional Edges\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lesson 8.2, we built a simple sequential graph with LangGraph. However, the true power of LangGraph lies in its ability to manage complex states and create non-linear control flows. This lesson will delve into how data is passed and updated within the graph, as well as how to use **Conditional Edges** to create logical branches based on conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concept of State in LangGraph: How data is passed and updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **State** is the heart of every LangGraph graph. It is a single, mutable data object that is passed from one Node to another throughout the graph's execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Data Passing:** Each **Node** in the graph receives the current copy of the state as input.\n",
    "* **Data Updates:** After performing its logic, a Node will return a **dictionary** containing updates to the state. LangGraph will automatically **merge** these updates into the overall state.\n",
    "* **Merge Mechanism:** This is a crucial aspect. When you define your `State Schema` using `TypedDict` (which will be covered in more detail later), you can specify how new values for each field will be combined with existing values.\n",
    "    * **Default Overwrite:** If nothing is specified, the new value will overwrite the old one.\n",
    "    * **`operator.add`:** For lists, `operator.add` (or `list.append`) is often used to append new elements to the existing list, rather than completely replacing the list. This is very useful for `chat_history`.\n",
    "    * **Custom Merging:** You can define custom merge functions for more complex data types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using TypedDict to Define the Graph's State Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the structure of the state clearly and type-safely, LangGraph recommends using `TypedDict` from Python's `typing` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Syntax:** You create a class that inherits from `TypedDict` and declare the fields along with their data types.\n",
    "* **`Annotated` and `operator.add`:** To specify a merge mechanism for a particular field (e.g., `chat_history`), you use `Annotated` along with a merge function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "import operator\n",
    "\n",
    "# Define a more complex state structure\n",
    "class AgentState(TypedDict):\n",
    "    # Chat history: List of messages, will be appended\n",
    "    chat_history: Annotated[List[BaseMessage], operator.add]\n",
    "    # Query classification: String (e.g., \"SEARCH_NEEDED\", \"DIRECT_ANSWER\")\n",
    "    query_classification: str\n",
    "    # Search results: String containing results from the search tool\n",
    "    search_results: str\n",
    "    # Other temporary variables\n",
    "    temp_data: Annotated[List[str], operator.add]\n",
    "    # Flag to control loops (e.g., True/False)\n",
    "    should_continue: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "* `chat_history` will have new messages appended.\n",
    "* `query_classification` and `search_results` will be overwritten each time a Node returns a new value for them (since no special merge mechanism is specified).\n",
    "* `temp_data` will also be appended.\n",
    "* `should_continue` will be overwritten.\n",
    "\n",
    "Defining the state clearly helps you easily track data and ensure that Nodes only access and update the parts of the data they need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conditional Edges: Creating branches in the graph based on results or conditions from a Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-world Agent applications, the processing flow is rarely just sequential. Agents need the ability to make decisions and branch based on conditions. **Conditional Edges** are LangGraph's mechanism for achieving this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** Instead of going from Node A to Node B in a fixed manner, a Node can return a value (typically a string) that LangGraph uses to decide the next Node.\n",
    "* **How it Works:**\n",
    "    1.  A Node performs processing and returns a string (e.g., \"search\", \"answer\", \"loop\").\n",
    "    2.  LangGraph uses this string value to look up in a defined mapping.\n",
    "    3.  This mapping indicates the next Node corresponding to that string value.\n",
    "    4.  If the Node returns `END`, the graph will terminate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using `add_conditional_edges` to route flow based on logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `add_conditional_edges` method is how you define conditional branches in LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Syntax:**\n",
    "    ```python\n",
    "    graph_builder.add_conditional_edges(\n",
    "        start_node: str,\n",
    "        condition: Callable[[State], str],\n",
    "        path_map: Dict[str, str]\n",
    "    )\n",
    "    ```\n",
    "    * `start_node`: The name of the Node from which you want to create conditional edges.\n",
    "    * `condition`: A Python function that takes the current graph state and returns a string (the name of the target Node) or `END`. This function contains the decision-making logic.\n",
    "    * `path_map`: A dictionary mapping the return values from the `condition` function to the actual target Node names in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Example of `condition` function:**\n",
    "    ```python\n",
    "    from langgraph.graph import END\n",
    "\n",
    "    def route_next_step(state: AgentState) -> str:\n",
    "        \"\"\"\n",
    "        This function decides the next step based on 'query_classification' in the state.\n",
    "        \"\"\"\n",
    "        if state[\"query_classification\"] == \"SEARCH_NEEDED\":\n",
    "            return \"perform_search_node\"\n",
    "        elif state[\"query_classification\"] == \"DIRECT_ANSWER\":\n",
    "            return \"generate_final_answer_node\"\n",
    "        else:\n",
    "            return END # End if no condition matches (or error handling)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Example of `add_conditional_edges`:**\n",
    "    ```python\n",
    "    workflow.add_conditional_edges(\n",
    "        \"classify_query_node\", # Node from which we branch\n",
    "        route_next_step,      # Function that decides the path\n",
    "        {\n",
    "            \"SEARCH_NEEDED\": \"perform_search_node\",\n",
    "            \"DIRECT_ANSWER\": \"generate_final_answer_node\",\n",
    "            END: END # If route_next_step function returns END, the graph ends\n",
    "        }\n",
    "    )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Building a Graph with Branching Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a simple chatbot graph that can branch:\n",
    "* If the question requires external information, it will go through a search Node.\n",
    "* If the question can be answered directly, it will skip the search Node and answer immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have `langchain-openai`, `google-search-results` installed.\n",
    "* Set the `OPENAI_API_KEY` and `SERPAPI_API_KEY` environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt thư viện nếu chưa có\n",
    "# pip install langchain-openai openai google-search-results\n",
    "\n",
    "import os\n",
    "from typing import TypedDict, Annotated, List, Dict, Any\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "import operator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain.tools import Tool\n",
    "\n",
    "# Thiết lập biến môi trường cho khóa API của OpenAI và SerpAPI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"YOUR_SERPAPI_API_KEY\"\n",
    "\n",
    "# --- 1. Định nghĩa kiểu trạng thái cho đồ thị ---\n",
    "class AgentState(TypedDict):\n",
    "    chat_history: Annotated[List[BaseMessage], operator.add] # Lịch sử trò chuyện\n",
    "    initial_query: str # Giữ câu hỏi gốc của người dùng\n",
    "    query_classification: str # \"SEARCH_NEEDED\" hoặc \"DIRECT_ANSWER\"\n",
    "    search_results: str # Kết quả từ công cụ tìm kiếm\n",
    "\n",
    "# --- 2. Khởi tạo LLM và Tools ---\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Tool tìm kiếm\n",
    "search_tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    func=SerpAPIWrapper().run,\n",
    "    description=\"Hữu ích khi bạn cần tìm kiếm thông tin trên Google về các sự kiện hiện tại hoặc dữ liệu thực tế.\" # Useful when you need to search for information on Google about current events or factual data.\n",
    ")\n",
    "\n",
    "# --- 3. Định nghĩa các Node ---\n",
    "\n",
    "# Node 1: Lấy đầu vào ban đầu\n",
    "def initial_input_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Node này lấy tin nhắn Human mới nhất và lưu vào initial_query.\"\"\"\n",
    "    print(\"\\n--- Node: Initial Input (Lấy câu hỏi gốc) ---\") # --- Node: Initial Input (Get original question) ---\n",
    "    last_message = state[\"chat_history\"][-1]\n",
    "    return {\"initial_query\": last_message.content}\n",
    "\n",
    "# Node 2: Phân loại truy vấn (LLM quyết định)\n",
    "def classify_query_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Node này sử dụng LLM để phân loại xem câu hỏi có cần tìm kiếm hay không.\n",
    "    Trả về \"SEARCH_NEEDED\" hoặc \"DIRECT_ANSWER\".\n",
    "    \"\"\"\\n\n",
    "    print(\"--- Node: Classify Query (Phân loại câu hỏi) ---\") # --- Node: Classify Query (Classify question) ---\n",
    "    query = state[\"initial_query\"]\n",
    "    classification_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Phân loại câu hỏi sau đây: \"{query}\".\n",
    "        Trả lời \"SEARCH_NEEDED\" nếu câu hỏi cần tìm kiếm thông tin bên ngoài (ví dụ: dữ kiện, tin tức, thời tiết, số liệu cụ thể).\n",
    "        Trả lời \"DIRECT_ANSWER\" nếu câu hỏi có thể được trả lời trực tiếp bằng kiến thức chung của bạn hoặc không cần tìm kiếm.\n",
    "        Chỉ trả lời \"SEARCH_NEEDED\" hoặc \"DIRECT_ANSWER\".\"\"\" # Classify the following question: \"{query}\". Reply \"SEARCH_NEEDED\" if the question requires external information (e.g., facts, news, weather, specific data). Reply \"DIRECT_ANSWER\" if the question can be answered directly using your general knowledge or does not require a search. Only reply \"SEARCH_NEEDED\" or \"DIRECT_ANSWER\".\n",
    "    )\n",
    "    chain = classification_prompt | llm | StrOutputParser()\n",
    "    classification = chain.invoke({\"query\": query}).strip().upper()\n",
    "    print(f\"  Phân loại: {classification}\") #   Classification:\n",
    "    return {\"query_classification\": classification}\n",
    "\n",
    "# Node 3: Thực hiện tìm kiếm (nếu cần)\n",
    "def perform_search_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Node này thực hiện tìm kiếm bằng công cụ tìm kiếm.\"\"\"\n",
    "    print(\"--- Node: Perform Search (Thực hiện tìm kiếm) ---\") # --- Node: Perform Search (Perform search) ---\n",
    "    query = state[\"initial_query\"]\n",
    "    search_result = search_tool.run(query)\n",
    "    print(f\"  Kết quả tìm kiếm: {search_result[:100]}...\") #   Search result:\n",
    "    return {\"search_results\": search_result}\n",
    "\n",
    "# Node 4: Tạo câu trả lời cuối cùng\n",
    "def generate_final_answer_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Node này tạo câu trả lời cuối cùng dựa trên câu hỏi và kết quả tìm kiếm (nếu có).\"\"\"\n",
    "    print(\"--- Node: Generate Final Answer (Tạo câu trả lời cuối cùng) ---\") # --- Node: Generate Final Answer (Generate final answer) ---\n",
    "    query = state[\"initial_query\"]\n",
    "    search_results = state.get(\"search_results\", \"Không có thông tin tìm kiếm.\") # Lấy kết quả tìm kiếm nếu có # No search information.\n",
    "    \n",
    "    answer_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Bạn là một trợ lý hữu ích. Trả lời câu hỏi sau đây. Nếu có kết quả tìm kiếm, hãy sử dụng chúng. Nếu không, trả lời dựa trên kiến thức chung của bạn.\"), # You are a helpful assistant. Answer the following question. If search results are available, use them. Otherwise, answer based on your general knowledge.\n",
    "        (\"human\", f\"Câu hỏi: {query}\\n\\nKết quả tìm kiếm: {search_results}\"), # Question: {query}\\n\\nSearch Results: {search_results}\n",
    "    ])\n",
    "    chain = answer_prompt | llm | StrOutputParser()\n",
    "    final_answer = chain.invoke({\"question\": query, \"search_results\": search_results})\n",
    "    \n",
    "    # Thêm câu trả lời của AI vào chat_history\n",
    "    return {\"chat_history\": [AIMessage(content=final_answer)]}\n",
    "\n",
    "# --- 4. Định nghĩa hàm điều kiện cho Conditional Edge ---\n",
    "def route_next_step(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Hàm này quyết định Node tiếp theo dựa trên 'query_classification'.\n",
    "    \"\"\"\n",
    "    if state[\"query_classification\"] == \"SEARCH_NEEDED\":\n",
    "        return \"perform_search_node\"\n",
    "    elif state[\"query_classification\"] == \"DIRECT_ANSWER\":\n",
    "        return \"generate_final_answer_node\"\n",
    "    else:\n",
    "        # Trường hợp không mong muốn, có thể chuyển đến Node xử lý lỗi hoặc kết thúc\n",
    "        print(f\"  Phân loại không xác định: {state['query_classification']}. Kết thúc.\") # Unknown classification: {state['query_classification']}. Ending.\n",
    "        return END\n",
    "\n",
    "# --- 5. Xây dựng đồ thị ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Thêm các Node\n",
    "workflow.add_node(\"initial_input\", initial_input_node)\n",
    "workflow.add_node(\"classify_query\", classify_query_node)\n",
    "workflow.add_node(\"perform_search_node\", perform_search_node)\n",
    "workflow.add_node(\"generate_final_answer_node\", generate_final_answer_node)\n",
    "\n",
    "# Đặt điểm bắt đầu\n",
    "workflow.set_entry_point(\"initial_input\")\n",
    "\n",
    "# Định nghĩa cạnh cố định từ initial_input đến classify_query\n",
    "workflow.add_edge(\"initial_input\", \"classify_query\")\n",
    "\n",
    "# Định nghĩa cạnh có điều kiện từ classify_query\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_query\", # Node mà từ đó chúng ta rẽ nhánh\n",
    "    route_next_step,  # Hàm quyết định đường đi\n",
    "    {\n",
    "        \"SEARCH_NEEDED\": \"perform_search_node\",    # Nếu \"SEARCH_NEEDED\", đi đến perform_search_node\n",
    "        \"DIRECT_ANSWER\": \"generate_final_answer_node\", # Nếu \"DIRECT_ANSWER\", đi đến generate_final_answer_node\n",
    "        END: END # Nếu hàm route_next_step trả về END, thì đồ thị kết thúc\n",
    "    }\n",
    ")\n",
    "\n",
    "# Định nghĩa cạnh cố định từ perform_search_node đến generate_final_answer_node\n",
    "workflow.add_edge(\"perform_search_node\", \"generate_final_answer_node\")\n",
    "\n",
    "# Đặt điểm kết thúc\n",
    "workflow.set_finish_point(\"generate_final_answer_node\")\n",
    "\n",
    "# Biên dịch đồ thị\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"\\n--- Bắt đầu thực hành đồ thị có logic phân nhánh ---\") # --- Starting practical: graph with branching logic ---\n",
    "\n",
    "# --- Tình huống 1: Câu hỏi cần tìm kiếm ---\n",
    "print(\"\\n--- Tình huống 1: Câu hỏi cần tìm kiếm ---\") # --- Scenario 1: Question requires search ---\n",
    "initial_state_1 = {\"chat_history\": [HumanMessage(content=\"Ai là tổng thống hiện tại của Hoa Kỳ?\")]} # Who is the current president of the United States?\n",
    "final_state_1 = app.invoke(initial_state_1)\n",
    "print(f\"\\nPhản hồi cuối cùng:\") # Final response:\n",
    "for message in final_state_1[\"chat_history\"]:\n",
    "    print(f\"{message.type.capitalize()}: {message.content}\")\n",
    "\n",
    "# --- Tình huống 2: Câu hỏi có thể trả lời trực tiếp ---\n",
    "print(\"\\n--- Tình huống 2: Câu hỏi có thể trả lời trực tiếp ---\") # --- Scenario 2: Question can be answered directly ---\n",
    "initial_state_2 = {\"chat_history\": [HumanMessage(content=\"Mặt trời có màu gì?\")]} # What color is the sun?\n",
    "final_state_2 = app.invoke(initial_state_2)\n",
    "print(f\"\\nPhản hồi cuối cùng:\") # Final response:\n",
    "for message in final_state_2[\"chat_history\"]:\n",
    "    print(f\"{message.type.capitalize()}: {message.content}\")\n",
    "\n",
    "print(\"\\n--- Kết thúc thực hành ---\") # --- End of practical ---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
