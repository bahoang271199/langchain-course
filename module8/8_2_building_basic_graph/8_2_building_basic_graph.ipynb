{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8.2: Building Basic Graphs with LangGraph\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lesson 8.1, we learned about the concept and importance of **LangGraph** in building complex, multi-step, stateful Agent applications. This lesson will dive into practical implementation, guiding you on how to **build a basic graph** in LangGraph, from defining the state to connecting processing steps (Nodes) with control flows (Edges)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initializing a StateGraph and Defining the State Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every graph in LangGraph starts with defining the **state** it will manage and pass between Nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`StateGraph`:** Is the base class for building graphs. When initializing `StateGraph`, you need to pass a **State Schema**.\n",
    "* **State Schema:**\n",
    "    * Defines the data structure of the state.\n",
    "    * Typically defined by inheriting from `TypedDict` (a Python feature for creating dictionaries with specific data types).\n",
    "    * Each key in the `TypedDict` will be a field in the graph's state.\n",
    "    * You also need to specify how new values for each field will be **merged** into the existing state (e.g., `operator.add` for string concatenation, `list.append` for appending to a list, or simply overwriting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Dict, Any\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph\n",
    "import operator\n",
    "\n",
    "# Define the state type for our graph\n",
    "# Annotated[List[BaseMessage], operator.add] means:\n",
    "# - chat_history is a list of BaseMessage objects.\n",
    "# - When a Node returns an update for chat_history, it will be APPENDED (add) to the existing list,\n",
    "#   not completely overwritten.\n",
    "class AgentState(TypedDict):\n",
    "    chat_history: Annotated[List[BaseMessage], operator.add]\n",
    "    # You can add other fields to the state, e.g.:\n",
    "    # current_query: str\n",
    "    # tool_output: str\n",
    "    # final_answer: str\n",
    "\n",
    "# Initialize StateGraph with the defined state type\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "print(\"StateGraph initialized with AgentState.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining Nodes (functions or LangChain Runnables) in the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nodes** are the processing steps in your graph. Each Node receives the current graph state, performs some logic, and returns an update to that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Python Function:** A Node can be a regular Python function. This function must accept the current state as its first argument and return a dictionary containing state updates.\n",
    "* **LangChain Runnable:** A Node can also be a `Runnable` (like an LLM, Chain, Agent) from LangChain. LangGraph will automatically handle passing the state in and receiving the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Node 1: Preprocessing input\n",
    "def preprocess_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    This node takes chat_history, gets the latest message, and preprocesses it.\n",
    "    Returns an update to the state.\n",
    "    \"\"\"\n",
    "    print(\"--- Running Node: Preprocessing ---\")\n",
    "    last_message = state[\"chat_history\"][-1]\n",
    "    # Simulate preprocessing: convert Human message to a simple string\n",
    "    processed_input = last_message.content.strip().lower()\n",
    "    # Return a dictionary to update the state\n",
    "    return {\"chat_history\": [HumanMessage(content=f\"Processed: {processed_input}\")]}\n",
    "\n",
    "# Node 2: LLM Call\n",
    "# We'll create a simple Chain to act as the LLM Node\n",
    "llm_chain = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer the following question:\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "]) | llm | StrOutputParser()\n",
    "\n",
    "def llm_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    This node calls the LLM with the preprocessed message.\n",
    "    Returns the AI's response to update chat_history.\n",
    "    \"\"\"\n",
    "    print(\"--- Running Node: LLM Call ---\")\n",
    "    # Get the preprocessed message (or original message)\n",
    "    input_message = state[\"chat_history\"][-1].content\n",
    "    response = llm_chain.invoke({\"question\": input_message})\n",
    "    # Return the AI's response to add to chat_history\n",
    "    return {\"chat_history\": [AIMessage(content=response)]}\n",
    "\n",
    "# Node 3: Postprocessing\n",
    "def postprocess_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    This node receives the AI's response and performs postprocessing.\n",
    "    Returns an update to the state.\n",
    "    \"\"\"\n",
    "    print(\"--- Running Node: Postprocessing ---\")\n",
    "    last_ai_message = state[\"chat_history\"][-1]\n",
    "    # Simulate postprocessing: add an exclamation mark\n",
    "    final_response = last_ai_message.content + \" (Processed!)\"\n",
    "    # Update the last AI message in history\n",
    "    # Note: If you want to overwrite the previous AI message, you need more complex logic\n",
    "    # or redefine the merge behavior for chat_history.\n",
    "    # Here, we will add a new message for illustration.\n",
    "    return {\"chat_history\": [AIMessage(content=final_response)]}\n",
    "\n",
    "print(\"Defined nodes: preprocess_node, llm_node, postprocess_node.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining Edges (Control Flow) Between Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edges** define the control flow between Nodes. They tell LangGraph which Node will run after which."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`add_node(name, node_function)`:** Adds a Node to the graph with a unique name and its corresponding function/Runnable.\n",
    "* **`add_edge(start_node_name, end_node_name)`:** Adds a fixed edge from `start_node_name` to `end_node_name`. Control flow will always follow this edge.\n",
    "* **`set_entry_point(node_name)`:** Sets the Node where the graph will begin execution.\n",
    "* **`set_finish_point(node_name)`:** Sets the Node where the graph will end execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to the workflow\n",
    "workflow.add_node(\"preprocess\", preprocess_node)\n",
    "workflow.add_node(\"llm_process\", llm_node)\n",
    "workflow.add_node(\"postprocess\", postprocess_node)\n",
    "\n",
    "# Define sequential edges\n",
    "workflow.add_edge(\"preprocess\", \"llm_process\")\n",
    "workflow.add_edge(\"llm_process\", \"postprocess\")\n",
    "\n",
    "# Set entry and finish points\n",
    "workflow.set_entry_point(\"preprocess\")\n",
    "workflow.set_finish_point(\"postprocess\")\n",
    "\n",
    "print(\"Nodes and Edges added to workflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Simple Sequential Graph and Running It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining all Nodes and Edges, you need to \"compile\" the graph into a runnable `Runnable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`workflow.compile()`:** This method transforms the defined graph into a callable object, ready to receive input and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"Graph compiled.\")\n",
    "\n",
    "# Run the graph with an initial input\n",
    "initial_state = {\"chat_history\": [HumanMessage(content=\"What's the weather like today?\")]}\n",
    "print(\"\\n--- Running graph 1st time ---\")\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print(\"\\n--- Final state of graph 1st run ---\")\n",
    "for message in final_state[\"chat_history\"]:\n",
    "    print(f\"{message.type.capitalize()}: {message.content}\")\n",
    "\n",
    "# Run the graph a 2nd time with a different input\n",
    "initial_state_2 = {\"chat_history\": [HumanMessage(content=\"What is LangGraph?\")]}\n",
    "print(\"\\n--- Running graph 2nd time ---\")\n",
    "final_state_2 = app.invoke(initial_state_2)\n",
    "\n",
    "print(\"\\n--- Final state of graph 2nd run ---\")\n",
    "for message in final_state_2[\"chat_history\"]:\n",
    "    print(f\"{message.type.capitalize()}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Creating a Multi-step Text Processing Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the above concepts to create a simple text processing graph:\n",
    "**Preprocess -> LLM Process -> Postprocess**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire code has been presented in the sections above.\n",
    "# Below is how you can run the entire example seamlessly.\n",
    "\n",
    "import os\n",
    "from typing import TypedDict, Annotated, List, Dict, Any\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph\n",
    "import operator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 1. Define the state type\n",
    "class TextProcessingState(TypedDict):\n",
    "    input_text: str\n",
    "    processed_text: str\n",
    "    llm_response: str\n",
    "    final_output: str\n",
    "\n",
    "# 2. Initialize StateGraph\n",
    "text_workflow = StateGraph(TextProcessingState)\n",
    "\n",
    "# 3. Initialize LLM\n",
    "llm_for_text_processing = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "# 4. Define the Nodes\n",
    "def initial_input_node(state: TextProcessingState) -> Dict[str, Any]:\n",
    "    \"\"\"Initial node, just takes input_text from the initial state.\"\"\"\n",
    "    print(\"--- Node: Initial Input ---\")\n",
    "    return {\"input_text\": state[\"input_text\"]} # Ensure input_text is passed through\n",
    "\n",
    "def preprocess_text_node(state: TextProcessingState) -> Dict[str, Any]:\n",
    "    \"\"\"Node for text preprocessing.\"\"\"\n",
    "    print(\"--- Node: Preprocessing Text ---\")\n",
    "    text = state[\"input_text\"]\n",
    "    processed_text = text.strip().replace(\"  \", \" \").lower() # Example: remove extra spaces, convert to lowercase\n",
    "    return {\"processed_text\": processed_text}\n",
    "\n",
    "def llm_summarize_node(state: TextProcessingState) -> Dict[str, Any]:\n",
    "    \"\"\"Node that calls the LLM to summarize the preprocessed text.\"\"\"\n",
    "    print(\"--- Node: LLM Summarization ---\")\n",
    "    text_to_summarize = state[\"processed_text\"]\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "        \"Summarize the following text concisely (around 20 words): {text}\"\n",
    "    )\n",
    "    chain = prompt_template | llm_for_text_processing | StrOutputParser()\n",
    "    summary = chain.invoke({\"text\": text_to_summarize})\n",
    "    return {\"llm_response\": summary}\n",
    "\n",
    "def postprocess_output_node(state: TextProcessingState) -> Dict[str, Any]:\n",
    "    \"\"\"Node for postprocessing the final output.\"\"\"\n",
    "    print(\"--- Node: Postprocessing Output ---\")\n",
    "    llm_response = state[\"llm_response\"]\n",
    "    final_output = f\"Final Summary: \\\"{llm_response}\\\"\"\n",
    "    return {\"final_output\": final_output}\n",
    "\n",
    "# 5. Add Nodes to the graph\n",
    "text_workflow.add_node(\"initial_input\", initial_input_node)\n",
    "text_workflow.add_node(\"preprocess_text\", preprocess_text_node)\n",
    "text_workflow.add_node(\"llm_summarize\", llm_summarize_node)\n",
    "text_workflow.add_node(\"postprocess_output\", postprocess_output_node)\n",
    "\n",
    "# 6. Define sequential Edges\n",
    "text_workflow.add_edge(\"initial_input\", \"preprocess_text\")\n",
    "text_workflow.add_edge(\"preprocess_text\", \"llm_summarize\")\n",
    "text_workflow.add_edge(\"llm_summarize\", \"postprocess_output\")\n",
    "\n",
    "# 7. Set entry and finish points\n",
    "text_workflow.set_entry_point(\"initial_input\")\n",
    "text_workflow.set_finish_point(\"postprocess_output\")\n",
    "\n",
    "# 8. Compile the graph\n",
    "text_app = text_workflow.compile()\n",
    "\n",
    "print(\"\\n--- Practical: Running a Multi-step Text Processing Graph ---\")\n",
    "input_data = {\"input_text\": \"   LangGraph is an extension   of LangChain. It helps build complex and stateful Agents.   \"}\n",
    "final_result = text_app.invoke(input_data)\n",
    "\n",
    "print(\"\\n--- Final State of Text Processing Graph ---\")\n",
    "print(f\"Input Text: {final_result['input_text']}\")\n",
    "print(f\"Processed Text: {final_result['processed_text']}\")\n",
    "print(f\"LLM Response (Summary): {final_result['llm_response']}\")\n",
    "print(f\"Final Output: {final_result['final_output']}\")\n",
    "\n",
    "print(\"\\n--- End of Practical ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
