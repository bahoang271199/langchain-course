{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3.3: Embeddings and Vector Stores\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lessons, we learned how to load and split large documents into smaller chunks. The next step to transform these chunks into retrievable knowledge for Large Language Models (LLMs) is to convert them into numerical representations. This lesson will introduce **Embeddings** and **Vector Stores**, two core concepts in building **Retrieval-Augmented Generation (RAG)** systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concept of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What are Embeddings?\n",
    "\n",
    "**Embeddings** are numerical representations of text (or images, audio, etc.) as vectors in a multi-dimensional space. Each vector is a list of real numbers. The goal of embeddings is to capture the **semantic meaning** of the text.\n",
    "\n",
    "* **Important Property:** Texts with similar meanings (e.g., \"dog\" and \"puppy\") will have their embedding vectors located close to each other in the vector space. Conversely, texts with different meanings will have vectors located far apart.\n",
    "* **How it Works:** Embedding models (also called embedding encoders) are trained on a large amount of data to learn how to map text to this vector space.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. The Role of Embeddings in Semantic Search and RAG\n",
    "\n",
    "Embeddings are the backbone of **semantic search** and **Retrieval-Augmented Generation (RAG)** systems.\n",
    "\n",
    "* **Semantic Search:** Instead of searching based on exact keywords (like traditional search), semantic search allows you to find documents or text snippets based on their meaning.\n",
    "    * **Process:**\n",
    "        1.  Create an embedding for the user's query.\n",
    "        2.  Compare the query's embedding with the embeddings of all stored document chunks.\n",
    "        3.  Retrieve the chunks whose embeddings are closest (i.e., most semantically similar) to the query.\n",
    "* **RAG (Retrieval-Augmented Generation):** Embeddings are an essential component of RAG, a technique that allows an LLM to retrieve relevant information from an external knowledge base before generating a response. This helps the LLM to:\n",
    "    * **Answer more accurately:** Based on factual information, reducing \"hallucinations.\"\n",
    "    * **Update knowledge:** Can access the latest information or private data not present in the LLM's training data.\n",
    "    * **Transparency:** Can indicate the source of information used to generate the answer.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Embedding Models in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain provides a unified interface for working with various embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed:\n",
    "# pip install langchain-openai openai\n",
    "# pip install langchain-google-genai google-generativeai\n",
    "# pip install langchain-huggingface sentence-transformers # for local HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. `OpenAIEmbeddings`\n",
    "\n",
    "* **Concept:** Uses OpenAI's embedding models (e.g., `text-embedding-ada-002`, `text-embedding-3-small`, `text-embedding-3-large`). These are powerful and popular models.\n",
    "* **Requirement:** OpenAI API Key (`OPENAI_API_KEY`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Initialize OpenAIEmbeddings\n",
    "# You can specify model_name, e.g., \"text-embedding-ada-002\"\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Create an embedding for a text snippet\n",
    "text_to_embed = \"LangChain helps build LLM applications.\"\n",
    "embedding = openai_embeddings.embed_query(text_to_embed)\n",
    "\n",
    "print(f\"--- OpenAIEmbeddings ---\")\n",
    "print(f\"Văn bản: '{text_to_embed}'\") # Text: '{text_to_embed}'\n",
    "print(f\"Kích thước vector embedding: {len(embedding)}\") # Embedding vector size:\n",
    "print(f\"Một vài phần tử đầu tiên của vector: {embedding[:5]}...\") # First few elements of the vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. `HuggingFaceEmbeddings` (Local Models)\n",
    "\n",
    "* **Concept:** Allows you to use embedding models from the Hugging Face Hub, including models that can run locally on your machine (e.g., `all-MiniLM-L6-v2`). This is useful when you want to avoid API costs or have data privacy requirements.\n",
    "* **Requirement:** Requires `sentence-transformers` to be installed: `pip install sentence-transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if not already installed:\n",
    "# pip install langchain-huggingface sentence-transformers\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize HuggingFaceEmbeddings with a local model\n",
    "# 'sentence-transformers/all-MiniLM-L6-v2' is a small, efficient model\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create an embedding for a text snippet\n",
    "text_to_embed = \"Semantic search is crucial.\"\n",
    "embedding = hf_embeddings.embed_query(text_to_embed)\n",
    "\n",
    "print(f\"\\n--- HuggingFaceEmbeddings (Cục bộ) ---\") # --- HuggingFaceEmbeddings (Local) ---\n",
    "print(f\"Văn bản: '{text_to_embed}'\") # Text:\n",
    "print(f\"Kích thước vector embedding: {len(embedding)}\") # Embedding vector size:\n",
    "print(f\"Một vài phần tử đầu tiên của vector: {embedding[:5]}...\") # First few elements of the vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. `GoogleGenerativeAIEmbeddings`\n",
    "\n",
    "* **Concept:** Uses Google Generative AI's embedding models (e.g., `embedding-001`).\n",
    "* **Requirement:** Google Cloud API Key (`GOOGLE_API_KEY`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if not already installed:\n",
    "# pip install langchain-google-genai google-generativeai\n",
    "\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Set environment variable for Google API key\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"\n",
    "\n",
    "# Initialize GoogleGenerativeAIEmbeddings\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"embedding-001\")\n",
    "\n",
    "# Create an embedding for a text snippet\n",
    "text_to_embed = \"Vector Stores store embeddings.\"\n",
    "embedding = gemini_embeddings.embed_query(text_to_embed)\n",
    "\n",
    "print(f\"\\n--- GoogleGenerativeAIEmbeddings ---\")\n",
    "print(f\"Văn bản: '{text_to_embed}'\") # Text:\n",
    "print(f\"Kích thước vector embedding: {len(embedding)}\") # Embedding vector size:\n",
    "print(f\"Một vài phần tử đầu tiên của vector: {embedding[:5]}...\") # First few elements of the vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introduction to Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. What are Vector Stores?\n",
    "\n",
    "**Vector Stores** (Vector Databases) are specialized databases designed to store and manage vector embeddings. They are optimized for efficiently searching for the \"closest\" vectors (i.e., most semantically similar) to a given query vector.\n",
    "\n",
    "* **Architecture:** A Vector Store typically uses Approximate Nearest Neighbor (ANN) algorithms to perform fast searches on millions or billions of vectors.\n",
    "* **Types of Vector Stores:**\n",
    "    * **In-memory:** Simple, fast for small cases, but data is lost when the program ends (e.g., local `FAISS`, local `Chroma`).\n",
    "    * **Cloud-based/Managed:** Provides scalability, persistence, and management by the provider (e.g., Pinecone, Weaviate, Qdrant, Milvus, ChromaDB Cloud).\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Why are Vector Stores Needed?\n",
    "\n",
    "* **Store Embeddings:** Provides persistent storage for generated vector embeddings.\n",
    "* **Efficient Search:** Performs fast and accurate semantic searches on a large dataset.\n",
    "* **Easy Integration:** LangChain provides a unified interface for interacting with various Vector Stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Vector Store Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are many different Vector Stores, they generally share the following basic operations:\n",
    "\n",
    "* **`add_documents`:** Adds one or more `Document` objects (already chunked) to the Vector Store. The Vector Store will automatically create embeddings for these `Document`s using the configured embedding model and store them.\n",
    "* **`similarity_search`:** Takes a query string, creates an embedding for that query, and searches for the most semantically similar `Document`s (chunks) in the Vector Store. It returns a list of `Document`s sorted by relevance.\n",
    "* **`as_retriever`:** A convenient method to turn a Vector Store into a `Retriever`. A `Retriever` is a standard interface in LangChain specifically used for retrieving relevant documents based on a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Practical Example with `FAISS` (In-memory Vector Store)\n",
    "\n",
    "**FAISS (Facebook AI Similarity Search)** is a library that enables efficient similarity search on vectors. LangChain provides integration with FAISS to create a local, in-memory Vector Store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Requirement:** Requires `faiss-cpu` (or `faiss-gpu` if you have a GPU) to be installed: `pip install faiss-cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if not already installed:\n",
    "# pip install langchain-openai openai faiss-cpu\n",
    "\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 1. Initialize Embeddings model\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 2. Prepare documents (chunks)\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain is a framework for developing LLM applications.\", metadata={\"source\": \"langchain_docs\"}),\n",
    "    Document(page_content=\"Retrieval-Augmented Generation (RAG) improves LLM accuracy.\", metadata={\"source\": \"rag_article\"}),\n",
    "    Document(page_content=\"Embeddings convert text into numerical vectors.\", metadata={\"source\": \"embedding_guide\"}),\n",
    "    Document(page_content=\"Vector Stores efficiently store and search embeddings.\", metadata={\"source\": \"vector_db_overview\"}),\n",
    "    Document(page_content=\"Machine learning is a branch of artificial intelligence.\", metadata={\"source\": \"ai_basics\"}),\n",
    "    Document(page_content=\"Pip is the package manager for Python.\", metadata={\"source\": \"python_guide\"}),\n",
    "]\n",
    "\n",
    "# 3. Create Vector Store from documents and embeddings model\n",
    "# FAISS.from_documents will automatically create embeddings and add them to the store\n",
    "vector_store = FAISS.from_documents(documents, embeddings_model)\n",
    "\n",
    "print(\"--- FAISS Vector Store ---\")\n",
    "print(f\"Đã thêm {len(documents)} tài liệu vào FAISS Vector Store.\") # Added {len(documents)} documents to FAISS Vector Store.\n",
    "\n",
    "# 4. Similarity Search\n",
    "query = \"How can LLMs access new knowledge?\"\n",
    "print(f\"\\nĐang tìm kiếm cho truy vấn: '{query}'\") # Searching for query:\n",
    "\n",
    "# similarity_search returns the most similar Documents\n",
    "# k=2 means return the 2 most relevant documents\n",
    "retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "print(\"\\n--- Các tài liệu được truy xuất ---\") # --- Retrieved Documents ---\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Tài liệu {i+1} (Nguồn: {doc.metadata.get('source', 'Không rõ')}):\") # Document {i+1} (Source: {doc.metadata.get('source', 'Unknown')}):\n",
    "    print(f\"  Nội dung: {doc.page_content}\") # Content:\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# 5. Convert to Retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1}) # Only retrieve 1 document\n",
    "\n",
    "print(f\"\\n--- Sử dụng Retriever ---\") # --- Using Retriever ---\n",
    "query_retriever = \"Which vector represents text meaning?\"\n",
    "retrieved_by_retriever = retriever.invoke(query_retriever)\n",
    "print(f\"Truy xuất bằng Retriever cho '{query_retriever}':\") # Retrieved by Retriever for:\n",
    "for doc in retrieved_by_retriever:\n",
    "    print(f\"  Nội dung: {doc.page_content}\") # Content:\n",
    "    print(f\"  Siêu dữ liệu: {doc.metadata}\") # Metadata:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "* `FAISS.from_documents(documents, embeddings_model)`: This is a convenient way to create a Vector Store from a list of `Document`s and an `Embeddings` model. FAISS will automatically create embeddings for each `Document` and store them.\n",
    "* `vector_store.similarity_search(query, k=2)`: Performs a semantic search. It will take the `query` string, create an embedding for it, and find the 2 `Document`s with the closest embeddings in the store.\n",
    "* `vector_store.as_retriever()`: Converts the Vector Store into a `Retriever` object. A `Retriever` is a standard interface in LangChain for retrieving documents, very useful when building RAG Chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Storing and Loading a Vector Store (FAISS)\n",
    "\n",
    "For `FAISS`, you can store the created Vector Store to disk and load it later, avoiding the need to recreate embeddings every time you run the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if not already installed:\n",
    "# pip install langchain-openai openai faiss-cpu\n",
    "\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "import shutil # For removing directory\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Prepare documents (similar to the example above)\n",
    "documents_to_save = [\n",
    "    Document(page_content=\"LangChain is a framework for developing LLM applications.\", metadata={\"source\": \"langchain_docs\"}),\n",
    "    Document(page_content=\"Retrieval-Augmented Generation (RAG) improves LLM accuracy.\", metadata={\"source\": \"rag_article\"}),\n",
    "    Document(page_content=\"Embeddings convert text into numerical vectors.\", metadata={\"source\": \"embedding_guide\"}),\n",
    "]\n",
    "\n",
    "# Create Vector Store\n",
    "vector_store_to_save = FAISS.from_documents(documents_to_save, embeddings_model)\n",
    "\n",
    "# Save Vector Store to disk\n",
    "save_path = \"faiss_index\"\n",
    "vector_store_to_save.save_local(save_path)\n",
    "print(f\"\\nĐã lưu Vector Store vào thư mục: {save_path}\") # Vector Store saved to directory:\n",
    "\n",
    "# Load Vector Store from disk\n",
    "loaded_vector_store = FAISS.load_local(save_path, embeddings_model, allow_dangerous_deserialization=True)\n",
    "# allow_dangerous_deserialization=True is necessary when loading objects from disk\n",
    "# Be cautious when using this option with untrusted sources.\n",
    "\n",
    "print(f\"Đã tải lại Vector Store từ thư mục: {save_path}\") # Vector Store reloaded from directory:\n",
    "\n",
    "# Perform search with the reloaded Vector Store\n",
    "query_loaded = \"Which framework helps develop AI applications?\"\n",
    "retrieved_loaded_docs = loaded_vector_store.similarity_search(query_loaded, k=1)\n",
    "\n",
    "print(f\"\\n--- Tìm kiếm với Vector Store đã tải lại ---\") # --- Search with Reloaded Vector Store ---\n",
    "print(f\"Truy vấn: '{query_loaded}'\") # Query:\n",
    "for doc in retrieved_loaded_docs:\n",
    "    print(f\"  Nội dung: {doc.page_content}\") # Content:\n",
    "    print(f\"  Siêu dữ liệu: {doc.metadata}\") # Metadata:\n",
    "\n",
    "# Clean up index directory\n",
    "if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "    print(f\"Đã xóa thư mục '{save_path}'.\") # Directory '{save_path}' removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For cloud-based Vector Stores (like Pinecone, Weaviate), you would not store locally but connect directly to their cloud service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson delved into two core concepts in building RAG systems: **Embeddings** and **Vector Stores**. We understood that **Embeddings** are numerical representations of text, capturing semantic meaning and serving as the foundation for **semantic search**. You learned how to use popular embedding models like **`OpenAIEmbeddings`**, **`HuggingFaceEmbeddings`** (local), and **`GoogleGenerativeAIEmbeddings`** to convert text into vectors.\n",
    "\n",
    "Next, we explored **Vector Stores** as efficient places to store and manage vector embeddings, enabling fast similarity search. Finally, through practical examples with **FAISS** (an in-memory Vector Store), you performed basic operations such as **adding documents**, **similarity search**, and converting a Vector Store into a **Retriever**, as well as how to **store and load** a local Vector Store. Mastering these concepts is a crucial step towards building LLM applications capable of effectively retrieving and utilizing external knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
