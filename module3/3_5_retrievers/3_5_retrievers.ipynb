{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3.5: Retrievers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we learned how to load documents, split them into chunks, create embeddings, and store them in a Vector Store. Now, we need a mechanism to retrieve relevant text segments from the Vector Store when a user asks a question. This is the role of **Retrievers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concept of Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What are Retrievers?\n",
    "\n",
    "**Retrievers** are tools in LangChain designed to query relevant text segments (or documents) from a data source (typically a Vector Store) based on an input query. They act as a bridge between the user's question and your external knowledge base.\n",
    "\n",
    "* **Relationship:** Retrievers are a core component in the **Retrieval-Augmented Generation (RAG)** architecture. They receive the user's query, search for relevant documents in your knowledge base, and then pass these documents along with the query to the LLM to generate the final answer.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. The Role of Retrievers in RAG\n",
    "\n",
    "In a RAG system, the processing flow typically goes as follows:\n",
    "1.  The user poses a **query**.\n",
    "2.  The **Retriever** receives this query.\n",
    "3.  The Retriever uses a search mechanism (e.g., similarity search in a Vector Store) to identify the most relevant text segments (chunks) to the query.\n",
    "4.  The Retriever returns these relevant text segments.\n",
    "5.  These retrieved text segments are then provided as **context** to the LLM along with the original query for the LLM to generate the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `VectorStoreRetriever`: Basic Retriever based on Vector Store Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VectorStoreRetriever` is the most common and basic type of Retriever in LangChain. It works by performing a similarity search on a configured Vector Store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. How it Works\n",
    "\n",
    "When you create a `VectorStoreRetriever` from a `VectorStore` (like FAISS or Chroma), it will use the embedding model associated with that Vector Store to:\n",
    "1.  Convert the user's query into an embedding vector.\n",
    "2.  Perform a similarity search within the Vector Store to find the text segments (chunks) whose embeddings are closest to the query's embedding.\n",
    "3.  Return the `Document` objects corresponding to those chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Configuring `k` (Number of Documents to Return)\n",
    "\n",
    "The `k` parameter is one of the most important configurations for `VectorStoreRetriever`.\n",
    "\n",
    "* **Concept:** `k` determines the number of top relevant text segments (chunks) that the Retriever will return from the Vector Store.\n",
    "* **Impact:**\n",
    "    * **Small `k`:** The LLM will receive less context. This can be good if you want the LLM to focus on very specific information or if you are dealing with strict token limits. However, there's a risk of missing important information.\n",
    "    * **Large `k`:** The LLM will receive more context. This helps ensure all relevant information is included, but can lead to the LLM being \"overwhelmed\" with information or exceeding token limits.\n",
    "* **Selection:** The optimal value of `k` depends on the length of your chunks, the token limit of the LLM you are using, and the complexity of the questions. Typically, a `k` value between 3 and 5 is a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Practical Example with `VectorStoreRetriever` (using Chroma)\n",
    "\n",
    "We will use Chroma (from Lesson 3.4) to create a Vector Store, then turn it into a Retriever and perform a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have `langchain-openai` and `chromadb` installed.\n",
    "* Set the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai chromadb\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 1. Initialize Embeddings model\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 2. Prepare sample documents\n",
    "long_text_content = \"\"\"\n",
    "LangChain is an open-source framework designed to help developers build applications powered by Large Language Models (LLMs) more easily and efficiently. It provides a set of tools, components, and abstractions to simplify complex processes related to LLMs, from prompt management to connecting LLMs with external data sources and tools.\n",
    "\n",
    "Key components of LangChain include:\n",
    "- Models: Interfaces for LLMs and Chat Models.\n",
    "- Prompts: Tools for constructing and managing prompts for LLMs.\n",
    "- Chains: Connecting components together to form a processing flow.\n",
    "- Agents: Allowing LLMs to make decisions and use tools.\n",
    "- Retrieval: Tools for loading, splitting, embedding, and retrieving documents.\n",
    "- Memory: Maintaining state and context in conversations.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a popular architecture in LLM applications. RAG allows an LLM to retrieve information from an external knowledge base (often a Vector Store) before generating a response. This helps improve accuracy and reduce LLM \"hallucinations.\"\n",
    "\n",
    "To build a RAG system, the main steps typically include:\n",
    "1. Document Loading.\n",
    "2. Text Splitting into chunks.\n",
    "3. Creating embeddings for text chunks.\n",
    "4. Storing embeddings in a Vector Store.\n",
    "5. When queried, searching for relevant chunks in the Vector Store.\n",
    "6. Passing relevant chunks and the query to the LLM to generate an answer.\n",
    "\n",
    "Chroma is an open-source vector database, easy to use, supporting local and client/server storage.\n",
    "\"\"\"\n",
    "\n",
    "doc = Document(page_content=long_text_content, metadata={\"source\": \"langchain_guide\"})\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "chunks = text_splitter.split_documents([doc])\n",
    "\n",
    "# 3. Create Chroma Vector Store from chunks\n",
    "persist_directory = \"./chroma_db_retriever_demo\"\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "\n",
    "print(f\"Creating Chroma Vector Store at: {persist_directory}...\")\n",
    "vector_store = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embeddings_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "print(\"Chroma Vector Store created successfully.\")\n",
    "\n",
    "# 4. Convert Vector Store to Retriever\n",
    "# Configure k=2 to retrieve 2 most relevant text segments\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "print(f\"\\nRetriever created with k=2.\")\n",
    "\n",
    "# 5. Perform query with Retriever\n",
    "query = \"What are the steps to build a RAG system?\"\n",
    "print(f\"\\nPerforming query with Retriever: '{query}'\")\n",
    "\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"\\n--- Documents retrieved by Retriever ---\")\n",
    "for i, doc_retrieved in enumerate(retrieved_docs):\n",
    "    print(f\"Document {i+1} (Source: {doc_retrieved.metadata.get('source', 'Unknown')}):\")\n",
    "    print(f\"  Content: {doc_retrieved.page_content[:200]}...\")\n",
    "    print(f\"  Metadata: {doc_retrieved.metadata}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Clean up Chroma directory\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "    print(f\"\\nChroma directory '{persist_directory}' removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "* `vector_store.as_retriever(search_kwargs={\"k\": 2})`: This is how you convert a Vector Store (Chroma here) into a Retriever. `search_kwargs` is a dictionary to pass custom parameters for the search process, in this case `k=2` to specify the number of documents to retrieve.\n",
    "* `retriever.invoke(query)`: Invokes the Retriever with the user's question. The Retriever will perform internal steps (embedding the query, searching the Vector Store) and return a list of relevant `Document`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Retriever Types (Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain provides various advanced Retriever types to address more complex challenges in information retrieval. Here's an introduction to some common ones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. `ContextualCompressionRetriever`\n",
    "\n",
    "* **Concept:** The `ContextualCompressionRetriever` works by retrieving a larger initial set of documents (from a base Retriever), and then uses an LLM or another model to \"compress\" or \"filter\" the information within those documents, keeping only the most important parts directly relevant to the query.\n",
    "* **When to Use:** When you are concerned that retrieved documents might contain a lot of noise or irrelevant information, \"diluting\" the context for the main LLM. It helps ensure the LLM receives only concise and high-quality context.\n",
    "* **How it Works:** Typically uses a `BaseRetriever` (e.g., `VectorStoreRetriever`) and a `BaseLLMCompressor` (e.g., `LLMChainExtractor` or `LLMChainFilter`).\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. `MultiQueryRetriever`\n",
    "\n",
    "* **Concept:** Instead of searching based on a single query, the `MultiQueryRetriever` uses an LLM to generate multiple different sub-queries from the user's original query. It then performs searches with all these sub-queries and combines the results.\n",
    "* **When to Use:** When the user's query might be ambiguous or have multiple interpretations, or when you want to ensure a more comprehensive search to avoid missing relevant information.\n",
    "* **Benefit:** Increases the likelihood of finding relevant documents by exploring different facets of the query.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. `ParentDocumentRetriever`\n",
    "\n",
    "* **Concept:** The `ParentDocumentRetriever` addresses the challenge where you want to search on small text segments (chunks) for high precision, but also want to provide the LLM with broader context from the \"parent\" document containing those segments. It stores both the small chunks and larger documents. During retrieval, it searches on the small chunks, but then returns the corresponding parent document.\n",
    "* **When to Use:** When you need a balance between the precision of searching on small chunks and the need for full context for the LLM. For example, when you search for a small detail in a book, but want the LLM to have the entire chapter or relevant section to answer.\n",
    "* **How it Works:** Uses two Vector Stores: one for the small chunks and one for the parent documents.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These advanced Retriever types allow you to build more sophisticated RAG systems, optimizing the information retrieval process to provide the best possible context to the LLM, thereby improving answer quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson introduced **Retrievers** in LangChain, essential tools for querying and fetching relevant text segments from a Vector Store. We delved into **`VectorStoreRetriever`**, the most basic Retriever type, and how to configure the `k` parameter to control the number of documents returned. Through a practical example with Chroma, you saw how to create and use a `VectorStoreRetriever` to retrieve information. Finally, the lesson provided an overview of advanced Retriever types such as **`ContextualCompressionRetriever`** (context compression), **`MultiQueryRetriever`** (multi-query generation), and **`ParentDocumentRetriever`** (parent document retrieval), opening the door to building more complex and effective RAG systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
