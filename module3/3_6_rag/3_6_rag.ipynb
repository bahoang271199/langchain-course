{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3.6: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lessons, we built the foundation for data processing with LangChain: loading documents, splitting them, creating embeddings, and storing them in a Vector Store. Now, we will combine all these components to create one of the most powerful and popular architectures in modern LLM applications: **Retrieval-Augmented Generation (RAG)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concept of Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique in the field of Large Language Models (LLMs) that combines the ability to **retrieve information** from an external knowledge base with the LLM's **text generation** capabilities.\n",
    "\n",
    "Instead of solely relying on the knowledge pre-trained within the model (which can be outdated or incomplete), a RAG system will:\n",
    "1.  **Retrieve** relevant text segments or documents from a database (typically a Vector Store) based on the user's query.\n",
    "2.  Provide this retrieved information as **additional context** to the LLM.\n",
    "3.  The LLM then uses both the original user query and the retrieved context to **generate** the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Benefits of RAG\n",
    "\n",
    "RAG offers several significant advantages, addressing inherent limitations of standalone LLMs:\n",
    "\n",
    "* **Reduces Hallucination:** LLMs tend to \"make up\" information if they lack sufficient data or context. RAG provides factual, reliable information, helping the LLM generate evidence-based answers.\n",
    "* **Increases Accuracy and Relevance:** By retrieving documents directly relevant to the query, RAG ensures that the LLM's answers are more accurate and pertinent to the user's needs.\n",
    "* **Provides Attribution:** RAG allows you to cite the source documents the LLM used to generate its answer, increasing transparency and trustworthiness. This is crucial in applications requiring high accuracy (e.g., medical, legal).\n",
    "* **Flexible Knowledge Updates:** LLMs only have knowledge up to their last training cutoff. RAG allows you to update the external knowledge base (Vector Store) independently without retraining the entire LLM, keeping the system up-to-date with the latest information.\n",
    "* **Handles Private/Proprietary Information:** You can use RAG to give an LLM access to internal, private company data without having to retrain the LLM on that data.\n",
    "* **Cost Reduction:** Instead of using very large (and expensive) LLMs with massive context windows or fine-tuning LLMs, RAG allows you to use smaller, more efficient LLMs by providing precise context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture of a Basic RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic RAG system is typically divided into two main phases: **Indexing** and **Runtime**, with Runtime comprising **Retrieval** and **Generation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Indexing Phase\n",
    "\n",
    "This phase occurs **before** a user asks a question. The goal is to prepare your knowledge base for efficient retrieval.\n",
    "\n",
    "1.  **Document Loading:**\n",
    "    * Use **Document Loaders** (Lesson 3.1) to load data from various sources (TXT files, PDFs, web pages, CSVs, etc.) into LangChain's `Document` objects.\n",
    "2.  **Text Splitting:**\n",
    "    * Use **Text Splitters** (Lesson 3.2) to break down large documents into smaller text segments (chunks) to fit within LLM token limits and optimize search.\n",
    "3.  **Creating Embeddings:**\n",
    "    * Use **Embedding models** (Lesson 3.3) to convert each text chunk into a numerical vector (embedding).\n",
    "4.  **Storing in Vector Store:**\n",
    "    * Store these embedding vectors along with the original `Document`s (or their metadata) in a **Vector Store** (Lesson 3.3, 3.4) like FAISS, Chroma, Pinecone, etc., for efficient searching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Retrieval Phase\n",
    "\n",
    "This phase occurs **when** a user asks a question.\n",
    "\n",
    "1.  The user poses a **query**.\n",
    "2.  This query is converted into an embedding vector using the same embedding model used in the Indexing phase.\n",
    "3.  The **Retriever** (Lesson 3.5) uses this query vector to search for the most similar embedding vectors in the Vector Store.\n",
    "4.  The Retriever returns the most relevant text segments (chunks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Generation Phase\n",
    "\n",
    "This phase also occurs **when** a user asks a question.\n",
    "\n",
    "1.  The relevant text segments retrieved from the Retrieval phase are provided as **context** to the LLM.\n",
    "2.  A **Prompt Template** is constructed, incorporating both the user's query and the retrieved context.\n",
    "3.  The LLM receives this prompt and generates the final answer based on both its internal knowledge and the information provided in the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Question Answering (Q&A) System on Your Own Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will build a simple Question Answering system using the RAG architecture on a custom document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Environment and Data Preparation\n",
    "\n",
    "**Preparation:**\n",
    "* Ensure you have the necessary libraries installed: `langchain-openai`, `chromadb`, `pypdf`.\n",
    "* Set the `OPENAI_API_KEY` environment variable.\n",
    "* Prepare a sample PDF file (e.g., `sample_document.pdf`) in the same directory as your code. If you don't have a real PDF, you can create a dummy file or use a long text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai chromadb pypdf reportlab\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 1. Prepare sample document (PDF file)\n",
    "# Create a dummy PDF file if a real one is not available for the code to run\n",
    "pdf_file_path = \"sample_document.pdf\"\n",
    "try:\n",
    "    from reportlab.pdfgen import canvas\n",
    "    c = canvas.Canvas(pdf_file_path)\n",
    "    c.drawString(100, 750, \"This is a sample document about LangChain and RAG.\")\n",
    "    c.drawString(100, 730, \"LangChain is a framework for building LLM applications.\")\n",
    "    c.drawString(100, 710, \"RAG combines information retrieval with LLM text generation.\")\n",
    "    c.drawString(100, 690, \"It helps reduce 'hallucinations' and increase accuracy.\")\n",
    "    c.drawString(100, 670, \"RAG steps include Loading, Splitting, Embedding, Storing, Retrieving, and Generating.\")\n",
    "    c.drawString(100, 650, \"Chroma is a popular open-source Vector Store.\")\n",
    "    c.drawString(100, 630, \"FAISS is also a good option for local Vector Stores.\")\n",
    "    c.save()\n",
    "    print(f\"Sample PDF file created: {pdf_file_path}\")\n",
    "except ImportError:\n",
    "    with open(pdf_file_path, \"w\") as f:\n",
    "        f.write(\"This is a dummy PDF file. Please replace with a real PDF.\\n\")\n",
    "    print(\"Could not create real PDF with reportlab. Using dummy file.\")\n",
    "    print(\"Please ensure you have a real PDF file 'sample_document.pdf' for the example to work best.\")\n",
    "\n",
    "# Initialize LLM and Embeddings Model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Directory for ChromaDB persistence\n",
    "persist_directory = \"./chroma_db_rag_demo\"\n",
    "# Remove old directory if it exists to ensure a clean start\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "    print(f\"Old Chroma directory removed: {persist_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Building the Indexing Phase\n",
    "\n",
    "We will load the document, split it, and store it in a Chroma Vector Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Indexing Phase ---\n",
    "\n",
    "# 1. Load documents\n",
    "print(\"Loading documents...\")\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} pages from PDF.\")\n",
    "\n",
    "# 2. Split text\n",
    "print(\"Splitting text...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(chunks)} chunks.\")\n",
    "\n",
    "# 3. Create Embeddings and Store in Vector Store\n",
    "print(\"Creating embeddings and storing in Chroma Vector Store...\")\n",
    "vector_store = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "print(\"Indexing phase completed.\")\n",
    "\n",
    "# 4. Create Retriever from Vector Store\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3}) # Retrieve 3 most relevant chunks\n",
    "print(f\"Retriever created, will retrieve k=3 documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Building the Retrieval and Generation Phase\n",
    "\n",
    "We will combine the Retriever with the LLM to create the RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Retrieval and Generation Phase ---\n",
    "\n",
    "# 1. Define Prompt for RAG\n",
    "# This prompt will include the user's query and the retrieved context\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful Q&A assistant. Answer the user's question based on the provided context. If you cannot find the answer in the context, state that you don't know.\"),\n",
    "    HumanMessage(content=\"Context: {context}\\n\\nQuestion: {question}\"),\n",
    "])\n",
    "\n",
    "# 2. Build the RAG chain using LCEL\n",
    "# This chain will:\n",
    "# - Take the user's question.\n",
    "# - Retrieve context using the Retriever.\n",
    "# - Format the prompt with the question and context.\n",
    "# - Invoke the LLM to generate the answer.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} # Retriever will take the question as input\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Executing the Q&A System\n",
    "\n",
    "Now, let's try asking a few questions to your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Execute the Q&A System ---\n",
    "\n",
    "print(\"\\n--- Starting RAG Q&A System ---\")\n",
    "\n",
    "# Question 1\n",
    "question_1 = \"What is RAG and how does it help?\"\n",
    "print(f\"\\nQuestion: {question_1}\")\n",
    "answer_1 = rag_chain.invoke({\"question\": question_1})\n",
    "print(f\"Answer: {answer_1}\")\n",
    "\n",
    "# Question 2\n",
    "question_2 = \"What are the main steps to build a RAG system?\"\n",
    "print(f\"\\nQuestion: {question_2}\")\n",
    "answer_2 = rag_chain.invoke({\"question\": question_2})\n",
    "print(f\"Answer: {answer_2}\")\n",
    "\n",
    "# Question 3 (out of context)\n",
    "question_3 = \"What is the capital of France?\"\n",
    "print(f\"\\nQuestion: {question_3}\")\n",
    "answer_3 = rag_chain.invoke({\"question\": question_3})\n",
    "print(f\"Answer: {answer_3}\") # LLM might answer if it has general knowledge, or say it doesn't know if the prompt is well-crafted.\n",
    "\n",
    "# Clean up sample PDF file and Chroma directory\n",
    "os.remove(pdf_file_path)\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "print(f\"\\nSample PDF file and Chroma directory '{persist_directory}' removed.\")\n",
    "\n",
    "print(\"\\n--- RAG Q&A System Ended ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Flow Explanation in `rag_chain`:**\n",
    "\n",
    "1.  `rag_chain.invoke({\"question\": question_1})`: The input is a dictionary containing the `question` key.\n",
    "2.  `{\"context\": retriever, \"question\": RunnablePassthrough()}`:\n",
    "    * This is a `RunnableParallel`.\n",
    "    * `retriever`: Receives the value of `question` from the `rag_chain`'s input and performs retrieval of relevant documents from the Vector Store. The result is a list of `Document` objects. This list is assigned to the `context` key.\n",
    "    * `RunnablePassthrough()`: Receives the entire original input dictionary (`{\"question\": \"...\"}`) and passes it through. This result is assigned to the `question` key in the `RunnableParallel`'s output.\n",
    "    * The output of this step is a dictionary: `{\"context\": [doc1, doc2, ...], \"question\": \"...\"}`.\n",
    "3.  `| rag_prompt`:\n",
    "    * `rag_prompt` receives this dictionary. It will take the list of `Document`s from the `context` key and the `question` from the `question` key.\n",
    "    * It formats the prompt according to the defined structure, inserting the content of the `Document`s into the `{context}` placeholder and the question into the `{question}`.\n",
    "    * The result is a `ChatPromptValue` object (a list of messages).\n",
    "4.  `| llm`:\n",
    "    * `llm` receives the `ChatPromptValue` object and sends it to the LLM model (e.g., GPT-3.5 Turbo).\n",
    "    * The LLM processes the prompt and generates a response. The result is an `AIMessage` object.\n",
    "5.  `| StrOutputParser()`:\n",
    "    * `StrOutputParser` receives the `AIMessage` object and extracts the string content of the LLM's response.\n",
    "    * This is the final output of the `rag_chain`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson provided a comprehensive introduction to **Retrieval-Augmented Generation (RAG)**, a powerful architecture that combines information retrieval capabilities with LLM text generation. We learned about the **benefits of RAG**, including reducing \"hallucinations,\" increasing accuracy, and providing attribution. The lesson also delved into the **architecture of a basic RAG system**, divided into two main phases: **Indexing** (loading, splitting, embedding, and storing documents) and **Runtime** (comprising **Retrieval** for fetching relevant text segments and **Generation** for the LLM to produce answers based on the retrieved context). Finally, you practiced **building a Question Answering (Q&A) system on your own documents**, integrating all the learned concepts about Document Loaders, Text Splitters, Embeddings, Vector Stores, and Retrievers to create an LLM application capable of querying specialized knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
