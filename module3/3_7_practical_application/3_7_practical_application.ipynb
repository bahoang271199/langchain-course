{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3.7: Practical Application - Building a Q&A System\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Module 3, we've journeyed from loading raw data to preparing it for LLM consumption: **Document Loaders**, **Text Splitters**, **Embeddings**, **Vector Stores**, and **Retrievers**. Now it's time to bring all these pieces together to build a practical and powerful application: a **Question Answering (Q&A) system based on Retrieval-Augmented Generation (RAG)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lesson will guide you step-by-step through building a complete RAG pipeline, from preparing your data to generating intelligent answers based on your own knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objectives of this Practical Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objectives of this practical exercise are to:\n",
    "* Apply all the knowledge learned from Module 3 to a real-world project.\n",
    "* Understand the data flow and how LangChain components interact within a RAG system.\n",
    "* Build a Q&A system capable of answering questions based on a specific set of documents you provide.\n",
    "* Test and evaluate the performance of the Q&A system.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, ensure you have installed the necessary libraries and set up your API keys for your LLM and Embedding models. We will use OpenAI for LLM and Embeddings, along with ChromaDB as the Vector Store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Libraries:**\n",
    "```bash\n",
    "pip install langchain-openai openai chromadb pypdf reportlab\n",
    "```\n",
    "* `langchain-openai`: LangChain integration with OpenAI.\n",
    "* `openai`: Official OpenAI Python library.\n",
    "* `chromadb`: Open-source vector database.\n",
    "* `pypdf`: For reading PDF files.\n",
    "* `reportlab`: (Optional) For creating sample PDF files if you don't have them readily available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API Key Setup:**\n",
    "```python\n",
    "import os\n",
    "# Set your API key in an environment variable\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Sample Documents:**\n",
    "We will create two simple sample PDF files for illustration. In a real-world scenario, you would replace these with your own documents (e.g., company internal documents, books, reports)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Thiết lập biến môi trường cho khóa API của OpenAI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Khởi tạo LLM và Embeddings Model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Thư mục lưu trữ ChromaDB\n",
    "persist_directory = \"./chroma_qa_system_db\"\n",
    "# Xóa thư mục cũ nếu tồn tại để đảm bảo sạch sẽ\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "    print(f\"Old Chroma directory removed: {persist_directory}\")\n",
    "\n",
    "# Tạo hai file PDF mẫu\n",
    "pdf_file_path_1 = \"document_part_1.pdf\"\n",
    "pdf_file_path_2 = \"document_part_2.pdf\"\n",
    "\n",
    "try:\n",
    "    from reportlab.pdfgen import canvas\n",
    "    # Nội dung cho PDF 1\n",
    "    c1 = canvas.Canvas(pdf_file_path_1)\n",
    "    c1.drawString(100, 750, \"Tài liệu này nói về các nguyên tắc cơ bản của Trí tuệ Nhân tạo (AI).\")\n",
    "    c1.drawString(100, 730, \"AI là lĩnh vực khoa học máy tính tập trung vào việc tạo ra máy móc thông minh.\")\n",
    "    c1.drawString(100, 710, \"Các nhánh chính của AI bao gồm Học máy, Học sâu và Xử lý Ngôn ngữ Tự nhiên (NLP).\")\n",
    "    c1.drawString(100, 690, \"Học máy cho phép hệ thống học từ dữ liệu mà không cần lập trình rõ ràng.\")\n",
    "    c1.save()\n",
    "    print(f\"Sample PDF file created: {pdf_file_path_1}\")\n",
    "\n",
    "    # Nội dung cho PDF 2\n",
    "    c2 = canvas.Canvas(pdf_file_path_2)\n",
    "    c2.drawString(100, 750, \"Phần này tập trung vào ứng dụng của AI trong y tế và giáo dục.\")\n",
    "    c2.drawString(100, 730, \"Trong y tế, AI giúp chẩn đoán bệnh sớm và phát triển thuốc mới.\")\n",
    "    c2.drawString(100, 710, \"Trong giáo dục, AI cá nhân hóa trải nghiệm học tập và đánh giá tiến độ.\")\n",
    "    c2.drawString(100, 690, \"AI cũng được sử dụng trong xe tự lái và trợ lý ảo thông minh.\")\n",
    "    c2.save()\n",
    "    print(f\"Sample PDF file created: {pdf_file_path_2}\")\n",
    "\n",
    "except ImportError:\n",
    "    with open(pdf_file_path_1, \"w\") as f:\n",
    "        f.write(\"This is a dummy PDF file 1. Please replace with a real PDF.\\n\")\n",
    "    with open(pdf_file_path_2, \"w\") as f:\n",
    "        f.write(\"This is a dummy PDF file 2. Please replace with a real PDF.\\n\")\n",
    "    print(\"Could not create real PDF with reportlab. Using dummy files.\")\n",
    "    print(\"Please ensure you have real PDF files for the example to work best.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Complete RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the RAG pipeline following the steps we've learned: Load, Split, Embed, Store, Retrieve, and Generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Indexing Phase (Load, Split, Embed, Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Indexing Phase ---\n",
    "\n",
    "# 1. Load documents using Document Loader\n",
    "print(\"Loading documents from PDF files...\") # Đang tải tài liệu từ các file PDF...\n",
    "loader1 = PyPDFLoader(pdf_file_path_1)\n",
    "loader2 = PyPDFLoader(pdf_file_path_2)\n",
    "\n",
    "documents = []\n",
    "documents.extend(loader1.load())\n",
    "documents.extend(loader2.load())\n",
    "print(f\"Loaded a total of {len(documents)} pages from PDF files.\") # Đã tải tổng cộng {len(documents)} trang từ các file PDF.\n",
    "\n",
    "# 2. Split documents using Text Splitter\n",
    "print(\"Splitting text into chunks...\") # Đang chia nhỏ văn bản thành các chunks...\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, # Desired chunk size\n",
    "    chunk_overlap=50, # Overlap between chunks\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(chunks)} chunks.\") # Đã chia thành {len(chunks)} chunks.\n",
    "\n",
    "# 3. Create embeddings and store in Vector Store (Chroma)\n",
    "print(\"Creating embeddings and storing in Chroma Vector Store...\") # Đang tạo embeddings và lưu trữ vào Chroma Vector Store...\n",
    "vector_store = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "print(\"Indexing phase completed. Data is ready for querying.\") # Đã hoàn thành giai đoạn Indexing. Dữ liệu đã sẵn sàng để truy vấn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Runtime Phase (Retrieval and Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Runtime Phase ---\n",
    "\n",
    "# 4. Set up Retriever\n",
    "# The Retriever will retrieve relevant text segments from the Vector Store.\n",
    "# k=3 means it will return the 3 most similar text segments.\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(f\"Retriever set up, will retrieve k=3 documents for each query.\") # Đã thiết lập Retriever, sẽ truy xuất k=3 tài liệu cho mỗi truy vấn.\n",
    "\n",
    "# 5. Connect with LLM to generate answers (Build RAG Chain)\n",
    "# This prompt will instruct the LLM on how to use the provided context.\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful Q&A assistant. Answer the user's question based on the provided context. If you cannot find the answer in the context, state that you don't know.\"),\n",
    "    HumanMessage(content=\"Context: {context}\\n\\nQuestion: {question}\"),\n",
    "])\n",
    "\n",
    "# Build the RAG chain using LCEL\n",
    "# Data flow:\n",
    "# - Input: {\"question\": \"User's question\"}\n",
    "# - Step 1 (RunnableParallel):\n",
    "#   - \"context\": Retriever will receive \"question\" and retrieve relevant documents.\n",
    "#   - \"question\": RunnablePassthrough will pass the original \"question\" through.\n",
    "# - Step 2 (rag_prompt): Receives {\"context\": [docs], \"question\": \"...\"} and formats the prompt.\n",
    "# - Step 3 (llm): Receives the prompt and generates a response.\n",
    "# - Step 4 (StrOutputParser): Extracts the string from the LLM's response.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"Complete RAG chain built.\") # Đã xây dựng chuỗi RAG hoàn chỉnh.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing and Evaluating the Q&A System's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try asking a few questions to the system and observe how it responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test and Evaluate ---\n",
    "\n",
    "print(\"\\n--- Starting RAG Q&A System Test ---\") # --- Bắt đầu kiểm tra hệ thống Hỏi & Đáp RAG ---\n",
    "\n",
    "# Question 1: Related to PDF content 1\n",
    "question_1 = \"What are the main branches of Artificial Intelligence?\"\n",
    "print(f\"\\nQuestion: {question_1}\") # Câu hỏi:\n",
    "answer_1 = rag_chain.invoke({\"question\": question_1})\n",
    "print(f\"Answer: {answer_1}\") # Trả lời:\n",
    "\n",
    "# Question 2: Related to PDF content 2\n",
    "question_2 = \"How is AI applied in the medical field?\"\n",
    "print(f\"\\nQuestion: {question_2}\") # Câu hỏi:\n",
    "answer_2 = rag_chain.invoke({\"question\": question_2})\n",
    "print(f\"Answer: {answer_2}\") # Trả lời:\n",
    "\n",
    "# Question 3: Related to both PDFs or general knowledge\n",
    "question_3 = \"What is LangChain and how is it related to RAG?\"\n",
    "print(f\"\\nQuestion: {question_3}\") # Câu hỏi:\n",
    "answer_3 = rag_chain.invoke({\"question\": question_3})\n",
    "print(f\"Answer: {answer_3}\") # Trả lời:\n",
    "\n",
    "# Question 4: Out of provided context\n",
    "question_4 = \"Who invented the light bulb?\"\n",
    "print(f\"\\nQuestion: {question_4}\") # Câu hỏi:\n",
    "answer_4 = rag_chain.invoke({\"question\": question_4})\n",
    "print(f\"Answer: {answer_4}\") # Trả lời:\n",
    "# LLM might answer if it has general knowledge, or say it doesn't know if the prompt is well-crafted\n",
    "# and the question is truly outside the provided context.\n",
    "\n",
    "print(\"\\n--- RAG Q&A System Test Ended ---\") # --- Kết thúc kiểm tra hệ thống Hỏi & Đáp RAG ---\n",
    "\n",
    "# Clean up sample PDF files and Chroma directory\n",
    "os.remove(pdf_file_path_1)\n",
    "os.remove(pdf_file_path_2)\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "    print(f\"Removed sample PDF files and Chroma directory '{persist_directory}'.\") # Đã xóa các file PDF mẫu và thư mục Chroma '{persist_directory}'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Evaluation:**\n",
    "\n",
    "* **Accuracy:** Is the answer correct based on the information in your documents?\n",
    "* **Relevance:** Does the answer directly address the question?\n",
    "* **Completeness:** Does the answer provide sufficient necessary information?\n",
    "* **Out-of-context question handling:** Does the system respond with \"I don't know\" or avoid hallucinating information when the question is not in the documents? (This heavily depends on your prompt engineering).\n",
    "* **Speed:** How fast does the system respond?\n",
    "\n",
    "For a more scientific evaluation, you can create a dataset of (question, correct answer) pairs and relevant documents, then use automated or manual evaluation tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "In this lesson, you practiced building a **complete Question Answering (Q&A) system based on Retrieval-Augmented Generation (RAG)**, applying all the knowledge learned from Module 3. You went through each step of the RAG pipeline:\n",
    "* **Loading documents** using `PyPDFLoader`.\n",
    "* **Splitting documents** using `RecursiveCharacterTextSplitter`.\n",
    "* **Creating embeddings** and **storing** them in a **Chroma Vector Store**.\n",
    "* **Setting up a Retriever** to fetch relevant text segments.\n",
    "* **Connecting** the Retriever with an LLM using LCEL to **generate answers** based on the retrieved context.\n",
    "\n",
    "This practical exercise not only reinforced theoretical knowledge but also provided you with hands-on experience in building a crucial LLM application, capable of being scaled and customized with your own data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
