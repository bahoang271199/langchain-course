{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3.4: Practical with Vector Stores\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we learned about the concepts of **Embeddings** and **Vector Stores**. Now, we will dive into practical exercises with some common Vector Stores in LangChain, including local solutions like **FAISS** and **Chroma**, along with an overview of cloud solutions like **Pinecone** or **Weaviate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As learned, **Vector Stores** are specialized databases designed to store and manage vector embeddings. They play a central role in **Retrieval-Augmented Generation (RAG)** systems by enabling efficient semantic search, helping LLMs retrieve relevant information from your knowledge base.\n",
    "\n",
    "The choice of Vector Store depends on your project's scale, performance requirements, scalability, and budget.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FAISS (Facebook AI Similarity Search): Local, Simple to Start Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FAISS** is an open-source library developed by Facebook AI, providing efficient algorithms for similarity search on large sets of vectors. LangChain integrates FAISS to create a local Vector Store, which is very suitable for development and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. FAISS Characteristics\n",
    "\n",
    "* **Local (In-memory/Disk-backed):** FAISS can operate entirely in memory or save the index to disk for later loading.\n",
    "* **Simple:** Easy to set up and use for small to medium-sized cases.\n",
    "* **Efficient:** Provides high search performance for large vector datasets.\n",
    "* **No Distributed Scalability:** Not designed for large-scale multi-user or distributed applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Practical with FAISS\n",
    "\n",
    "We will create a FAISS index from a set of documents and then perform a similarity search.\n",
    "\n",
    "**Preparation:**\n",
    "* Ensure you have `langchain-openai` and `faiss-cpu` installed.\n",
    "* Set the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai faiss-cpu\n",
    "\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 1. Initialize Embeddings model\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 2. Prepare sample documents (similar to Lesson 3.2)\n",
    "long_text_content = \"\"\"\n",
    "LangChain is an open-source framework designed to help developers build applications powered by Large Language Models (LLMs) more easily and efficiently. It provides a set of tools, components, and abstractions to simplify complex processes related to LLMs, from prompt management to connecting LLMs with external data sources and tools.\n",
    "\n",
    "Key components of LangChain include:\n",
    "- Models: Interfaces for LLMs and Chat Models.\n",
    "- Prompts: Tools for constructing and managing prompts for LLMs.\n",
    "- Chains: Connecting components together to form a processing flow.\n",
    "- Agents: Allowing LLMs to make decisions and use tools.\n",
    "- Retrieval: Tools for loading, splitting, embedding, and retrieving documents.\n",
    "- Memory: Maintaining state and context in conversations.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a popular architecture in LLM applications. RAG allows an LLM to retrieve information from an external knowledge base (often a Vector Store) before generating a response. This helps improve accuracy and reduce LLM \"hallucinations.\"\n",
    "\n",
    "To build a RAG system, the main steps typically include:\n",
    "1. Document Loading.\n",
    "2. Text Splitting into chunks.\n",
    "3. Creating embeddings for text chunks.\n",
    "4. Storing embeddings in a Vector Store.\n",
    "5. When queried, searching for relevant chunks in the Vector Store.\n",
    "6. Passing relevant chunks and the query to the LLM to generate an answer.\n",
    "\n",
    "FAISS is an efficient similarity search library, often used as a local Vector Store in LangChain. It allows you to store embeddings and perform fast searches on them.\n",
    "\"\"\"\n",
    "\n",
    "# Create a Document from the text string\n",
    "doc = Document(page_content=long_text_content, metadata={\"source\": \"langchain_overview\"})\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "chunks = text_splitter.split_documents([doc])\n",
    "\n",
    "print(f\"Tổng số chunks sau khi chia nhỏ: {len(chunks)}\\n\") # Total chunks after splitting:\n",
    "\n",
    "# 3. Create FAISS index from documents\n",
    "print(\"Đang tạo FAISS index từ các chunks...\") # Creating FAISS index from chunks...\n",
    "faiss_vector_store = FAISS.from_documents(chunks, embeddings_model)\n",
    "print(\"Đã tạo FAISS index thành công.\") # FAISS index created successfully.\n",
    "\n",
    "# 4. Perform similarity search\n",
    "query_faiss = \"What are the main components of LangChain?\"\n",
    "print(f\"\\nĐang tìm kiếm cho truy vấn: '{query_faiss}'\") # Searching for query:\n",
    "retrieved_docs_faiss = faiss_vector_store.similarity_search(query_faiss, k=2)\n",
    "\n",
    "print(\"\\n--- Kết quả tìm kiếm từ FAISS ---\") # --- Search Results from FAISS ---\n",
    "for i, doc_retrieved in enumerate(retrieved_docs_faiss):\n",
    "    print(f\"Tài liệu {i+1} (Nguồn: {doc_retrieved.metadata.get('source', 'Không rõ')}):\") # Document {i+1} (Source: {doc_retrieved.metadata.get('source', 'Unknown')}):\n",
    "    print(f\"  Nội dung: {doc_retrieved.page_content[:200]}...\") # Content:\n",
    "    print(f\"  Siêu dữ liệu: {doc_retrieved.metadata}\") # Metadata:\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# 5. Save and load FAISS index\n",
    "import shutil\n",
    "save_path_faiss = \"faiss_index_demo\"\n",
    "faiss_vector_store.save_local(save_path_faiss)\n",
    "print(f\"\\nĐã lưu FAISS index vào thư mục: {save_path_faiss}\") # FAISS index saved to directory:\n",
    "\n",
    "loaded_faiss_vector_store = FAISS.load_local(\n",
    "    save_path_faiss,\n",
    "    embeddings_model,\n",
    "    allow_dangerous_deserialization=True # Necessary when loading from disk\n",
    ")\n",
    "print(f\"Đã tải lại FAISS index từ thư mục: {save_path_faiss}\") # FAISS index reloaded from directory:\n",
    "\n",
    "query_loaded_faiss = \"What is RAG and why is it important?\"\n",
    "print(f\"\\nĐang tìm kiếm với FAISS index đã tải lại cho truy vấn: '{query_loaded_faiss}'\") # Searching with reloaded FAISS index for query:\n",
    "retrieved_loaded_faiss = loaded_faiss_vector_store.similarity_search(query_loaded_faiss, k=1)\n",
    "print(\"\\n--- Kết quả tìm kiếm từ FAISS đã tải lại ---\") # --- Search Results from Reloaded FAISS ---\n",
    "for doc_retrieved in retrieved_loaded_faiss:\n",
    "    print(f\"  Nội dung: {doc_retrieved.page_content[:200]}...\") # Content:\n",
    "    print(f\"  Siêu dữ liệu: {doc_retrieved.metadata}\") # Metadata:\n",
    "\n",
    "# Clean up index directory\n",
    "if os.path.exists(save_path_faiss):\n",
    "    shutil.rmtree(save_path_faiss)\n",
    "    print(f\"\\nĐã xóa thư mục '{save_path_faiss}'.\") # Directory '{save_path_faiss}' removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chroma: Open-Source, Easy-to-Use Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chroma** is an open-source vector database designed to simplify the process of building LLM applications. It focuses on ease of use and provides intuitive APIs for embedding, storing, and searching vectors. It supports both local (in-memory or disk-backed) and client/server modes for larger applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Chroma Characteristics\n",
    "\n",
    "* **Open-Source:** Completely free and customizable.\n",
    "* **Easy to Use:** Simple, intuitive API.\n",
    "* **Flexible Storage:** Can run entirely in memory, persist data to local disk, or be deployed as a standalone server.\n",
    "* **Metadata Filtering Support:** Allows filtering search results based on document metadata. This is very useful when you want to limit searches to documents that meet certain criteria (e.g., only search in documents from a specific source or created after a certain date)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Practical with Chroma\n",
    "\n",
    "We will set up ChromaDB, store documents, and perform queries.\n",
    "\n",
    "**Preparation:**\n",
    "* Ensure you have `langchain-openai` and `chromadb` installed.\n",
    "* Set the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai chromadb\n",
    "\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import shutil\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 1. Initialize Embeddings model\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# 2. Prepare sample documents (similar to FAISS example)\n",
    "long_text_content_chroma = \"\"\"\n",
    "Chroma is an open-source vector database designed to simplify the process of building LLM applications. It focuses on ease of use and provides intuitive APIs for embedding, storing, and searching vectors.\n",
    "\n",
    "Chroma supports multiple operating modes:\n",
    "- Local (in-memory) mode: Data only exists within the current session.\n",
    "- Local (disk-backed) mode: Data is stored in a directory on disk, allowing persistence between sessions.\n",
    "- Server (client/server) mode: Deploy Chroma as a standalone service, allowing multiple client applications to connect and access data.\n",
    "\n",
    "An important feature of Chroma is the ability to filter search results based on metadata. This is very useful when you want to limit searches to documents that meet certain criteria (e.g., only search in documents from a specific source or created after a certain date).\n",
    "\n",
    "Chroma also integrates well with LangChain, allowing you to easily use it as a Vector Store for your RAG applications.\n",
    "\"\"\"\n",
    "\n",
    "doc_chroma = Document(page_content=long_text_content_chroma, metadata={\"source\": \"chroma_docs\", \"version\": \"1.0\"})\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter_chroma = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "chunks_chroma = text_splitter_chroma.split_documents([doc_chroma])\n",
    "\n",
    "print(f\"Tổng số chunks sau khi chia nhỏ cho Chroma: {len(chunks_chroma)}\\n\") # Total chunks after splitting for Chroma:\n",
    "\n",
    "# 3. Set up ChromaDB and store documents\n",
    "persist_directory = \"./chroma_db_demo\" # Directory to store Chroma data\n",
    "# Delete old directory if it exists to ensure a clean start\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "\n",
    "print(f\"Đang tạo Chroma Vector Store và lưu trữ vào: {persist_directory}...\") # Creating Chroma Vector Store and storing to:\n",
    "chroma_vector_store = Chroma.from_documents(\n",
    "    chunks_chroma,\n",
    "    embeddings_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "print(\"Đã tạo và lưu trữ Chroma Vector Store thành công.\") # Chroma Vector Store created and stored successfully.\n",
    "\n",
    "# 4. Query documents\n",
    "query_chroma = \"How does ChromaDB work and what are its modes?\"\n",
    "print(f\"\\nĐang tìm kiếm cho truy vấn: '{query_chroma}'\") # Searching for query:\n",
    "retrieved_docs_chroma = chroma_vector_store.similarity_search(query_chroma, k=2)\n",
    "\n",
    "print(\"\\n--- Kết quả tìm kiếm từ Chroma ---\") # --- Search Results from Chroma ---\n",
    "for i, doc_retrieved in enumerate(retrieved_docs_chroma):\n",
    "    print(f\"Tài liệu {i+1} (Nguồn: {doc_retrieved.metadata.get('source', 'Không rõ')}):\") # Document {i+1} (Source: {doc_retrieved.metadata.get('source', 'Unknown')}):\n",
    "    print(f\"  Nội dung: {doc_retrieved.page_content[:200]}...\") # Content:\n",
    "    print(f\"  Siêu dữ liệu: {doc_retrieved.metadata}\") # Metadata:\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# 5. Load Chroma Vector Store from disk and query\n",
    "print(f\"\\nĐang tải lại Chroma Vector Store từ: {persist_directory}...\") # Loading Chroma Vector Store from:\n",
    "loaded_chroma_vector_store = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embeddings_model # Important: must pass the embedding_function again\n",
    ")\n",
    "print(\"Đã tải lại Chroma Vector Store thành công.\") # Chroma Vector Store reloaded successfully.\n",
    "\n",
    "query_loaded_chroma = \"What are the benefits of Chroma's metadata filtering?\"\n",
    "print(f\"\\nĐang tìm kiếm với Chroma index đã tải lại cho truy vấn: '{query_loaded_chroma}'\") # Searching with reloaded Chroma index for query:\n",
    "retrieved_loaded_chroma = loaded_chroma_vector_store.similarity_search(query_loaded_chroma, k=1)\n",
    "print(\"\\n--- Kết quả tìm kiếm từ Chroma đã tải lại ---\") # --- Search Results from Reloaded Chroma ---\n",
    "for doc_retrieved in retrieved_loaded_chroma:\n",
    "    print(f\"  Nội dung: {doc_retrieved.page_content[:200]}...\") # Content:\n",
    "    print(f\"  Siêu dữ liệu: {doc_retrieved.metadata}\") # Metadata:\n",
    "\n",
    "# Clean up Chroma directory\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "    print(f\"\\nĐã xóa thư mục Chroma '{persist_directory}'.\") # Chroma directory '{persist_directory}' removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pinecone/Weaviate (Optional): Introduction to Cloud Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large-scale, production LLM applications, you will need Vector Stores that offer scalability, high durability, and easy management. **Pinecone** and **Weaviate** are two of the leading cloud-based Vector Stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Architecture and Benefits Overview\n",
    "\n",
    "* **Architecture:** Both Pinecone and Weaviate are managed services running in the cloud. They provide APIs for you to interact with, eliminating the need for you to manage infrastructure yourself.\n",
    "* **Benefits:**\n",
    "    * **Scalability:** Easily scale to handle millions or billions of vectors without worrying about resources.\n",
    "    * **Durability:** Data is backed up and protected, ensuring no loss.\n",
    "    * **High Performance:** Optimized for high-speed similarity queries.\n",
    "    * **Reliability:** Managed by the provider, ensuring high uptime.\n",
    "    * **Advanced Features:** Support complex filtering, real-time data updates, and index management features.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Basic Integration (Conceptual, Not for Deep Implementation)\n",
    "\n",
    "Integrating Pinecone or Weaviate with LangChain typically involves the following steps:\n",
    "\n",
    "1.  **Sign up for an account and get API Key:** You need to register an account on their platform (Pinecone, Weaviate Cloud) and obtain an API key along with other configuration information (e.g., environment, endpoint URL).\n",
    "2.  **Install Python library:** Install the corresponding Python SDK package (e.g., `pinecone-client`, `weaviate-client`).\n",
    "3.  **Set environment variables:** Store API keys and other configuration information in environment variables.\n",
    "4.  **Initialize Vector Store in LangChain:** Use LangChain's integration class (e.g., `PineconeVectorStore`, `WeaviateVectorStore`) and pass the necessary configuration information.\n",
    "5.  **Add and query documents:** Use methods similar to FAISS or Chroma (`from_documents`, `similarity_search`, `as_retriever`).\n",
    "\n",
    "**Configuration Example (will not run without actual account and API keys):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone configuration example (will not run without actual account and API keys)\n",
    "# Install if not already installed: pip install pinecone-client langchain-pinecone\n",
    "\n",
    "# import os\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_pinecone import PineconeVectorStore\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# os.environ[\"PINECONE_API_KEY\"] = \"YOUR_PINECONE_API_KEY\"\n",
    "# os.environ[\"PINECONE_ENVIRONMENT\"] = \"YOUR_PINECONE_ENVIRONMENT\" # e.g.: \"gcp-starter\"\n",
    "# index_name = \"my-langchain-index\"\n",
    "\n",
    "# embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# # Initialize Pinecone\n",
    "# # from pinecone import Pinecone, ServerlessSpec\n",
    "# # pinecone = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# # if index_name not in pinecone.list_indexes():\n",
    "# #     pinecone.create_index(\n",
    "# #         name=index_name,\n",
    "# #         dimension=embeddings_model.client.dimensions, # Vector size of the embedding model\n",
    "# #         metric='cosine',\n",
    "# #         spec=ServerlessSpec(cloud='aws', region='us-east-1') # Or StarterSpec()\n",
    "# #     )\n",
    "\n",
    "# # Connect to PineconeVectorStore\n",
    "# # vector_store_pinecone = PineconeVectorStore.from_existing_index(\n",
    "# #     index_name=index_name,\n",
    "# #     embedding=embeddings_model\n",
    "# # )\n",
    "\n",
    "# # Or if you want to add new documents:\n",
    "# # documents_pinecone = [Document(page_content=\"Pinecone is a cloud vector database.\", metadata={\"source\": \"pinecone_docs\"})]\n",
    "# # vector_store_pinecone = PineconeVectorStore.from_documents(\n",
    "# #     documents_pinecone,\n",
    "# #     embeddings_model,\n",
    "# #     index_name=index_name\n",
    "# # )\n",
    "\n",
    "# # query_pinecone = \"Which cloud vector database is popular?\"\n",
    "# # retrieved_docs_pinecone = vector_store_pinecone.similarity_search(query_pinecone, k=1)\n",
    "# # print(f\"Results from Pinecone: {retrieved_docs_pinecone}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the appropriate Vector Store is a crucial decision in designing your RAG application. For small projects and learning, local FAISS and Chroma are excellent choices. For production applications requiring scalability and durability, cloud solutions like Pinecone or Weaviate are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson provided a practical insight into working with **Vector Stores** in LangChain. We practiced in detail with **FAISS**, a simple yet efficient local Vector Store, including creating an index, performing similarity searches, and saving/loading the index. Next, we explored **Chroma**, a flexible open-source Vector Store, supporting both local and client/server storage, and performed similar operations. Finally, the lesson provided an overview of large-scale cloud Vector Stores like **Pinecone** and **Weaviate**, highlighting their architecture, benefits, and basic integration. Mastering these Vector Stores is the final and crucial step for you to build a complete and effective RAG system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
