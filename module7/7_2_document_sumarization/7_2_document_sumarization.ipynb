{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7.2: Automatic Document Summarization System\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-world applications, we often deal with very large documents that exceed the context window limits of Large Language Models (LLMs). Effectively summarizing these documents is a significant challenge. This lesson will introduce strategies and techniques in LangChain to build an **automatic document summarization system**, with a focus on handling large documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Strategies for Summarizing Large Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main issue when summarizing large documents is the LLM's token limit. You cannot simply feed the entire document into the prompt. Strategies for summarizing large documents typically revolve around splitting the document into smaller parts and processing each part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Summarize then Combine (Map-Reduce):**\n",
    "    * **Concept:** Split the large document into multiple small segments (chunks). Summarize each segment independently. Then, combine these small summaries into a final overall summary.\n",
    "    * **Pros:** Can handle arbitrarily long documents, segment summarization tasks can run in parallel.\n",
    "    * **Cons:** Can lose coherence and overall context if segments are summarized too independently.\n",
    "    * **LangChain Application:** Implemented using the **Map-Reduce Chain**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Iterative Refinement (Refine Chain):**\n",
    "    * **Concept:** Start by summarizing the first segment of the document. Then, take that summary and the next segment of the document, asking the LLM to refine the existing summary based on the new information. This process repeats until all segments have been processed.\n",
    "    * **Pros:** Maintains better coherence and overall context because each summary is built upon the previous one.\n",
    "    * **Cons:** Sequential process, can be slower than Map-Reduce for very large documents.\n",
    "    * **LangChain Application:** Implemented using the **Refine Chain**.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Map-Reduce Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Map-Reduce** is a common design pattern in distributed computing, and it's also very effective for summarizing large documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **How it Works:**\n",
    "    1.  **Map (Summarize Each Chunk):** LangChain splits the document into chunks. Each chunk is sent to the LLM with a prompt to generate a small summary (map step).\n",
    "    2.  **Reduce (Combine):** All small summaries from the \"Map\" step are collected. They are sent to the LLM again with a prompt to generate a final overall summary (reduce step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Pros:**\n",
    "    * **Handles Long Documents:** Capable of summarizing documents of any length.\n",
    "    * **Parallelizable:** Segment summarization tasks can be performed in parallel (though LangChain doesn't do this by default and requires additional configuration).\n",
    "    * **Conceptually Simple:** Easy to understand the processing flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Cons:**\n",
    "    * **Context Loss:** Segment summaries are generated independently, which can miss important connections or overall context spanning across multiple segments.\n",
    "    * **Fluency:** The final summary might be less fluent as it's aggregated from several disjoint summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Structure in LangChain:**\n",
    "    ```python\n",
    "    from langchain.chains.summarize import load_summarize_chain\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "    # chain.invoke({\"input_documents\": docs})\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Refine Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Refine Chain** offers a different approach, focusing on maintaining coherence and overall context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **How it Works:**\n",
    "    1.  **Initial Summary:** The first document segment is sent to the LLM to generate an initial summary.\n",
    "    2.  **Iterative Refinement:** For each subsequent document segment, the existing summary and the new document segment are sent to the LLM. The LLM is asked to \"refine\" the current summary based on the information from the new segment.\n",
    "    3.  This process repeats until all segments have been processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Pros:**\n",
    "    * **Maintains Coherence:** The final summary is often more coherent and fluent because it's built sequentially and contextually.\n",
    "    * **Better Context Capture:** Capable of capturing relationships spanning across multiple segments better than Map-Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Cons:**\n",
    "    * **Slower:** The process is sequential, so it can be significantly slower for very large documents.\n",
    "    * **Order Dependent:** The quality of the summary can be affected by the order of the document segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Structure in LangChain:**\n",
    "    ```python\n",
    "    from langchain.chains.summarize import load_summarize_chain\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "    chain = load_summarize_chain(llm, chain_type=\"refine\", verbose=True)\n",
    "    # chain.invoke({\"input_documents\": docs})\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Complex Document Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with document summarization systems, you'll encounter various document types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Multiple Files:**\n",
    "    * **Loading:** Use appropriate `DocumentLoader`s (e.g., `PyPDFLoader`, `TextLoader`, `UnstructuredFileLoader`) to load each file.\n",
    "    * **Combining:** Collect all `Document` objects from the files into a single list. Then, split and summarize this list as a large document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Complex Structures (tables, images, headings):**\n",
    "    * **Text Splitting:** This is the most crucial step. Use `RecursiveCharacterTextSplitter` or more specialized `TextSplitter`s to divide the document into meaningful chunks, trying not to cut across tables, headings, or important information blocks.\n",
    "    * **Image/Table Processing:** For images, you might need OCR (Optical Character Recognition) techniques to extract text. For tables, ensure they are converted into a format easily understandable by the LLM (e.g., Markdown table). LangChain has more advanced loaders and splitters (e.g., `UnstructuredFileLoader` combined with `UnstructuredHTMLLoader` for HTML, or `PartitionPDFLoader` for PDF) that can help extract structure better.\n",
    "    * **Metadata:** Retain metadata (e.g., page numbers, section titles) during splitting so they can be used in the prompt or for source referencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Building a System to Summarize a Collection of Large Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a document summarization system using both **Map-Reduce** and **Refine Chain** to summarize a simulated large document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have the necessary libraries installed: `langchain-openai`, `pypdf`, `tiktoken`.\n",
    "* Set the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import time\n",
    "\n",
    "# Thiết lập biến môi trường cho khóa API của OpenAI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 1. Khởi tạo LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# 2. Tạo một tài liệu lớn giả lập\n",
    "# Chúng ta sẽ lặp lại một đoạn văn bản nhiều lần để mô phỏng tài liệu dài\n",
    "long_text = \"\"\"\n",
    "LangChain là một framework mã nguồn mở giúp các nhà phát triển xây dựng các ứng dụng được hỗ trợ bởi các mô hình ngôn ngữ lớn (LLM). Nó cung cấp các công cụ và thành phần để tạo ra các chuỗi xử lý phức tạp, cho phép LLM tương tác với các nguồn dữ liệu bên ngoài, thực hiện các hành động và duy trì trạng thái cuộc trò chuyện. Các thành phần chính của LangChain bao gồm:\n",
    "\n",
    "1.  **Models:** Giao diện cho các mô hình ngôn ngữ khác nhau (LLM, Chat Models, Embeddings).\n",
    "2.  **Prompts:** Quản lý và tối ưu hóa các prompt gửi đến LLM.\n",
    "3.  **Chains:** Kết hợp các LLM và các thành phần khác thành một chuỗi các bước.\n",
    "4.  **Agents:** Cho phép LLM ra quyết định về việc sử dụng các công cụ để thực hiện hành động.\n",
    "5.  **Tools:** Các chức năng mà Agent có thể gọi để tương tác với thế giới bên ngoài (tìm kiếm web, tính toán, API).\n",
    "6.  **Retrieval:** Tích hợp các nguồn dữ liệu bên ngoài để tăng cường khả năng của LLM (ví dụ: RAG).\n",
    "7.  **Memory:** Giúp LLM duy trì ngữ cảnh cuộc trò chuyện.\n",
    "\n",
    "LangChain giúp đơn giản hóa quá trình phát triển các ứng dụng LLM phức tạp như chatbot, hệ thống Q&A, và các tác vụ tự động hóa. Nó được thiết kế để có tính mô-đun, linh hoạt và dễ dàng mở rộng.\n",
    "\"\"\" * 10 # Lặp lại 10 lần để tạo tài liệu lớn\n",
    "\n",
    "# 3. Chia nhỏ tài liệu thành các đoạn (chunks)\n",
    "# Sử dụng RecursiveCharacterTextSplitter để chia văn bản\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, # Kích thước tối đa của mỗi đoạn\n",
    "    chunk_overlap=100, # Độ chồng lấn giữa các đoạn\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "docs = [Document(page_content=long_text)] # Bọc văn bản dài vào một đối tượng Document\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Tổng số đoạn (chunks) được tạo: {len(chunks)}\") # Total chunks created:\n",
    "print(f\"Kích thước đoạn đầu tiên: {len(chunks[0].page_content)} ký tự\") # Size of the first chunk: {len(chunks[0].page_content)} characters\n",
    "print(f\"Kích thước đoạn cuối cùng: {len(chunks[-1].page_content)} ký tự\") # Size of the last chunk: {len(chunks[-1].page_content)} characters\n",
    "\n",
    "# 4. Thực hành tóm tắt với Map-Reduce Chain\n",
    "print(\"\\n--- Bắt đầu tóm tắt với Map-Reduce Chain ---\") # --- Starting summarization with Map-Reduce Chain ---\n",
    "map_reduce_chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "start_time_mr = time.time()\n",
    "map_reduce_summary = map_reduce_chain.invoke({\"input_documents\": chunks})\n",
    "end_time_mr = time.time()\n",
    "\n",
    "print(f\"\\nBản tóm tắt Map-Reduce:\\n{map_reduce_summary['output_text']}\") # Map-Reduce Summary:\n",
    "print(f\"Thời gian thực thi Map-Reduce: {end_time_mr - start_time_mr:.2f} giây\") # Map-Reduce execution time: {end_time_mr - start_time_mr:.2f} seconds\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 5. Thực hành tóm tắt với Refine Chain\n",
    "print(\"\\n--- Bắt đầu tóm tắt với Refine Chain ---\") # --- Starting summarization with Refine Chain ---\n",
    "refine_chain = load_summarize_chain(llm, chain_type=\"refine\", verbose=True)\n",
    "\n",
    "start_time_refine = time.time()\n",
    "refine_summary = refine_chain.invoke({\"input_documents\": chunks})\n",
    "end_time_refine = time.time()\n",
    "\n",
    "print(f\"\\nBản tóm tắt Refine:\\n{refine_summary['output_text']}\") # Refine Summary:\n",
    "print(f\"Thời gian thực thi Refine: {end_time_refine - start_time_refine:.2f} giây\") # Refine execution time: {end_time_refine - start_time_refine:.2f} seconds\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n--- So sánh kết quả ---\") # --- Comparing results ---\n",
    "print(\"Map-Reduce có thể nhanh hơn nhưng có thể kém mạch lạc hơn.\") # Map-Reduce might be faster but can be less coherent.\n",
    "print(\"Refine có thể chậm hơn nhưng thường tạo ra bản tóm tắt mạch lạc hơn.\") # Refine might be slower but usually produces a more coherent summary.\n",
    "\n",
    "print(\"\\n--- Kết thúc hệ thống tóm tắt tài liệu tự động ---\") # --- Ending automatic document summarization system ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "* We create a very long text string to simulate a large document.\n",
    "* `RecursiveCharacterTextSplitter` is used to split the document into smaller chunks that the LLM can process.\n",
    "* Both `map_reduce_chain` and `refine_chain` are initialized with the same LLM and document chunks.\n",
    "* `verbose=True` is used so you can see the individual LLM calls and the step-by-step summarization process of each chain type.\n",
    "* You will observe differences in execution time and the quality of the summary between the two methods. `Refine` typically takes longer due to its sequential nature, but the summary might be more fluent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson equipped you with strategies and techniques to build an **automatic document summarization system**, with a focus on handling large documents. You learned about two main strategies: **summarize then combine (Map-Reduce)**, and **iterative refinement (Refine)**, along with the pros and cons of each. We also discussed how to **handle complex document types** by text splitting and managing metadata. Finally, you **practiced building a summarization system** using both chain types on a simulated large document, helping you visualize how they work and make appropriate choices for your use cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
