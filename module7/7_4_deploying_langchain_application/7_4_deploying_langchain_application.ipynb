{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7.4: Deploying LangChain Applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building and testing your LangChain applications, the final and crucial step is to **deploy** them so users can access and use them. Deploying an LLM application can be slightly more complex than traditional web applications due to computational resource requirements and model management. This lesson will explore common deployment options, how to package applications with Docker, and how to deploy to cloud platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deployment Options for LangChain Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the type of LangChain application you build, you can choose different deployment approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **FastAPI / Flask: Building API Backend**\n",
    "    * **Concept:** If your LangChain application is part of a larger system or needs to provide LLM functionalities via an API for other frontend applications (web, mobile), you can build an API backend using web frameworks like **FastAPI** (modern, high-performance, async support) or **Flask** (lightweight, flexible).\n",
    "    * **Pros:** Provides RESTful APIs, good scalability, separation of frontend/backend.\n",
    "    * **When to use:** Building microservices, integrating into existing systems, or when you need tight control over data flow and authentication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Streamlit / Gradio: Deploying Simple Web Applications**\n",
    "    * **Concept:** For demo applications, prototypes, or internal tools that require a quick user interface, **Streamlit** and **Gradio** are excellent choices. They allow you to build web interfaces directly in Python without needing HTML/CSS/JavaScript knowledge.\n",
    "    * **Pros:** Rapid development, easy to use, suitable for interactive applications.\n",
    "    * **When to use:** Demos, MVPs (Minimum Viable Products), internal tools, or personal projects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Packaging Applications with Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Docker** is a powerful tool for **packaging your application and all its dependencies into a self-contained container**. This ensures that your application will run consistently across any environment, from your development machine to a production server in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Why use Docker?**\n",
    "    * **Consistency:** \"Works on my machine\" will work everywhere.\n",
    "    * **Environment Isolation:** Prevents dependency conflicts between applications.\n",
    "    * **Scalability:** Easily deploy multiple replicas of your application.\n",
    "    * **Dependency Management:** All necessary libraries and configurations are within the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Creating a Dockerfile:**\n",
    "    * A `Dockerfile` is a text file that contains instructions for building a Docker image.\n",
    "    * It specifies the base operating system, installs dependencies, copies source code, and configures the command to run the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Building and running a Docker image:**\n",
    "    * **Build:** `docker build -t your_app_name .` (creates an image from the Dockerfile).\n",
    "    * **Run:** `docker run -p 8501:8501 your_app_name` (runs the container from the image, mapping ports).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploying to Cloud Platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your application is packaged into a Docker container, you can deploy it to various cloud platforms. Major cloud providers offer services suitable for running containerized applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **AWS (Amazon Web Services):**\n",
    "    * **EC2 (Elastic Compute Cloud):** Virtual machines (VMs) where you have full control over the environment. Suitable for applications requiring large resources or custom configurations.\n",
    "    * **Lambda (Serverless Functions):** Run code without managing servers. Suitable for short, stateless, event-driven tasks.\n",
    "    * **ECS (Elastic Container Service) / EKS (Elastic Kubernetes Service):** Container management services for Docker applications. ECS is simpler, EKS is Kubernetes-based for more complex deployments.\n",
    "    * **App Runner:** Easy-to-use service for deploying containerized web applications and APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Google Cloud Platform (GCP):**\n",
    "    * **Cloud Run:** Serverless platform for containers. Automatically scales from zero to thousands of instances. Ideal for web applications and APIs.\n",
    "    * **App Engine:** PaaS (Platform as a Service) for deploying web applications. Easy to use, but less flexible than Cloud Run with custom containers.\n",
    "    * **Google Kubernetes Engine (GKE):** Managed Kubernetes service. Similar to AWS EKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Microsoft Azure:**\n",
    "    * **Azure App Service:** PaaS for deploying web applications, APIs, and mobile backends. Supports Docker containers.\n",
    "    * **Azure Functions:** Serverless service similar to AWS Lambda.\n",
    "    * **Azure Kubernetes Service (AKS):** Managed Kubernetes service. Similar to EKS/GKE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Managing Environment and Secrets in Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When deploying applications, you need to manage environment variables (e.g., `OPENAI_API_KEY`, `SERPAPI_API_KEY`) and secrets safely. **Never hardcode them into your source code or Dockerfile.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Environment Variables:**\n",
    "    * Cloud platforms all have mechanisms for you to configure environment variables for your application.\n",
    "    * Example: In Cloud Run, you can add environment variables directly in the UI or via the `gcloud` CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Secrets Management:**\n",
    "    * For sensitive information like API keys, you should use dedicated secrets management services from cloud providers:\n",
    "        * **AWS Secrets Manager**\n",
    "        * **Google Cloud Secret Manager**\n",
    "        * **Azure Key Vault**\n",
    "    * These services help store, manage the lifecycle, and securely grant access to secrets for your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Deploying a Streamlit Application with Docker (Conceptual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the steps to package and deploy a simple Streamlit application (similar to the chatbot in Lesson 7.1) using Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Structure:**\n",
    "\n",
    "```\n",
    "my_langchain_app/\n",
    "‚îú‚îÄ‚îÄ app.py\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îî‚îÄ‚îÄ Dockerfile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`requirements.txt` content:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit\n",
    "langchain-openai\n",
    "openai\n",
    "google-search-results\n",
    "numexpr\n",
    "requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`app.py` content (simplified version from Lesson 7.1 to focus on deployment):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_community.tools import Tool\n",
    "from langchain_community.tools.calculator.tool import Calculator\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Get environment variables. In deployment environments, these will be provided.\n",
    "# Ensure these variables are set in your deployment environment.\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "if not openai_api_key or not serpapi_api_key:\n",
    "    st.error(\"Vui l√≤ng thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng OPENAI_API_KEY v√† SERPAPI_API_KEY.\") # Please set OPENAI_API_KEY and SERPAPI_API_KEY environment variables.\n",
    "    st.stop()\n",
    "\n",
    "# --- Initialize LLM ---\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# --- Define Tools ---\n",
    "search_tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    func=SerpAPIWrapper(serpapi_api_key=serpapi_api_key).run,\n",
    "    description=\"H·ªØu √≠ch khi b·∫°n c·∫ßn t√¨m ki·∫øm th√¥ng tin tr√™n Google v·ªÅ c√°c s·ª± ki·ªán hi·ªán t·∫°i ho·∫∑c d·ªØ li·ªáu th·ª±c t·∫ø.\" # Useful when you need to search for information on Google about current events or factual data.\n",
    ")\n",
    "calculator_tool = Calculator()\n",
    "tools = [search_tool, calculator_tool]\n",
    "\n",
    "# --- Set up Memory for Agent ---\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# --- Define Prompt for Agent ---\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"B·∫°n l√† m·ªôt tr·ª£ l√Ω ƒëa ch·ª©c nƒÉng h·ªØu √≠ch. B·∫°n c√≥ quy·ªÅn truy c·∫≠p v√†o c√°c c√¥ng c·ª• sau: {tools}. S·ª≠ d·ª•ng ch√∫ng ƒë·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng. H√£y duy tr√¨ ng·ªØ c·∫£nh cu·ªôc tr√≤ chuy·ªán v√† tr·∫£ l·ªùi m·ªôt c√°ch t·ª± nhi√™n.\"), # You are a helpful multi-functional assistant. You have access to the following tools: {tools}. Use them to answer user questions. Maintain conversation context and respond naturally.\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# --- Create Agent and Agent Executor ---\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# --- Streamlit User Interface ---\n",
    "st.set_page_config(page_title=\"Chatbot Tri·ªÉn Khai\", page_icon=\"üöÄ\") # Deployment Chatbot\n",
    "st.title(\"üöÄ Chatbot Tri·ªÉn Khai LangChain\") # LangChain Deployment Chatbot\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "if user_query := st.chat_input(\"B·∫°n mu·ªën h·ªèi g√¨?\"): # What do you want to ask?\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_query)\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"ƒêang suy nghƒ©...\"): # Thinking...\n",
    "            try:\n",
    "                response = agent_executor.invoke({\"input\": user_query})\n",
    "                ai_response = response[\"output\"]\n",
    "            except Exception as e:\n",
    "                ai_response = f\"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {e}. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c h·ªèi c√¢u h·ªèi kh√°c.\" # Sorry, an error occurred: {e}. Please try again or ask another question.\n",
    "                st.error(ai_response)\n",
    "        \n",
    "        st.markdown(ai_response)\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "if st.button(\"X√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán\"): # Clear chat history\n",
    "    st.session_state.messages = []\n",
    "    memory.clear()\n",
    "    st.experimental_rerun()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Dockerfile` content:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S·ª≠ d·ª•ng m·ªôt image Python ch√≠nh th·ª©c l√†m base image\n",
    "FROM python:3.9-slim-buster\n",
    "\n",
    "# ƒê·∫∑t th∆∞ m·ª•c l√†m vi·ªác trong container\n",
    "WORKDIR /app\n",
    "\n",
    "# Sao ch√©p t·ªáp requirements.txt v√†o th∆∞ m·ª•c l√†m vi·ªác\n",
    "COPY requirements.txt .\n",
    "\n",
    "# C√†i ƒë·∫∑t c√°c ph·ª• thu·ªôc Python\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Sao ch√©p m√£ ngu·ªìn ·ª©ng d·ª•ng v√†o th∆∞ m·ª•c l√†m vi·ªác\n",
    "COPY . .\n",
    "\n",
    "# M·ªü c·ªïng m√† Streamlit s·ª≠ d·ª•ng (m·∫∑c ƒë·ªãnh l√† 8501)\n",
    "EXPOSE 8501\n",
    "\n",
    "# L·ªánh ƒë·ªÉ ch·∫°y ·ª©ng d·ª•ng Streamlit khi container kh·ªüi ƒë·ªông\n",
    "ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployment Steps (Conceptual):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  **Build Docker Image:**\n",
    "    ```bash\n",
    "    cd my_langchain_app\n",
    "    docker build -t my-langchain-chatbot:latest .\n",
    "    ```\n",
    "2.  **Run Locally (for testing):**\n",
    "    ```bash\n",
    "    docker run -p 8501:8501 -e OPENAI_API_KEY=\"your_openai_key\" -e SERPAPI_API_KEY=\"your_serpapi_key\" my-langchain-chatbot:latest\n",
    "    ```\n",
    "    (Replace `your_openai_key` and `your_serpapi_key` with your actual API keys. In a production environment, you would use secrets management services.)\n",
    "3.  **Push Image to Container Registry:** (e.g., Docker Hub, Google Container Registry, AWS ECR)\n",
    "    ```bash\n",
    "    # Log in to your registry\n",
    "    docker login\n",
    "\n",
    "    # Tag the image with the registry name (example for Google Container Registry)\n",
    "    docker tag my-langchain-chatbot:latest gcr.io/your-gcp-project-id/my-langchain-chatbot:latest\n",
    "\n",
    "    # Push the image\n",
    "    docker push gcr.io/your-gcp-project-id/my-langchain-chatbot:latest\n",
    "    ```\n",
    "4.  **Deploy to Cloud Platform (Example: Google Cloud Run):**\n",
    "    * Access Google Cloud Console.\n",
    "    * Search for \"Cloud Run.\"\n",
    "    * Create a new service.\n",
    "    * Select the image you just pushed from Container Registry.\n",
    "    * Configure environment variables (`OPENAI_API_KEY`, `SERPAPI_API_KEY`) in the \"Variables & Secrets\" section.\n",
    "    * Configure the container port to `8501`.\n",
    "    * Select the region and other options (CPU, RAM, auto-scaling).\n",
    "    * Deploy the service.\n",
    "\n",
    "After deployment, Cloud Run will provide you with a public URL to access your chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson equipped you with the necessary knowledge to **deploy your LangChain applications**. You learned about different **deployment options** such as building API backends with FastAPI/Flask or simple web applications with Streamlit/Gradio. We delved into the importance of **Docker** for packaging applications to ensure consistency and scalability. Finally, you explored options for **deploying to major cloud platforms** (AWS, GCP, Azure) and how to **manage environments and secrets** securely. With this knowledge, you can confidently bring your LangChain applications from idea to users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
