{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9.4: LangSmith for Evaluation and Monitoring\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we built LLM applications with LangChain and LangGraph, and learned about the importance of evaluation and monitoring. This lesson will focus in-depth on **LangSmith**, the platform built by LangChain, providing a comprehensive set of tools to effectively develop, debug, test, and monitor your LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. In-depth Introduction to LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangSmith** is a SaaS (Software as a Service) platform specifically designed to support the entire development lifecycle of LLM-powered applications. It addresses the inherent challenges of working with LLMs, such as stochasticity, difficulty in debugging complex chains, and lack of observability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **What is LangSmith?**\n",
    "    LangSmith provides an intuitive interface and APIs to log, visualize, and analyze \"traces\" of your LLM application runs. Each trace is a detailed record of every step your application takes, from LLM calls to tool usage, RAG retrieval, and custom logic.\n",
    "* **Why is LangSmith necessary?**\n",
    "    * **Complex Debugging:** LLM applications often have multiple interacting components (LLMs, Prompts, Chains, Agents, Tools, Retrievers). LangSmith helps you \"see\" what's happening internally, pinpointing exactly where errors or unexpected behaviors occur.\n",
    "    * **Understanding Agent Behavior:** Especially useful for Agents and LangGraph graphs, where control flow can be complex and iterative. LangSmith visualizes ReAct loops, Agent decisions, and tool results.\n",
    "    * **Systematic Evaluation:** Provides tools to create evaluation datasets, run automated and manual tests, and compare performance across versions.\n",
    "    * **Production Monitoring:** Tracks key metrics like latency, token cost, error rate, and output quality over time.\n",
    "    * **Iterative Improvement:** By providing detailed insights, LangSmith helps you make data-driven decisions to refine prompts, select better models, or optimize architecture.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up and Configuring LangSmith for Your Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating LangSmith into your LangChain project is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  **Install the library:**\n",
    "    ```bash\n",
    "    pip install langsmith\n",
    "    ```\n",
    "2.  **Sign up for a LangSmith account:**\n",
    "    Visit [app.langsmith.com](https://www.langchain.com/langsmith) and sign up for an account. You will receive a `LANGCHAIN_API_KEY`.\n",
    "3.  **Set environment variables:**\n",
    "    For LangChain to automatically send traces to LangSmith, you need to set these three environment variables:\n",
    "    ```bash\n",
    "    export LANGCHAIN_TRACING_V2=\"true\"\n",
    "    export LANGCHAIN_API_KEY=\"<your-langsmith-api-key>\"\n",
    "    export LANGCHAIN_PROJECT=\"<your-project-name>\" # Example: \"My First LLM App\"\n",
    "    ```\n",
    "    Or within your Python code (before initializing any LangChain components):\n",
    "    ```python\n",
    "    import os\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGSMITH_API_KEY\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"My First LLM App\"\n",
    "    ```\n",
    "    * `LANGCHAIN_TRACING_V2=\"true\"`: Enables v2 tracing.\n",
    "    * `LANGCHAIN_API_KEY`: Your API key from LangSmith.\n",
    "    * `LANGCHAIN_PROJECT`: The name of the project under which your traces will be grouped in LangSmith. This is very useful for organizing different experiments and applications.\n",
    "4.  **Initialize LangSmith Client (Optional but useful):**\n",
    "    If you want to interact directly with the LangSmith API (e.g., to create datasets, run custom evaluations), you can initialize a `Client`:\n",
    "    ```python\n",
    "    from langsmith import Client\n",
    "    langsmith_client = Client()\n",
    "    ```\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tracing and Logging Chains and Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once LangSmith is configured, every run of `Runnable`s in LangChain (including LLM calls, Chains, Agents, Tools, Retrievers) will automatically be logged and sent to your LangSmith dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Trace View:** This is the core feature. You will see a visual graph of your application's execution flow. Each box in the graph represents a step (run), and you can click on it to see details:\n",
    "    * **Inputs/Outputs:** The input and output data for that step.\n",
    "    * **Logs:** Any logs generated during execution.\n",
    "    * **Errors:** Any errors or exceptions that occurred.\n",
    "    * **Duration:** The execution time of the step.\n",
    "    * **Tokens/Cost:** The number of tokens used and estimated cost (for LLM calls).\n",
    "* **Run Details:** Each trace is a \"Run.\" You can view a list of Runs in your project, filter by time, status, or type.\n",
    "* **Agent Scratchpad:** For Agents, LangSmith clearly displays the `agent_scratchpad`, allowing you to follow the Agent's thought, action, and observation sequence through each iterative step. This is incredibly useful for debugging complex ReAct loops.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing Performance, Cost, and Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith not only helps with debugging but also provides analytical tools to understand your application's performance and cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Overview Statistics:** The project dashboard displays aggregated metrics such as total runs, total cost, and average latency.\n",
    "* **Performance Analysis:**\n",
    "    * **Latency Distribution Charts:** Helps identify unusually slow cases.\n",
    "    * **Cost Analysis:** Tracks cost over time, by model, or by call type.\n",
    "    * **Bottleneck Identification:** By viewing the execution time of each step in the trace, you can quickly identify components causing the highest latency.\n",
    "* **Issue Identification and Resolution:**\n",
    "    * **Error Filtering:** Easily filter failed Runs to inspect and understand the root cause.\n",
    "    * **A/B Comparison:** LangSmith allows you to run A/B tests by grouping Runs into different \"experiments\" and comparing metrics between them. This is very useful when you're experimenting with different prompt versions, models, or architectures.\n",
    "    * **Regression Analysis:** After making changes, you can re-run tests on the same dataset and compare traces to ensure no performance degradation.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using LangSmith to Create and Run Evaluation Datasets, Compare Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith is not just a monitoring tool but also a powerful platform for LLM application evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Creating Datasets:**\n",
    "    * You can create evaluation datasets directly in LangSmith by uploading (question, context, ground truth answer) pairs or extracting them from existing traces.\n",
    "    * This data can be used for regression testing or quality evaluation.\n",
    "* **Running Evaluations:**\n",
    "    * **Automatic Evaluators:** LangSmith has built-in automatic evaluators (e.g., measuring accuracy, factual consistency, relevance using LLM-as-a-Judge). You can configure these evaluators to run on your dataset.\n",
    "    * **Human Annotators:** You can send Runs to a group of human evaluators to manually score them based on qualitative criteria.\n",
    "    * **Version Comparison:** LangSmith allows you to run different versions of your application on the same dataset and visualize evaluation results to compare performance. This helps you make data-driven decisions about which version is better to deploy.\n",
    "* **Prompt Hub:**\n",
    "    * LangSmith also provides a Prompt Hub where you can store, version, and share your prompts. This helps ensure prompt consistency and reusability across projects.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Example: Integrating LangSmith into an Existing LangChain Application and Performing Monitoring, Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the simple Q&A system example from Lesson 9.2 and integrate LangSmith to trace runs and prepare for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have the necessary libraries installed: `langchain-openai`, `langsmith`.\n",
    "* Set the `OPENAI_API_KEY`, `LANGCHAIN_API_KEY`, `LANGCHAIN_TRACING_V2=\"true\"`, `LANGCHAIN_PROJECT=\"My LangSmith Q&A App\"` environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai langsmith\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langsmith import Client # To interact with LangSmith\n",
    "\n",
    "# --- Set environment variables for OpenAI and LangSmith API keys ---\n",
    "# Make sure you replace \"YOUR_OPENAI_API_KEY\" and \"YOUR_LANGSMITH_API_KEY\" with your actual keys.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGSMITH_API_KEY\"\n",
    "\n",
    "# Enable tracing and set LangSmith project name\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"My LangSmith Q&A App\"\n",
    "\n",
    "# Initialize LangSmith Client (optional, but useful for advanced tasks)\n",
    "langsmith_client = Client()\n",
    "\n",
    "# --- 1. Simple Q&A System (our application) ---\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "def simple_qa_system(question: str, context: str = None) -> str:\n",
    "    \"\"\"\n",
    "    A simple Q&A system that uses an LLM to answer questions based on context.\n",
    "    \"\"\"\n",
    "    qa_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful Q&A assistant. Answer the following question. If context is provided, use it to answer. Otherwise, answer based on your general knowledge.\"),\n",
    "        (\"user\", f\"Context: {context}\\n\\nQuestion: {question}\" if context else f\"Question: {question}\"),\n",
    "    ])\n",
    "    qa_chain = qa_prompt_template | llm | StrOutputParser()\n",
    "    \n",
    "    # When you call invoke/stream/batch on a Runnable, LangSmith will automatically create a trace.\n",
    "    response = qa_chain.invoke({\"question\": question, \"context\": context})\n",
    "    return response\n",
    "\n",
    "# --- 2. Test Dataset (to generate runs that LangSmith will trace) ---\n",
    "test_questions = [\n",
    "    {\"question\": \"What is the capital of Vietnam?\", \"context\": \"Hanoi is the capital of Vietnam.\"}, # Thủ đô của Việt Nam là gì? Hà Nội là thủ đô của Việt Nam.\n",
    "    {\"question\": \"Who invented the light bulb?\", \"context\": None}, # Ai là người phát minh ra bóng đèn?\n",
    "    {\"question\": \"Calculate 123 multiplied by 45.\", \"context\": None}, # Tính 123 nhân 45.\n",
    "    {\"question\": \"How many days are there in 2024?\", \"context\": \"2024 is a leap year, with 366 days.\"}, # Năm 2024 có bao nhiêu ngày? Năm 2024 là năm nhuận, có 366 ngày.\n",
    "]\n",
    "\n",
    "# --- 3. Run the application and observe on LangSmith ---\n",
    "print(\"Starting Q&A application runs to generate traces on LangSmith...\")\n",
    "\n",
    "for i, item in enumerate(test_questions):\n",
    "    question = item[\"question\"]\n",
    "    context = item[\"context\"]\n",
    "    print(f\"\\n--- Question {i+1} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    if context:\n",
    "        print(f\"Context: {context}\")\n",
    "    \n",
    "    # Call the Q&A system\n",
    "    llm_response = simple_qa_system(question, context)\n",
    "    print(f\"AI Response: {llm_response}\")\n",
    "\n",
    "print(\"\\nFinished running questions. Please check your LangSmith dashboard.\")\n",
    "print(f\"Visit: https://app.langsmith.com/projects/{os.getenv('LANGCHAIN_PROJECT')}/runs\")\n",
    "\n",
    "# --- 4. (Optional) Log custom feedback to LangSmith ---\n",
    "# You can log feedback directly from code if desired.\n",
    "# For example: Suppose you want to log feedback for the first question.\n",
    "# After simple_qa_system runs, you can get the run_id from the trace.\n",
    "# In a real environment, you would get the run_id from a callback or API response.\n",
    "\n",
    "# Example illustrating how to log feedback (not automatically run in this example)\n",
    "# run_id_example = \"some_run_id_from_langsmith_trace\"\n",
    "# try:\n",
    "#     langsmith_client.create_feedback(\n",
    "#         run_id=run_id_example,\n",
    "#         key=\"user_satisfaction\",\n",
    "#         score=5, # Example: 1-5 stars\n",
    "#         comment=\"Response was very accurate and helpful!\"\n",
    "#     )\n",
    "#     print(f\"\\nLogged feedback for run_id: {run_id_example}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nCould not log feedback: {e}. Ensure run_id is valid and you have access.\")\n",
    "\n",
    "# --- 5. (Optional) Create Dataset and run Evaluation in LangSmith ---\n",
    "# This is a conceptual/guidance section, no direct executable code here\n",
    "# because creating datasets and running evaluations are typically done via the LangSmith UI\n",
    "# or by using the LangSmith SDK in a more detailed manner.\n",
    "\n",
    "# Step 1: Create Dataset in LangSmith\n",
    "#   You can create a new dataset on the LangSmith UI and import (question, ground_truth_answer, context) pairs.\n",
    "#   Or use client.upload_examples().\n",
    "\n",
    "# Step 2: Run Evaluation on Dataset\n",
    "#   On the LangSmith UI, select your Dataset, then choose \"Run Evaluation.\"\n",
    "#   You can select LLM Evaluators (LLM-as-a-Judge) or connect to Human Evaluators.\n",
    "#   LangSmith will run your application on each example in the dataset and collect scores.\n",
    "\n",
    "# Step 3: Compare Versions\n",
    "#   After running evaluations for multiple versions (e.g., version A with old prompt, version B with new prompt),\n",
    "#   you can use the \"Compare Runs\" or \"Compare Experiments\" feature in LangSmith\n",
    "#   to view evaluation metrics and detailed traces, helping you decide which version performs better.\n",
    "\n",
    "print(\"\\n--- End of Practical ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
