{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9.1: Introduction to LLM Application Evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building applications based on Large Language Models (LLMs) using LangChain and LangGraph, the next and equally important step is to **evaluate** them. Evaluation helps us understand the application's performance, identify weaknesses, and improve quality over time. This lesson will introduce the importance of LLM application evaluation, its inherent challenges, key evaluation metrics, and common evaluation types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Evaluate LLM Applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation is an indispensable part of the development cycle for any LLM application. It ensures that the final product meets expectations and delivers real value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Quality Assurance:** Verify that the application functions as expected, without generating incorrect, irrelevant, or harmful responses.\n",
    "* **Accuracy and Reliability:** Especially crucial for applications requiring high precision (e.g., legal, medical, financial advice). Evaluation helps quantify the trustworthiness of responses.\n",
    "* **Performance Optimization:** Identify areas for improvement, such as prompt engineering, model selection, and Retrieval-Augmented Generation (RAG) configuration.\n",
    "* **Model/Strategy Comparison and Selection:** Helps make data-driven decisions when choosing between different LLMs, different RAG strategies, or different Agent configurations.\n",
    "* **Risk Mitigation:** Detect and reduce issues like hallucinations, bias, and sensitive information leakage.\n",
    "* **Progress Measurement:** Track the application's improvement across different versions.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Challenges in LLM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating LLMs is not a simple task due to some of their inherent characteristics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Stochasticity:** LLMs can generate different responses for the same input, even when using the same `temperature` or `seed`. This makes consistent evaluation challenging.\n",
    "* **Output Diversity:** There are many different ways to answer a question correctly. LLMs can produce correct but varied responses in terms of wording, structure, or length, making automatic comparison to a single \"ground truth\" answer complex.\n",
    "* **Subjectivity:** Metrics like \"fluency,\" \"relevance,\" or \"overall quality\" are often subjective and difficult to quantify objectively.\n",
    "* **Cost and Time:** Especially for manual evaluation, this is time-consuming and resource-intensive.\n",
    "* **Lack of Ground Truth Data:** Creating high-quality evaluation datasets with reliable \"ground truth\" answers for complex LLM applications is a significant challenge.\n",
    "* **Context and Multi-step Issues:** Agent applications or chatbots with conversation history need to be evaluated within the full context of the conversation, not just individual turns.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate LLMs, we use various metrics, each focusing on a specific aspect of output quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Accuracy:**\n",
    "    * The degree to which the LLM's response matches factual or correct information.\n",
    "    * Often measured by comparing the output to a known \"ground truth\" answer.\n",
    "    * Examples: Correctly answering multiple-choice questions, accurately extracting information.\n",
    "* **Relevance:**\n",
    "    * The degree to which the response is appropriate for the user's question or request.\n",
    "    * A response can be accurate but irrelevant to the context.\n",
    "    * Examples: Chatbot staying on topic, summary containing only important information.\n",
    "* **Fluency:**\n",
    "    * The degree to which the response is grammatically correct, coherent, natural-sounding, and easy to read.\n",
    "    * No spelling, grammar, or awkward sentence structure errors.\n",
    "* **Safety:**\n",
    "    * The degree to which the response does not contain harmful, discriminatory, hateful, violent, or inappropriate content.\n",
    "    * This is a crucial and increasingly emphasized metric in LLM development.\n",
    "* **Factual Consistency / Groundedness:**\n",
    "    * The degree to which the information in the response is supported by the provided data sources (especially important in RAG).\n",
    "    * Helps detect \"hallucinations\" â€“ when the LLM generates information that sounds plausible but has no factual basis.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Types of Evaluation: Qualitative and Quantitative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main approaches to evaluating LLM applications, often used in combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Qualitative Evaluation (Manual Evaluation / Human Evaluation)\n",
    "\n",
    "* **Concept:** Humans (annotators) read and score LLM responses based on qualitative criteria (such as fluency, relevance, overall quality, etc.).\n",
    "* **Pros:**\n",
    "    * Captures nuances and complex contexts that automated methods struggle with.\n",
    "    * Evaluates subjective aspects like creativity, tone, helpfulness.\n",
    "    * Especially important for metrics like safety and factual consistency.\n",
    "* **Cons:**\n",
    "    * **Costly:** Time-consuming and resource-intensive.\n",
    "    * **Inconsistent:** Results can be affected by annotator subjectivity and bias.\n",
    "    * **Difficult to Scale:** Impractical for evaluating millions of responses.\n",
    "* **When to use:** Early development stages, testing edge cases, evaluating overall quality, verifying safety/ethical issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Quantitative Evaluation (Automatic Evaluation)\n",
    "\n",
    "* **Concept:** Uses algorithms or other models to automatically score LLM responses by comparing them against known standards or reference models.\n",
    "* **Pros:**\n",
    "    * **Fast and Cost-Effective:** Can be run on large datasets.\n",
    "    * **Repeatable:** Consistent and objective results (as long as the algorithm doesn't change).\n",
    "    * **Scalable:** Ideal for regression testing and continuous integration/continuous delivery (CI/CD).\n",
    "* **Cons:**\n",
    "    * **Limited Nuance:** Struggles to capture subjective aspects or the diversity of correct responses.\n",
    "    * **Requires Reference Data:** Often needs \"ground truth\" answers or reference examples for comparison.\n",
    "    * **Can be Fooled:** LLMs can generate responses that score high on automatic metrics but are not practically useful.\n",
    "* **Common Automatic Metrics:**\n",
    "    * **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Compares generated summaries to reference summaries based on n-gram overlap (words, phrases).\n",
    "    * **BLEU (Bilingual Evaluation Understudy):** Originally for machine translation, compares n-gram overlap between generated text and reference text.\n",
    "    * **BERTScore:** Uses word embeddings from BERT to calculate semantic similarity between sentences, better than ROUGE/BLEU at capturing nuance.\n",
    "    * **LLM-as-a-Judge:** Uses a more powerful LLM to evaluate the response of another LLM. This is a hybrid approach, combining LLM reasoning capabilities with automation speed.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson provided an overview of **LLM application evaluation**. You understood **why evaluation is necessary** to ensure the quality, accuracy, and reliability of applications. We discussed the inherent **challenges** in evaluating LLMs, including stochasticity and output diversity. You also grasped the **key evaluation metrics** such as accuracy, relevance, fluency, safety, and factual consistency. Finally, we explored the two main types of evaluation: **qualitative (manual)** and **quantitative (automatic)**, along with their pros and cons and common metrics/methods for each. Mastering these concepts is fundamental to building and maintaining high-quality LLM applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
