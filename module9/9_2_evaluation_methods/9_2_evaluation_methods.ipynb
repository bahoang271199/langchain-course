{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9.2: Evaluation Methods\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lesson 9.1, we explored the reasons and challenges of evaluating LLM applications, along with important metrics. This lesson will delve into specific **evaluation methods**, from setting up datasets to using techniques like **model-based evaluation** and **A/B testing**, helping you establish an effective evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Criterion-based Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criterion-based evaluation** is a structured qualitative (manual) evaluation method where human evaluators use a predefined set of criteria to score or rank LLM responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** Instead of just judging \"good\" or \"bad,\" evaluators examine each response and score it against specific criteria such as:\n",
    "    * **Accuracy:** Is the response factually correct?\n",
    "    * **Relevance:** Does the response directly answer the question?\n",
    "    * **Fluency:** Is the grammar, spelling, and sentence structure good?\n",
    "    * **Safety:** Does it contain harmful or biased content?\n",
    "    * **Groundedness:** Is the information in the response supported by the provided context (for RAG)?\n",
    "    * **Helpfulness:** Does the response help the user solve their problem?\n",
    "* **Process:**\n",
    "    1.  **Define Criteria:** Clearly list criteria and scoring scales (e.g., 1-5 stars, Yes/No).\n",
    "    2.  **Train Evaluators:** Ensure all evaluators understand and apply criteria consistently.\n",
    "    3.  **Collect Responses:** Run the application with questions from the evaluation dataset and collect responses.\n",
    "    4.  **Manual Scoring:** Evaluators read each (question, response) pair and score it against the criteria.\n",
    "    5.  **Analyze Results:** Calculate average scores for each criterion, identify strengths and weaknesses.\n",
    "* **Pros:** Captures nuances and subjective aspects, provides detailed information for improvement.\n",
    "* **Cons:** Costly, not scalable, potentially inconsistent across evaluators.\n",
    "* **When to use:** Early development stages, testing edge cases, evaluating overall quality, verifying safety/ethical issues.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model-based Evaluation (LLM-as-a-Judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model-based evaluation** is an automated evaluation method that uses a more powerful LLM (often a larger or better-tuned model) to evaluate the output of another LLM. This technique is commonly known as **\"LLM-as-a-Judge\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** Instead of humans, a \"judge\" LLM reads the question, context (if any), and the response from the LLM being evaluated, then provides a score or commentary based on criteria provided in the prompt.\n",
    "* **Process:**\n",
    "    1.  **Select Judge LLM:** Choose an LLM with strong reasoning capabilities and an understanding of evaluation criteria.\n",
    "    2.  **Design Prompt for Judge LLM:** This prompt is extremely critical. It must instruct the judge LLM on its role, evaluation criteria, scoring scale, and desired output format.\n",
    "    3.  **Run Evaluation:** Pass the (question, LLM response) pairs through the judge LLM.\n",
    "    4.  **Analyze Results:** Collect scores and comments from the judge LLM.\n",
    "* **Pros:**\n",
    "    * **Automation and Scalability:** Can evaluate thousands or millions of responses quickly.\n",
    "    * **Captures Nuance:** Better than traditional n-gram based metrics at understanding semantics and context.\n",
    "    * **Reduced Human Cost:** Significantly reduces the need for manual evaluators.\n",
    "* **Cons:**\n",
    "    * **LLM Cost:** Calling the judge LLM still incurs cost.\n",
    "    * **Judge LLM Bias:** The judge LLM might have its own biases or not fully understand complex criteria.\n",
    "    * **Consistency:** There might still be variability in the judge LLM's scores.\n",
    "* **When to use:** Large-scale evaluation, regression testing, quick comparison of model versions.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A/B Testing Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A/B testing** is a production environment evaluation method where different versions of an application are deployed to separate user groups to compare real-world performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** Divide users into two or more groups. Each group is exposed to a different version of the application (e.g., version A is the current version, version B is a new version with LLM/Agent improvements). User interaction metrics are collected and compared.\n",
    "* **Metrics Measured:**\n",
    "    * **Conversion Rate:** E.g., the percentage of users completing a task (placing an order, signing up).\n",
    "    * **Engagement:** Time spent using, number of questions asked.\n",
    "    * **User Satisfaction:** Through surveys, ratings.\n",
    "    * **Churn Rate:** The percentage of users who stop using the application.\n",
    "    * **Error Rate:** Number of times the Agent fails to respond or gives an undesirable response.\n",
    "* **Pros:**\n",
    "    * **Real-world Data:** Provides reliable information about performance in a real environment.\n",
    "    * **Direct User Feedback:** Captures actual user experience.\n",
    "* **Cons:**\n",
    "    * **Traffic Requirements:** Needs enough users for statistically significant results.\n",
    "    * **Time-Consuming:** Can take a long time to collect enough data.\n",
    "    * **Risk:** The new version might perform worse, affecting user experience.\n",
    "    * **Complex to Implement:** Requires A/B testing infrastructure.\n",
    "* **When to use:** Validating improvements before widespread deployment, optimizing user experience.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting up an Evaluation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high-quality evaluation dataset is the foundation for any evaluation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Purpose:** Provides (input, expected_output/context) pairs to systematically test LLM performance.\n",
    "* **Characteristics of a Good Dataset:**\n",
    "    * **Diverse:** Includes various types of questions, topics, and complexities.\n",
    "    * **Representative:** Reflects the types of questions real users will ask.\n",
    "    * **Includes Edge Cases:** Difficult, ambiguous, or error-prone questions.\n",
    "    * **Ground Truth:** For Q&A tasks, requires accurate answers or relevant context.\n",
    "    * **Metadata:** Includes additional information like question type, difficulty, source.\n",
    "* **Data Sources:**\n",
    "    * **Historical Data:** Real conversations from existing systems.\n",
    "    * **Synthetic Data:** Generate questions and answers using LLMs or other tools.\n",
    "    * **Human-Annotated Data:** Humans create or annotate (question, answer) pairs.\n",
    "* **Format:** Typically a CSV, JSON file, or a database, with each row/object representing an evaluation case.\n",
    "    * Example for Q&A: `{\"question\": \"...\", \"context\": \"...\", \"ground_truth_answer\": \"...\"}`\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Building a Simple Evaluation Pipeline for a Q&A System or Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a simple evaluation pipeline using the **LLM-as-a-Judge** method to assess the relevance and factual consistency of responses from a simulated Q&A system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have the necessary libraries installed: `langchain-openai`, `langsmith`.\n",
    "* Set the `OPENAI_API_KEY`, `LANGCHAIN_API_KEY`, `LANGCHAIN_TRACING_V2=\"true\"`, `LANGCHAIN_PROJECT=\"LLM Evaluation Example\"` environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai langsmith\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langsmith import Client # To interact with LangSmith\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Set up LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGSMITH_API_KEY\" # Replace with your LangSmith API key\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"LLM Evaluation Example\" # Set your project name\n",
    "\n",
    "# Initialize LangSmith Client\n",
    "langsmith_client = Client()\n",
    "\n",
    "# --- 1. Simulated Q&A System (LLM being evaluated) ---\n",
    "# This is the LLM whose response quality we want to evaluate.\n",
    "target_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "def simple_qa_system(question: str, context: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Simulated Q&A system that uses an LLM to answer questions based on context.\n",
    "    \"\"\"\n",
    "    qa_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful Q&A assistant. Answer the following question. If context is provided, use it to answer. Otherwise, answer based on your general knowledge.\"),\n",
    "        (\"user\", f\"Context: {context}\\n\\nQuestion: {question}\" if context else f\"Question: {question}\"),\n",
    "    ])\n",
    "    qa_chain = qa_prompt_template | target_llm | StrOutputParser()\n",
    "    response = qa_chain.invoke({\"question\": question, \"context\": context})\n",
    "    return response\n",
    "\n",
    "# --- 2. Evaluation Dataset ---\n",
    "# Each item includes: question, context (if any), and ground truth answer\n",
    "evaluation_dataset = [\n",
    "    {\n",
    "        \"question\": \"Thủ đô của Pháp là gì?\", # What is the capital of France?\n",
    "        \"context\": \"Paris là thủ đô và thành phố đông dân nhất của Pháp.\", # Paris is the capital and most populous city of France.\n",
    "        \"ground_truth_answer\": \"Paris\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ai là người đầu tiên đặt chân lên mặt trăng?\", # Who was the first person to step on the moon?\n",
    "        \"context\": \"Neil Armstrong là người Mỹ đầu tiên đặt chân lên mặt trăng vào năm 1969.\", # Neil Armstrong was the first American to step on the moon in 1969.\n",
    "        \"ground_truth_answer\": \"Neil Armstrong\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Tính 15 cộng 7.\", # Calculate 15 plus 7.\n",
    "        \"context\": None, # No context for calculation question\n",
    "        \"ground_truth_answer\": \"22\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Thời tiết hôm nay ở Đà Nẵng thế nào?\", # What's the weather like today in Da Nang?\n",
    "        \"context\": \"Thời tiết Đà Nẵng hôm nay nắng đẹp, nhiệt độ 30 độ C.\", # The weather in Da Nang today is sunny and beautiful, 30 degrees Celsius.\n",
    "        \"ground_truth_answer\": \"Nắng đẹp, 30 độ C\" # Sunny and beautiful, 30 degrees Celsius\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Màu sắc của bầu trời vào ban đêm?\", # What color is the sky at night?\n",
    "        \"context\": None,\n",
    "        \"ground_truth_answer\": \"Màu đen hoặc xanh đậm\" # Black or dark blue\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- 3. LLM Judge ---\n",
    "# This LLM will evaluate the responses of the target_llm\n",
    "judge_llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0) # Use a more powerful model as judge\n",
    "\n",
    "def evaluate_with_llm_judge(question: str, context: str, ground_truth: str, llm_response: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses a judge LLM to evaluate the relevance and factual consistency of a response.\n",
    "    \"\"\"\n",
    "    judge_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an AI evaluation expert. Your task is to assess the quality of a response from another AI.\n",
    "        You will be provided with:\n",
    "        - The original user question.\n",
    "        - The context provided to the AI (if any).\n",
    "        - The human ground truth answer.\n",
    "        - The AI's response to be evaluated.\n",
    "\n",
    "        Evaluate the AI's response based on the following 2 criteria:\n",
    "        1.  **Relevance:** Does the AI's response directly answer the question? (0 = not relevant, 1 = partially relevant, 2 = fully relevant)\n",
    "        2.  **Factual Consistency / Groundedness:** Is the information in the AI's response supported by the provided context (if any) and/or reliable general knowledge? (0 = not consistent/hallucination, 1 = partially consistent, 2 = fully consistent)\n",
    "\n",
    "        Respond in the following JSON format:\n",
    "        ```json\n",
    "        {{\n",
    "            \"relevance_score\": <relevance score>,\n",
    "            \"factual_consistency_score\": <factual consistency score>,\n",
    "            \"reasoning\": \"<reasoning for scores>\"\n",
    "        }}\n",
    "        ```\n",
    "        \"\"\"\"),\n",
    "        (\"user\", f\"\"\"\n",
    "        Original Question: {question}\n",
    "        Context: {context if context else \"None\"}\n",
    "        Ground Truth Answer: {ground_truth}\n",
    "        AI Response: {llm_response}\n",
    "        \"\"\")\n",
    "    ])\n",
    "    \n",
    "    judge_chain = judge_prompt_template | judge_llm | StrOutputParser()\n",
    "    \n",
    "    try:\n",
    "        raw_evaluation = judge_chain.invoke({\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"llm_response\": llm_response\n",
    "        })\n",
    "        # Attempt to parse JSON. If LLM doesn't return valid JSON, it will raise an error.\n",
    "        evaluation_result = json.loads(raw_evaluation)\n",
    "        return evaluation_result\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON from judge LLM: {e}. Raw response: {raw_evaluation}\")\n",
    "        return {\n",
    "            \"relevance_score\": -1, # Error score\n",
    "            \"factual_consistency_score\": -1,\n",
    "            \"reasoning\": f\"JSON parsing error: {e}. Raw response: {raw_evaluation}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling judge LLM: {e}\")\n",
    "        return {\n",
    "            \"relevance_score\": -1,\n",
    "            \"factual_consistency_score\": -1,\n",
    "            \"reasoning\": f\"Error calling judge LLM: {e}\"\n",
    "        }\n",
    "\n",
    "# --- 4. Main Evaluation Pipeline ---\n",
    "import json # Import json module\n",
    "\n",
    "def run_evaluation_process(dataset: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Runs the evaluation pipeline for the entire dataset.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i, item in enumerate(dataset):\n",
    "        print(f\"\\n--- Evaluating case {i+1}/{len(dataset)} ---\")\n",
    "        question = item[\"question\"]\n",
    "        context = item[\"context\"]\n",
    "        ground_truth = item[\"ground_truth_answer\"]\n",
    "\n",
    "        # Step 1: Get response from our Q&A system\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Context: {context}\")\n",
    "        llm_response = simple_qa_system(question, context)\n",
    "        print(f\"AI Response: {llm_response}\")\n",
    "\n",
    "        # Step 2: Evaluate the response using the LLM judge\n",
    "        evaluation = evaluate_with_llm_judge(question, context, ground_truth, llm_response)\n",
    "        print(f\"Evaluation Result: {evaluation}\")\n",
    "        results.append(evaluation)\n",
    "    \n",
    "    # --- Aggregate Results ---\n",
    "    total_relevance_score = 0\n",
    "    total_factual_consistency_score = 0\n",
    "    valid_evaluations = 0\n",
    "\n",
    "    for res in results:\n",
    "        if res[\"relevance_score\"] != -1 and res[\"factual_consistency_score\"] != -1:\n",
    "            total_relevance_score += res[\"relevance_score\"]\n",
    "            total_factual_consistency_score += res[\"factual_consistency_score\"]\n",
    "            valid_evaluations += 1\n",
    "    \n",
    "    if valid_evaluations > 0:\n",
    "        avg_relevance = total_relevance_score / valid_evaluations\n",
    "        avg_factual_consistency = total_factual_consistency_score / valid_evaluations\n",
    "        print(f\"\\n--- Aggregated Results ---\")\n",
    "        print(f\"Average Relevance Score: {avg_relevance:.2f}\")\n",
    "        print(f\"Average Factual Consistency Score: {avg_factual_consistency:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nNo valid evaluations were performed.\")\n",
    "\n",
    "# --- Execute the evaluation pipeline ---\n",
    "print(\"Starting evaluation process...\")\n",
    "run_evaluation_process(evaluation_dataset)\n",
    "print(\"\\nEvaluation process completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
