{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2.2: LLMChain and Runnable\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we learned about core LangChain concepts like Models, Prompts, and Output Parsers. This lesson will focus on how to connect these components to form a complete processing flow, known as **Chains**. We'll start with the basic **LLMChain** concept and then move to the more modern and flexible approach using **LangChain Expression Language (LCEL)** and the `|` (pipe operator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to LLMChain (Basic Concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What is LLMChain?\n",
    "\n",
    "In the traditional sense of LangChain, an **LLMChain** is the simplest chain, designed to connect a **PromptTemplate** with an **LLM** (or Chat Model). It takes an input, formats it according to the `PromptTemplate`, sends the formatted prompt to the LLM, and receives a response from the LLM.\n",
    "\n",
    "* **Relationship:** This is the most basic building block for making a structured LLM call. It acts as a bridge between defining the prompt and invoking the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Structure of LLMChain\n",
    "\n",
    "A basic `LLMChain` consists of:\n",
    "1.  **PromptTemplate:** To define the prompt structure and input variables.\n",
    "2.  **LLM/Chat Model:** The large language model that will process the prompt.\n",
    "\n",
    "While `LLMChain` is an important concept to understand, in recent versions of LangChain, the recommended approach for building chains is to use **LangChain Expression Language (LCEL)** with the `|` operator. LCEL provides much greater flexibility and composability. We will focus on LCEL in the practical examples.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Practical Examples of Building LLM Chains (using LCEL) for Various Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly using the `LLMChain` class, we will build equivalent chains by combining **Runnables** with the `|` operator. This makes your code cleaner, more readable, and easier to extend.\n",
    "\n",
    "To run the examples below, ensure you have installed the necessary libraries and set up your API keys (as learned in Lesson 1.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Initialize Chat Model (using gpt-3.5-turbo as it's versatile)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Creative Text Generation (e.g., writing poetry, short stories)\n",
    "\n",
    "This is one of the most popular applications of LLMs. We'll build a simple chain to generate a creative text snippet based on a theme.\n",
    "\n",
    "* **Goal:** Write a poem or a short story.\n",
    "* **Components:** `ChatPromptTemplate` (to guide role and theme) -> `ChatOpenAI` (or equivalent LLM) -> `StrOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creative text generation: Write poetry\n",
    "print(\"--- Generating Poetry ---\")\n",
    "creative_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a talented poet, specializing in Vietnamese lục bát poetry.\"),\n",
    "    HumanMessage(content=\"Write a short lục bát poem about the theme: {theme}\"),\n",
    "])\n",
    "\n",
    "creative_chain = creative_prompt | llm | StrOutputParser()\n",
    "\n",
    "theme_poem = \"spring drizzle\"\n",
    "poem_response = creative_chain.invoke({\"theme\": theme_poem})\n",
    "print(f\"Theme: '{theme_poem}'\\nPoem:\\n{poem_response}\\n\")\n",
    "\n",
    "# Creative text generation: Write a short story\n",
    "print(\"--- Generating Short Story ---\")\n",
    "story_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a fairy tale storyteller. Write a short story of about 100 words.\"),\n",
    "    HumanMessage(content=\"Write a story about {character} and {unexpected_event}.\"),\n",
    "])\n",
    "\n",
    "story_chain = story_prompt | llm | StrOutputParser()\n",
    "\n",
    "story_response = story_chain.invoke({\"character\": \"a kitten\", \"unexpected_event\": \"finding a glowing stone\"})\n",
    "print(f\"Story:\\n{story_response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Text Summarization\n",
    "\n",
    "LLMs are excellent at distilling key information from long texts.\n",
    "\n",
    "* **Goal:** Summarize a text into a single sentence or a few key points.\n",
    "* **Components:** `ChatPromptTemplate` (to instruct summarization) -> `ChatOpenAI` -> `StrOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summarization\n",
    "print(\"--- Summarizing Text ---\")\n",
    "summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a summarization assistant. Summarize the following paragraph into a single sentence.\"),\n",
    "    HumanMessage(content=\"Paragraph: {text}\"),\n",
    "])\n",
    "\n",
    "summarize_chain = summarize_prompt | llm | StrOutputParser()\n",
    "\n",
    "long_text = \"\"\"\n",
    "LangChain is an open-source framework designed to help developers build applications powered by Large Language Models (LLMs) more easily and efficiently. It provides a set of tools, components, and abstractions to simplify complex processes related to LLMs, from prompt management to connecting LLMs with external data sources and tools. Key components include Models, Prompts, Chains, Agents, Retrieval, and Memory.\n",
    "\"\"\"\n",
    "summary_response = summarize_chain.invoke({\"text\": long_text})\n",
    "print(f\"Summary:\\n{summary_response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Language Translation\n",
    "\n",
    "LLMs have impressive capabilities in translating between various languages.\n",
    "\n",
    "* **Goal:** Translate a sentence or paragraph from one language to another.\n",
    "* **Components:** `ChatPromptTemplate` (to specify source and target languages) -> `ChatOpenAI` -> `StrOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language translation\n",
    "print(\"--- Translating Language ---\")\n",
    "translate_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a professional translator. Translate the following text from {source_language} to {target_language}.\"),\n",
    "    HumanMessage(content=\"Text: {text}\"),\n",
    "])\n",
    "\n",
    "translate_chain = translate_prompt | llm | StrOutputParser()\n",
    "\n",
    "text_to_translate = \"Hello, how are you today?\"\n",
    "source_lang = \"English\"\n",
    "target_lang = \"Vietnamese\"\n",
    "translation_response = translate_chain.invoke({\n",
    "    \"text\": text_to_translate,\n",
    "    \"source_language\": source_lang,\n",
    "    \"target_language\": target_lang\n",
    "})\n",
    "print(f\"Original text ({source_lang}): '{text_to_translate}'\\nTranslated to ({target_lang}): '{translation_response}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Text Classification\n",
    "\n",
    "LLMs can be used to classify text into specific categories (e.g., sentiment, topic).\n",
    "\n",
    "* **Goal:** Classify the sentiment of a sentence.\n",
    "* **Components:** `ChatPromptTemplate` (to define classification labels) -> `ChatOpenAI` -> `StrOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification\n",
    "print(\"--- Classifying Text ---\")\n",
    "classify_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a sentiment classification system. Classify the sentiment of the following sentence as 'positive', 'negative', or 'neutral'.\"),\n",
    "    HumanMessage(content=\"Sentence: {sentence}\"),\n",
    "])\n",
    "\n",
    "classify_chain = classify_prompt | llm | StrOutputParser()\n",
    "\n",
    "sentence_to_classify_1 = \"I really like this course, it's very helpful!\"\n",
    "classification_response_1 = classify_chain.invoke({\"sentence\": sentence_to_classify_1})\n",
    "print(f\"Sentence: '{sentence_to_classify_1}'\\nSentiment: {classification_response_1}\\n\")\n",
    "\n",
    "sentence_to_classify_2 = \"Today I'm not feeling well.\"\n",
    "classification_response_2 = classify_chain.invoke({\"sentence\": sentence_to_classify_2})\n",
    "print(f\"Sentence: '{sentence_to_classify_2}'\\nSentiment: {classification_response_2}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introduction to LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. What is LCEL?\n",
    "\n",
    "**LangChain Expression Language (LCEL)** is a powerful and flexible way to build chains in LangChain. It was introduced to provide a clear, readable, and highly composable syntax for creating complex pipelines from LangChain's basic components. LCEL allows you to chain together various \"Runnables.\"\n",
    "\n",
    "* **Historical Development:** LCEL was created to overcome the limitations of older chain building methods (like `SequentialChain` or `SimpleSequentialChain`), which could become cumbersome and difficult to manage as chains grew more complex. It brings the flexibility of graph building but with a simpler syntax.\n",
    "* **Relationship:** LCEL is a core part of the modern LangChain architecture, especially when you need to build custom or complex processing flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Benefits of LCEL\n",
    "\n",
    "* **Composability:** Easily combine `Runnables` with each other.\n",
    "* **Streaming:** Supports streaming output tokens, making applications feel faster.\n",
    "* **Async support:** Allows running chains asynchronously for improved performance.\n",
    "* **Batching:** Efficiently processes multiple inputs at once.\n",
    "* **Fallback:** Easily set up fallback models or chains in case of errors.\n",
    "* **Parallelism:** Run branches of a chain in parallel.\n",
    "* **Logging & Inspectability:** Easy integration with LangSmith for debugging and monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using `|` (Pipe Operator) to Connect Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LCEL, any component that can be chained together is called a **Runnable**. These `Runnables` include `PromptTemplate`, LLM/Chat Model, `OutputParser`, `Retriever`, and even custom Python functions.\n",
    "\n",
    "The `|` (pipe operator) is the primary syntax for connecting these `Runnables` into a pipeline. It works similarly to the pipe operator in a shell (`command1 | command2`), where the output of one command becomes the input of the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. `Runnable` Concept\n",
    "\n",
    "A `Runnable` is an object that has an `invoke()` or `stream()` method (and asynchronous methods `ainvoke()`, `astream()`) that takes an input and returns an output.\n",
    "\n",
    "* **Examples of `Runnable`:**\n",
    "    * `PromptTemplate`\n",
    "    * `ChatOpenAI` (or any LLM/Chat Model)\n",
    "    * `StrOutputParser` (or any Output Parser)\n",
    "    * `RunnablePassthrough` (to pass input directly)\n",
    "    * `RunnableParallel` (to run branches in parallel)\n",
    "    * Python functions wrapped with `@tool` or `RunnableLambda`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. How to Connect with `|` (Pipe Operator)\n",
    "\n",
    "When you use `A | B`, it means that the output of `A` will be passed as the input to `B`.\n",
    "\n",
    "* **Syntax:** `runnable_1 | runnable_2 | runnable_3`\n",
    "\n",
    "* **Comprehensive Chain Example using LCEL:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough # To pass data through\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Define Runnable components\n",
    "# 1. Prompt Template\n",
    "prompt_component = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a creative assistant, specializing in generating startup ideas based on market trends.\"),\n",
    "    HumanMessage(content=\"Suggest 3 novel startup ideas in the {field} sector and briefly explain each.\"),\n",
    "])\n",
    "\n",
    "# 2. LLM/Chat Model (already initialized above)\n",
    "# llm = ChatOpenAI(...)\n",
    "\n",
    "# 3. Output Parser\n",
    "parser_component = StrOutputParser()\n",
    "\n",
    "# Connect Runnables using the pipe operator (|)\n",
    "# The input to the chain is a dictionary containing 'field'\n",
    "# The Prompt will receive 'field' and generate messages\n",
    "# The LLM will receive messages and generate a response\n",
    "# The Parser will extract the string content from the LLM's response\n",
    "startup_idea_chain = prompt_component | llm | parser_component\n",
    "\n",
    "# Execute the chain\n",
    "field_of_interest = \"green technology\"\n",
    "ideas = startup_idea_chain.invoke({\"field\": field_of_interest})\n",
    "\n",
    "print(f\"Startup ideas in the '{field_of_interest}' sector:\\n{ideas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flow Explanation:**\n",
    "1.  `startup_idea_chain.invoke({\"field\": field_of_interest})`: The input is a dictionary `{\"field\": \"green technology\"}`.\n",
    "2.  `prompt_component`: Receives this dictionary, fills the \"green technology\" value into the `{field}` placeholder in the `HumanMessage`, and generates a list of messages.\n",
    "3.  `llm`: Receives the list of messages from `prompt_component`, processes it, and generates an `AIMessage` object (the LLM's response).\n",
    "4.  `parser_component`: Receives the `AIMessage` object from `llm`, extracts the string content of that message.\n",
    "5.  The final result is a text string containing 3 startup ideas.\n",
    "\n",
    "LCEL is an incredibly powerful tool that you will use extensively in LangChain to build complex processing flows, from simple chains like the ones above to more intricate graphs with LangGraph (to be covered later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson introduced the concept of **LLMChain** as the most basic chain in LangChain, connecting a Prompt Template to an LLM. We practiced building equivalent chains (using LCEL) to perform common tasks such as **creative text generation, text summarization, language translation, and text classification**. The core focus of the lesson was to introduce **LangChain Expression Language (LCEL)**, a modern and flexible approach to building chains. We understood that LCEL allows chaining **Runnables** (like Prompts, LLMs, Parsers) using the `|` (pipe operator), creating clear, powerful, and extensible pipelines. Mastering LCEL is a crucial step towards building more complex and effective LLM applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
