{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2.1: Working with Prompts and Prompt Templates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the world of Large Language Models (LLMs), how you \"talk\" to them (i.e., how you construct the **prompt**) greatly influences the quality and relevance of the response. This lesson will delve into the importance of **Prompt Engineering** and how to use LangChain's tools, especially **Prompt Templates**, to create effective prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Importance of Prompt Engineering in Controlling LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What is Prompt Engineering?\n",
    "\n",
    "**Prompt Engineering** is the art and science of designing effective prompts to communicate with Large Language Models (LLMs) and guide them to generate desired outputs. It involves selecting words, sentence structures, formatting, and examples (if any) to maximize the LLM's ability to perform a specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Why is Prompt Engineering Important?\n",
    "\n",
    "* **Output Control:** A well-designed prompt can direct the LLM to produce accurate, relevant, and desired format responses. A poor prompt can lead to irrelevant, ambiguous, or hallucinated responses.\n",
    "* **Performance Optimization:** With the same LLM, different prompts can yield significantly different performance in terms of accuracy, fluency, and creativity.\n",
    "* **Error and Bias Reduction:** Prompt Engineering can help minimize undesirable responses, such as unsafe or biased content, by setting clear constraints.\n",
    "* **Cost and Time Savings:** An effective prompt can reduce the number of tokens required for each LLM call, thereby saving API costs and speeding up processing. It also reduces the time needed for fine-tuning and debugging applications.\n",
    "* **Unlocking New Capabilities:** Advanced prompt engineering techniques can \"unlock\" hidden capabilities of LLMs that would otherwise not manifest without the right prompt (e.g., complex reasoning abilities).\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Basic PromptTemplate with Input Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LangChain, **PromptTemplate** is a powerful class that helps you build prompts in a structured, readable, and reusable way. Instead of manual string concatenation, you define placeholders (variables) within the prompt and fill them with data later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. `PromptTemplate` Concept\n",
    "\n",
    "* `PromptTemplate` allows you to create a prompt string with input variables. When you `format` the template, these variables will be replaced with actual values.\n",
    "* This is especially useful when you want to perform the same type of task with different inputs (e.g., summarizing various paragraphs, translating different languages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. How to Create and Use `PromptTemplate`\n",
    "\n",
    "You can create a `PromptTemplate` from a simple template string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if not already installed: pip install langchain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1. Define PromptTemplate with input variables\n",
    "# Variables are enclosed in curly braces: {variable_name}\n",
    "template = \"Write a short story about {character} in {setting}.\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# 2. Use PromptTemplate to create specific prompts\n",
    "# Call the .format() method and pass values for the variables\n",
    "prompt_1 = prompt_template.format(character=\"a dog\", setting=\"an old house\")\n",
    "print(f\"Prompt 1:\\n{prompt_1}\\n\")\n",
    "\n",
    "prompt_2 = prompt_template.format(character=\"a fairy\", setting=\"a magical forest\")\n",
    "print(f\"Prompt 2:\\n{prompt_2}\\n\")\n",
    "\n",
    "# Connect with LLM (illustrative example, requires LLM installation and configuration)\n",
    "# from langchain_openai import OpenAI\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "# llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.7)\n",
    "# response = llm.invoke(prompt_1)\n",
    "# print(f\"LLM Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "* `PromptTemplate.from_template(template)`: Creates a `PromptTemplate` object from the `template` string.\n",
    "* `prompt_template.format(...)`: Fills in the values for the `{character}` and `{setting}` placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using ChatPromptTemplate for Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat Models (like `gpt-3.5-turbo`, `gemini-pro`) work best when they receive a list of messages with specific roles (`system`, `human`, `ai`). LangChain's **ChatPromptTemplate** is designed to easily create these message lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. `ChatPromptTemplate` Concept\n",
    "\n",
    "* `ChatPromptTemplate` allows you to define the structure of the conversation using different message types:\n",
    "    * `SystemMessagePromptTemplate`: Provides general context or instructions to the LLM.\n",
    "    * `HumanMessagePromptTemplate`: Represents a message from the user.\n",
    "    * `AIMessagePromptTemplate`: Represents a message from the AI (often used to provide examples).\n",
    "* Each message can contain variables similar to `PromptTemplate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. How to Create and Use `ChatPromptTemplate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if not already installed: pip install langchain-openai openai\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Initialize Chat Model\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# 1. Define ChatPromptTemplate from a list of messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a professional text summarizer. Summarize the following text into 3 bullet points.\"),\n",
    "    HumanMessage(content=\"Text to summarize: {text}\"),\n",
    "])\n",
    "\n",
    "# 2. Build the Chain\n",
    "# Using LCEL: prompt | model | parser\n",
    "chain = chat_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# 3. Execute the Chain\n",
    "long_text = \"\"\"\n",
    "Artificial intelligence (AI) is changing the world at a rapid pace. From self-driving cars to virtual assistants, AI is permeating every aspect of life.\n",
    "One of the biggest breakthroughs in AI recently is the development of Large Language Models (LLMs). These models, such as OpenAI's GPT-4 or Google's Gemini, are capable of understanding and generating natural language in an astonishing way.\n",
    "However, building practical applications with LLMs is not simple. Developers often face challenges in prompt management, connecting LLMs to external data, and building complex logical chains. This is where LangChain comes into play.\n",
    "LangChain is an open-source framework that simplifies this process, providing tools to connect LLMs with data sources, tools, and other components, allowing for the construction of more powerful and flexible AI applications.\n",
    "\"\"\"\n",
    "response = chain.invoke({\"text\": long_text})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "* `ChatPromptTemplate.from_messages(...)`: Creates a `ChatPromptTemplate` from a list of message objects.\n",
    "* `SystemMessage(content=...)`: Sets the role or general instruction for the AI.\n",
    "* `HumanMessage(content=...)`: Represents the user's input, which can contain variables (`{text}`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Prompt Engineering Techniques: Few-shot Prompting, Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These techniques relate to how you provide examples to the LLM to help it better understand the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Zero-shot Prompting\n",
    "\n",
    "* **Concept:** You don't provide any examples to the LLM. You simply give direct instructions and expect the LLM to perform the task based on its pre-trained knowledge.\n",
    "* **When to Use:** Good for simple, straightforward tasks or when you don't have enough examples to provide.\n",
    "* **Examples:** \"Summarize the following paragraph:\", \"Translate this sentence into French:\", \"Classify the sentiment of the following sentence:\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Zero-shot Prompting\n",
    "prompt_zero_shot = ChatPromptTemplate.from_messages([\n",
    "    HumanMessage(content=\"Classify the sentiment of the following sentence: 'I am very happy to learn LangChain.' (positive/negative/neutral)\"),\n",
    "])\n",
    "\n",
    "chain_zero_shot = prompt_zero_shot | chat_model | StrOutputParser()\n",
    "print(f\"Zero-shot: {chain_zero_shot.invoke({})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Few-shot Prompting\n",
    "\n",
    "* **Concept:** You provide a few examples (typically 2-5 examples) of desired input-output pairs within the prompt itself. This helps the LLM learn the expected pattern and format for the task.\n",
    "* **When to Use:** Useful for more complex tasks, those requiring a specific output format, or when the LLM needs a better understanding of context or style.\n",
    "* **Examples:**\n",
    "    * **Input:** \"Hello\" -> **Output:** \"Hi there!\"\n",
    "    * **Input:** \"Goodbye\" -> **Output:** \"See you later!\"\n",
    "    * **Input:** \"How are you?\" -> **Output:** \"I'm fine, thank you!\"\n",
    "    * **Input:** \"What's your name?\" -> **Output:** \"I am a large language model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Few-shot Prompting\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# 1. Define examples\n",
    "examples = [\n",
    "    {\"input\": \"I am very happy to learn LangChain.\", \"output\": \"positive\"},\n",
    "    {\"input\": \"It's raining today, I feel sad.\", \"output\": \"negative\"},\n",
    "    {\"input\": \"This book is quite good.\", \"output\": \"neutral\"},\n",
    "]\n",
    "\n",
    "# 2. Define PromptTemplate for each example\n",
    "example_formatter_template = \"\"\"\n",
    "Sentence: {input}\n",
    "Sentiment: {output}\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "# 3. Create FewShotPromptTemplate\n",
    "# It will insert the examples into the main prompt\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Sentence: {sentence_to_classify}\\nSentiment:\", # Part after examples\n",
    "    input_variables=[\"sentence_to_classify\"],\n",
    ")\n",
    "\n",
    "# 4. Connect and execute\n",
    "chain_few_shot = few_shot_prompt | chat_model | StrOutputParser()\n",
    "print(f\"Few-shot: {chain_few_shot.invoke({'sentence_to_classify': 'The weather today is beautiful!'})}\")\n",
    "print(f\"Few-shot: {chain_few_shot.invoke({'sentence_to_classify': 'I don't really like this dish.'})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For `ChatPromptTemplate`, you would add examples as interleaved `HumanMessage` and `AIMessage` in the message list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_few_shot_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a sentiment classification assistant.\"),\n",
    "    HumanMessage(content=\"Sentence: I am very happy to learn LangChain.\"),\n",
    "    AIMessage(content=\"positive\"),\n",
    "    HumanMessage(content=\"Sentence: It's raining today, I feel sad.\"),\n",
    "    AIMessage(content=\"negative\"),\n",
    "    HumanMessage(content=\"Sentence: {sentence_to_classify}\"),\n",
    "])\n",
    "\n",
    "chain_chat_few_shot = chat_few_shot_prompt | chat_model | StrOutputParser()\n",
    "print(f\"Chat Few-shot: {chain_chat_few_shot.invoke({'sentence_to_classify': 'The weather today is beautiful!'})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimizing Prompts for Desired Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt optimization is an iterative process. Here are some principles and techniques:\n",
    "\n",
    "* **Clear and Specific:**\n",
    "    * Use clear, unambiguous language.\n",
    "    * Specify the desired output format (e.g., \"respond in JSON format\", \"list 3 bullet points\").\n",
    "    * Use delimiters (e.g., `---`, `###`) to separate different parts of the prompt (instructions, examples, input).\n",
    "\n",
    "* **Provide Context:**\n",
    "    * Explain the LLM's role (e.g., \"You are a marketing expert\", \"You are a scientist\").\n",
    "    * Provide necessary background information for the LLM to understand the problem.\n",
    "\n",
    "* **Step-by-step Instructions:**\n",
    "    * For complex tasks, break them down into clear steps. The Chain-of-Thought technique (to be covered in more detail in Module 10) is a prime example.\n",
    "    * Example: \"First, identify the main topic. Second, summarize the key points. Finally, present as a list.\"\n",
    "\n",
    "* **Constraints and Limitations:**\n",
    "    * Set limits (e.g., \"only 50 words\", \"no more than 3 sentences\").\n",
    "    * Specify content constraints (e.g., \"do not include personal information\", \"only use data from the provided text\").\n",
    "\n",
    "* **Experiment and Iterate:**\n",
    "    * Prompt Engineering is an experimental process. Try different prompt variations and compare results.\n",
    "    * Use tools like LangSmith (to be covered in Module 9) to track and evaluate your prompts.\n",
    "\n",
    "* **Use Examples (Few-shot):**\n",
    "    * If possible, provide a few high-quality examples to guide the LLM.\n",
    "\n",
    "* **Optimize Length:**\n",
    "    * Overly long prompts can be costly and sometimes reduce performance. Try to keep prompts concise but still informative.\n",
    "\n",
    "**Example of prompt optimization:**\n",
    "\n",
    "* **Poor prompt:** \"Write about dogs.\"\n",
    "* **Better prompt:** \"You are a children's story writer. Write a short story (no more than 100 words) about a brave dog that rescued a cat from a tree. The story must have a positive message about friendship.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of optimized prompt\n",
    "optimized_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a children's story writer.\"),\n",
    "    HumanMessage(content=\"\"\"\n",
    "    Write a short story (no more than 100 words) about a brave dog that rescued a cat from a tree.\n",
    "    The story must have a positive message about friendship.\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "optimized_chain = optimized_prompt | chat_model | StrOutputParser()\n",
    "print(f\"Optimized Prompt:\\n{optimized_chain.invoke({})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson emphasized the importance of **Prompt Engineering** in controlling LLM behavior and output quality. We learned how to use **PromptTemplate** to create structured and reusable prompts for traditional LLMs, and **ChatPromptTemplate** to build multi-role conversations for Chat Models. **Zero-shot prompting** and **Few-shot prompting** techniques were introduced as basic ways to guide LLMs. Finally, we explored principles and techniques for **optimizing prompts**, including clarity, providing context, step-by-step instructions, setting constraints, and the importance of iterative experimentation to achieve desired results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
