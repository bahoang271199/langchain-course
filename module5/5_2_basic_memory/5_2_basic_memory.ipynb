{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5.2: Basic Memory Types\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lesson 5.1, we learned about the importance of **Memory** in helping Large Language Models (LLMs) maintain context in conversations. This lesson will focus on two of the most basic and commonly used Memory types in LangChain: **ConversationBufferMemory** and **ConversationBufferWindowMemory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Concept and How it Works\n",
    "\n",
    "**ConversationBufferMemory** is the simplest type of Memory. It stores the **entire conversation history** (both user input and AI output) as a string or a list of message objects.\n",
    "\n",
    "* **Storage:** Each time there's a new conversation turn (user asks, AI responds), ConversationBufferMemory adds that (input, output) pair to its buffer.\n",
    "* **Retrieval:** When the LLM needs context, the entire content of the buffer is passed into the prompt.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Pros and Cons\n",
    "\n",
    "* **Pros:**\n",
    "    * **Simplicity:** Very easy to set up and use.\n",
    "    * **Full Context Retention:** Ensures no information is lost from previous conversation turns.\n",
    "    * **Suitable for Short Conversations:** Effective for interactions that are not too long.\n",
    "\n",
    "* **Cons:**\n",
    "    * **Risk of Memory/Token Overflow:** If the conversation becomes too long, the size of the conversation history can exceed the LLM's token limits, leading to errors or truncation of information.\n",
    "    * **Increased Cost:** Passing a large number of tokens in each LLM call will increase API costs.\n",
    "    * **Reduced Performance:** The LLM has to process a large amount of potentially unnecessary text, which can slow down response times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Practical Example Using ConversationBufferMemory with ConversationChain\n",
    "\n",
    "We will use `ConversationBufferMemory` with `ConversationChain` to see how it maintains context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 1. Initialize LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# 2. Initialize ConversationBufferMemory\n",
    "# By default, it will store history as a string in the 'history' variable\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# 3. Define Prompt for ConversationChain\n",
    "# This prompt will have a placeholder for conversation history.\n",
    "# ConversationChain will automatically populate this placeholder.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer user questions. Maintain conversation context.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"), # Placeholder for conversation history\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# 4. Initialize ConversationChain with Memory\n",
    "# verbose=True to see the chain's operation, including the history passed to the LLM\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt, # Pass custom prompt\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"--- Practical Example with ConversationBufferMemory ---\")\n",
    "\n",
    "# Conversation turn 1\n",
    "user_input_1 = \"What is the capital of France?\"\n",
    "print(f\"\\nYou: {user_input_1}\")\n",
    "response_1 = conversation.invoke({\"input\": user_input_1})\n",
    "print(f\"AI: {response_1['response']}\")\n",
    "\n",
    "# Conversation turn 2 (LLM remembers context)\n",
    "user_input_2 = \"What is its population?\"\n",
    "print(f\"\\nYou: {user_input_2}\")\n",
    "response_2 = conversation.invoke({\"input\": user_input_2})\n",
    "print(f\"AI: {response_2['response']}\")\n",
    "\n",
    "# Conversation turn 3 (LLM still remembers context)\n",
    "user_input_3 = \"What are its famous tourist attractions?\"\n",
    "print(f\"\\nYou: {user_input_3}\")\n",
    "response_3 = conversation.invoke({\"input\": user_input_3})\n",
    "print(f\"AI: {response_3['response']}\")\n",
    "\n",
    "print(\"\\n--- Full History Stored in Memory ---\")\n",
    "print(memory.buffer) # View the entire stored history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Concept and How it Works\n",
    "\n",
    "**ConversationBufferWindowMemory** is an improvement over ConversationBufferMemory. Instead of storing the entire history, it only stores a **window** of the last `k` messages (or `k` input/output pairs). When a new message is added, the oldest message outside the window is discarded.\n",
    "\n",
    "* **`k`**: The most important parameter, defining the size of the window.\n",
    "* **Storage:** Only keeps the last `k` messages.\n",
    "* **Retrieval:** Only these `k` messages are passed into the LLM's prompt.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Pros and Cons\n",
    "\n",
    "* **Pros:**\n",
    "    * **Memory Size Control:** Prevents conversation history from becoming too large, helping to avoid exceeding LLM token limits.\n",
    "    * **Token Cost Control:** Reduces the number of tokens passed in each LLM call, saving costs.\n",
    "    * **Suitable for Short-Term Context:** Good for conversations where only recent turns are important.\n",
    "\n",
    "* **Cons:**\n",
    "    * **Loss of Long-Term Context:** Important information from conversation turns older than `k` messages will be lost, potentially causing the LLM to forget crucial details.\n",
    "    * **Careful `k` Adjustment Needed:** Choosing the right `k` value is important to balance context retention and size control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Practical Example Using ConversationBufferWindowMemory\n",
    "\n",
    "We will practice with `ConversationBufferWindowMemory` to see how it only retains a portion of the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 1. Initialize LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# 2. Initialize ConversationBufferWindowMemory with k=2\n",
    "# This means it will only remember the last 2 message pairs (input + output)\n",
    "window_memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "# 3. Define Prompt for ConversationChain (similar to above)\n",
    "prompt_window = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Bạn là một trợ lý hữu ích. Hãy trả lời các câu hỏi của người dùng. Duy trì ngữ cảnh cuộc trò chuyện.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# 4. Initialize ConversationChain with Window Memory\n",
    "conversation_window = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=window_memory,\n",
    "    prompt=prompt_window,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- Practical Example with ConversationBufferWindowMemory (k=2) ---\")\n",
    "\n",
    "# Conversation turn 1\n",
    "user_input_w1 = \"Tôi tên là An.\"\n",
    "print(f\"\\nBạn: {user_input_w1}\")\n",
    "response_w1 = conversation_window.invoke({\"input\": user_input_w1})\n",
    "print(f\"AI: {response_w1['response']}\")\n",
    "\n",
    "# Conversation turn 2\n",
    "user_input_w2 = \"Bạn có thể giúp tôi làm gì?\"\n",
    "print(f\"\\nBạn: {user_input_w2}\")\n",
    "response_w2 = conversation_window.invoke({\"input\": user_input_w2})\n",
    "print(f\"AI: {response_w2['response']}\")\n",
    "\n",
    "# Conversation turn 3 (An's first message will be discarded from the window)\n",
    "user_input_w3 = \"Tôi muốn hỏi về trí tuệ nhân tạo.\"\n",
    "print(f\"\\nBạn: {user_input_w3}\")\n",
    "response_w3 = conversation_window.invoke({\"input\": user_input_w3})\n",
    "print(f\"AI: {response_w3['response']}\")\n",
    "\n",
    "# Conversation turn 4 (An's second message will be discarded from the window)\n",
    "user_input_w4 = \"Trí tuệ nhân tạo là gì?\"\n",
    "print(f\"\\nBạn: {user_input_w4}\")\n",
    "response_w4 = conversation_window.invoke({\"input\": user_input_w4})\n",
    "print(f\"AI: {response_w4['response']}\")\n",
    "\n",
    "print(\"\\n--- History in Window Memory (k=2) ---\")\n",
    "print(window_memory.buffer) # View the stored history (only the last 2 message pairs)\n",
    "\n",
    "# Try asking for the name again (LLM might not remember if it was pushed out of the window)\n",
    "user_input_w5 = \"Tên tôi là gì?\"\n",
    "print(f\"\\nBạn: {user_input_w5}\")\n",
    "response_w5 = conversation_window.invoke({\"input\": user_input_w5})\n",
    "print(f\"AI: {response_w5['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "* With `k=2`, `ConversationBufferWindowMemory` only keeps the last 2 message pairs.\n",
    "* You will see in the `verbose=True` output that when `user_input_w3` is added, the first message pair (\"Tôi tên là An.\" and AI's response) has been removed from the history sent to the LLM.\n",
    "* Therefore, when you ask \"Tên tôi là gì?\" in turn `user_input_w5`, the LLM might not remember your name because that information has been pushed out of the memory window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tóm tắt Bài học\n",
    "\n",
    "Bài học này đã đi sâu vào hai loại **Memory** cơ bản trong LangChain: **ConversationBufferMemory** và **ConversationBufferWindowMemory**. Bạn đã hiểu rằng **ConversationBufferMemory** lưu trữ toàn bộ lịch sử cuộc trò chuyện, đơn giản nhưng có thể gặp vấn đề về giới hạn token cho các cuộc hội thoại dài. Ngược lại, **ConversationBufferWindowMemory** chỉ lưu trữ một cửa sổ gồm `k` tin nhắn gần nhất, giúp kiểm soát kích thước bộ nhớ và chi phí token, nhưng có thể làm mất ngữ cảnh dài hạn. Thông qua các ví dụ thực hành với `ConversationChain`, bạn đã thấy cách mỗi loại Memory hoạt động và cách chúng ảnh hưởng đến khả năng duy trì ngữ cảnh của LLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
