{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5.1: Introduction to Memory\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we built Large Language Model (LLM) applications that could answer questions or perform tasks. However, if you tried to have a multi-turn conversation with them, you would notice a major limitation: they don't \"remember\" previous interactions. Each query is processed as a completely new conversation. This is the **statelessness problem** of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Statelessness Problem in LLM Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are inherently **stateless**. This means that every time you send a prompt to an LLM, it processes that prompt independently, without any memory of previous prompts or responses within the same \"conversation.\"\n",
    "\n",
    "* **Example:**\n",
    "    * **You:** \"What is the capital of France?\"\n",
    "    * **LLM:** \"The capital of France is Paris.\"\n",
    "    * **You:** \"What is its population?\"\n",
    "    * **LLM (without Memory):** \"I don't know what 'it' refers to.\" (Because it has forgotten Paris).\n",
    "\n",
    "For an LLM to maintain a fluid and contextual conversation, we need a mechanism to provide the conversation history back in each LLM call. This mechanism is called **Memory**.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Memory is Needed in Chatbot and Conversational Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory** is an essential component for any LLM application that requires maintaining context across multiple turns, such as chatbots, virtual assistants, or multi-step interactive applications.\n",
    "\n",
    "* **Context Preservation:** Allows the LLM to understand follow-up questions that depend on previously discussed information.\n",
    "* **Better User Experience:** When the LLM \"remembers,\" the conversation becomes more natural, fluid, and effective, similar to chatting with a human.\n",
    "* **Complex Problem Solving:** For multi-step tasks or those requiring state tracking (e.g., placing an order, filling out a form), Memory is indispensable.\n",
    "* **Personalization:** Enables the LLM to provide more tailored responses based on the history of interactions with a specific user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Types of Memory in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain offers various types of Memory, each with its own way of storing and retrieving conversation history, suitable for different needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. `ChatMessageHistory` (Most Basic)\n",
    "\n",
    "* **Concept:** A foundational class for storing a list of `Message` objects (e.g., `HumanMessage`, `AIMessage`). It's simply a buffer, with no complex logic.\n",
    "* **Usage:** You add messages to it and retrieve the entire list when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import ChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"Hello, how are you?\")\n",
    "history.add_ai_message(\"I'm fine, thank you! Do you have any questions?\")\n",
    "history.add_user_message(\"I want to ask about LangChain.\")\n",
    "\n",
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. `ConversationBufferMemory` (Stores Full History)\n",
    "\n",
    "* **Concept:** Stores the entire conversation history (all messages) in a buffer.\n",
    "* **Pros:** Simple, easy to use, retains full context.\n",
    "* **Cons:** Can become very large and exceed LLM token limits if the conversation is too long.\n",
    "* **Usage:** Integrates directly into Chains or Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Initialize ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Initialize ConversationChain with Memory\n",
    "# verbose=True to see the chain's operation\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"--- ConversationBufferMemory Example ---\")\n",
    "\n",
    "# Conversation turn 1\n",
    "response_1 = conversation.invoke({\"input\": \"What is the capital of France?\"})\n",
    "print(f\"You: What is the capital of France?\")\n",
    "print(f\"AI: {response_1['response']}\")\n",
    "\n",
    "# Conversation turn 2 (LLM remembers context)\n",
    "response_2 = conversation.invoke({\"input\": \"What is its population?\"})\n",
    "print(f\"You: What is its population?\")\n",
    "print(f\"AI: {response_2['response']}\")\n",
    "\n",
    "print(\"\\n--- History in Memory ---\")\n",
    "print(memory.buffer) # View the entire stored history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. `ConversationBufferWindowMemory` (Stores Last N Messages)\n",
    "\n",
    "* **Concept:** Stores only a window of the last `k` messages. When a new message arrives, the oldest message is discarded.\n",
    "* **Pros:** Prevents conversation history from becoming too long, helping control token limits.\n",
    "* **Cons:** Can lose important context if that information falls outside the `k`-message window.\n",
    "* **When to Use:** When you need to maintain short-term context and want to avoid exceeding token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Initialize ConversationBufferWindowMemory with k=1 (remembers only the last 1 message pair)\n",
    "window_memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "window_memory.save_context({\"input\": \"Hello\"}, {\"output\": \"I'm fine, how are you?\"})\n",
    "window_memory.save_context({\"input\": \"I want to ask about AI\"}, {\"output\": \"What do you want to know about AI?\"})\n",
    "window_memory.save_context({\"input\": \"What is AI?\"}, {\"output\": \"AI is artificial intelligence.\"})\n",
    "\n",
    "print(\"--- ConversationBufferWindowMemory Example (k=1) ---\")\n",
    "print(window_memory.buffer) # Only the last message remains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. `ConversationSummaryMemory` (Summarizes History)\n",
    "\n",
    "* **Concept:** Uses an LLM to periodically summarize the conversation history. Only the summary is stored instead of the raw messages.\n",
    "* **Pros:** Retains long-term context without exceeding token limits, as only the summary is stored.\n",
    "* **Cons:** Incurs additional cost and latency due to LLM calls for summarization. Summary quality depends on the LLM.\n",
    "* **When to Use:** When you need to maintain long-term context and the conversation can be very long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "llm_for_summary = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) # LLM for summarization\n",
    "\n",
    "# Initialize ConversationSummaryMemory\n",
    "summary_memory = ConversationSummaryMemory(llm=llm_for_summary)\n",
    "\n",
    "summary_memory.save_context({\"input\": \"Hello\"}, {\"output\": \"I'm fine, how are you?\"})\n",
    "summary_memory.save_context({\"input\": \"I want to ask about LangChain\"}, {\"output\": \"LangChain is a framework for building LLM applications.\"})\n",
    "summary_memory.save_context({\"input\": \"What are its components?\"}, {\"output\": \"Key components include Models, Prompts, Chains, Agents, Retrieval, Memory.\"})\n",
    "\n",
    "print(\"--- ConversationSummaryMemory Example ---\")\n",
    "print(summary_memory.buffer) # View the summary stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. `ConversationSummaryBufferMemory` (Combines Window and Summary)\n",
    "\n",
    "* **Concept:** Combines `ConversationBufferWindowMemory` and `ConversationSummaryMemory`. It stores the most recent messages in a window and summarizes older messages outside that window.\n",
    "* **Pros:** Provides detailed context for recent messages and general context for the rest of the conversation, optimizing token usage.\n",
    "* **When to Use:** When you want a balance between short-term detailed context and long-term general context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "llm_for_summary_buffer = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Initialize ConversationSummaryBufferMemory with max_token_limit\n",
    "# It will summarize when the total token count exceeds the limit\n",
    "summary_buffer_memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm_for_summary_buffer,\n",
    "    max_token_limit=100 # Token limit for the buffer\n",
    ")\n",
    "\n",
    "summary_buffer_memory.save_context({\"input\": \"Hello\"}, {\"output\": \"I'm fine, how are you?\"})\n",
    "summary_buffer_memory.save_context({\"input\": \"I want to ask about LangChain\"}, {\"output\": \"LangChain is a framework for building LLM applications.\"})\n",
    "summary_buffer_memory.save_context({\"input\": \"What are its components?\"}, {\"output\": \"Key components include Models, Prompts, Chains, Agents, Retrieval, Memory.\"})\n",
    "summary_buffer_memory.save_context({\"input\": \"What is RAG?\"}, {\"output\": \"RAG is Retrieval-Augmented Generation.\"})\n",
    "summary_buffer_memory.save_context({\"input\": \"Why is RAG important?\"}, {\"output\": \"RAG helps reduce hallucinations and increase accuracy.\"})\n",
    "\n",
    "print(\"--- ConversationSummaryBufferMemory Example ---\")\n",
    "print(summary_buffer_memory.buffer) # View the buffer (might have been summarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How to Integrate Memory into Chains and Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Integrating Memory into Chains\n",
    "\n",
    "For Chains like `ConversationChain`, you simply pass the Memory object to the `memory` parameter when initializing the Chain. The Chain will automatically manage injecting the history into the prompt and updating the memory after each turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example already illustrated in the ConversationBufferMemory section\n",
    "# conversation = ConversationChain(llm=llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Integrating Memory into Agents\n",
    "\n",
    "For Agents, integrating Memory is slightly more complex because Agents need flexibility in using Tools. You need to add a `MessagesPlaceholder` to the Agent's `PromptTemplate` so the LLM can see the conversation history. Then, you pass the conversation history to the `AgentExecutor` via the `chat_history` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai google-search-results numexpr\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_community.tools import Tool\n",
    "from langchain_community.tools.calculator.tool import Calculator\n",
    "from langchain.memory import ConversationBufferMemory # Use Memory for Agent\n",
    "\n",
    "# Set environment variables for OpenAI and SerpAPI keys\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"YOUR_SERPAPI_API_KEY\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Initialize Tools\n",
    "search_tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    func=SerpAPIWrapper().run,\n",
    "    description=\"Useful when you need to search for information on Google about current events or factual data.\"\n",
    ")\n",
    "calculator_tool = Calculator()\n",
    "tools = [search_tool, calculator_tool]\n",
    "\n",
    "# Initialize Memory for Agent\n",
    "agent_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Define Prompt for Agent with MessagesPlaceholder for chat_history\n",
    "prompt_with_memory = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. You have access to the following tools: {tools}. Use them to answer questions. Maintain the conversation context.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # This is where the chat history will be injected\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# Create Agent\n",
    "agent_with_memory = create_react_agent(llm, tools, prompt_with_memory)\n",
    "\n",
    "# Create Agent Executor\n",
    "agent_executor_with_memory = AgentExecutor(\n",
    "    agent=agent_with_memory,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    memory=agent_memory # Pass the memory object to the Agent Executor\n",
    ")\n",
    "\n",
    "print(\"\\n--- Agent with Memory Example ---\")\n",
    "\n",
    "# Conversation turn 1\n",
    "query_1 = \"What's the weather like today in Da Nang?\"\n",
    "print(f\"You: {query_1}\")\n",
    "response_1 = agent_executor_with_memory.invoke({\"input\": query_1})\n",
    "print(f\"AI: {response_1['output']}\")\n",
    "\n",
    "# Conversation turn 2 (LLM should remember \"Da Nang\")\n",
    "query_2 = \"What's the average temperature there in July?\"\n",
    "print(f\"You: {query_2}\")\n",
    "# Note: Here, the LLM might need to use the search tool again or infer from context.\n",
    "# If the mock weather tool doesn't have enough info, it will say it doesn't know.\n",
    "# For this example to work well, the web search tool (SerpAPI) would need to be used.\n",
    "response_2 = agent_executor_with_memory.invoke({\"input\": query_2})\n",
    "print(f\"AI: {response_2['output']}\")\n",
    "\n",
    "print(\"\\n--- History in Agent Memory ---\")\n",
    "print(agent_memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson addressed the inherent **statelessness problem** of LLMs and introduced the crucial concept of **Memory** in LangChain. You understood why Memory is essential for conversational applications and explored common Memory types such as `ChatMessageHistory`, `ConversationBufferMemory`, `ConversationBufferWindowMemory`, `ConversationSummaryMemory`, and `ConversationSummaryBufferMemory`, each with its own pros and cons for managing conversation history. Finally, you learned how to **integrate Memory into both Chains and Agents**, enabling your LLM applications to maintain context, resulting in a more natural and effective user experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
  "nbformat": 4,
  "nbformat_minor": 5
}
