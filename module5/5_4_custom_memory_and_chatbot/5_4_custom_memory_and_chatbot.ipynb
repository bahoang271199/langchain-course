{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5.4: Custom Memory and Chatbot Practice\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we explored LangChain's built-in **Memory** types, from basic ones like `ConversationBufferMemory` to more advanced ones like `ConversationSummaryMemory`. However, in some cases, you might need a more specialized way to store or process conversation history. This lesson will guide you on how to create **Custom Memory** and apply all your knowledge about Memory to build a practical chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Custom Memory Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. When is Custom Memory Needed?\n",
    "\n",
    "You should consider creating Custom Memory when:\n",
    "\n",
    "* **Integration with External Databases:** You want to store conversation history in a specific database (SQL, NoSQL, Redis, etc.) instead of just in application memory.\n",
    "* **Special Storage/Retrieval Logic:** You need more complex logic to decide which messages should be stored, when they should be deleted, or how to retrieve them.\n",
    "* **Data Pre/Post-processing:** You want to perform data processing steps before saving to or after retrieving from memory (e.g., encryption, compression, filtering).\n",
    "* **Integration with Other State Management Systems:** You already have a user state management system and want LangChain Memory to utilize it.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. How to Create Custom Memory\n",
    "\n",
    "To create Custom Memory, you will typically inherit from LangChain's `BaseMemory` class and implement the necessary methods. The most important methods include:\n",
    "\n",
    "* `load_memory_variables(inputs: Dict[str, Any]) -> Dict[str, Any]`: This method is called to load conversation history from memory and return it as a dictionary.\n",
    "* `save_context(inputs: Dict[str, Any], outputs: Dict[str, Any]) -> None`: This method is called to store user input and AI output into memory after each conversation turn.\n",
    "* `clear() -> None`: This method is called to clear the entire history in memory.\n",
    "\n",
    "You also need to define the input (`input_keys`) and output (`output_keys`) variables that Memory will manage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Simple Custom Memory (storing in a Python list)**\n",
    "\n",
    "Here's an example of Custom Memory that stores conversation history in a simple Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "from langchain_core.memory import BaseMemory\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "class SimpleListMemory(BaseMemory):\n",
    "    \"\"\"\n",
    "    Custom memory that stores conversation history as a list of messages.\n",
    "    \"\"\"\n",
    "    conversation_history: List[BaseMessage] = []\n",
    "    input_key: str = \"input\"\n",
    "    output_key: str = \"output\"\n",
    "\n",
    "    @property\n",
    "    def memory_variables(self) -> List[str]:\n",
    "        \"\"\"Variables that the memory will return.\"\"\"\n",
    "        return [\"history\"] # Name of the variable the prompt will use to get history\n",
    "\n",
    "    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Load conversation history from memory.\"\"\"\n",
    "        # Convert the list of messages into a formatted string for the prompt\n",
    "        formatted_history = \"\"\n",
    "        for msg in self.conversation_history:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                formatted_history += f\"Human: {msg.content}\\n\"\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                formatted_history += f\"AI: {msg.content}\\n\"\n",
    "        return {\"history\": formatted_history.strip()}\n",
    "\n",
    "    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -> None:\n",
    "        \"\"\"Store input and output into memory.\"\"\"\n",
    "        user_message = inputs.get(self.input_key)\n",
    "        ai_message = outputs.get(self.output_key)\n",
    "\n",
    "        if user_message:\n",
    "            self.conversation_history.append(HumanMessage(content=user_message))\n",
    "        if ai_message:\n",
    "            self.conversation_history.append(AIMessage(content=ai_message))\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the entire conversation history.\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "# Test Custom Memory\n",
    "print(\"--- Kiểm tra Custom Memory ---\") # Test Custom Memory\n",
    "custom_memory = SimpleListMemory()\n",
    "custom_memory.save_context({\"input\": \"Chào bạn\"}, {\"output\": \"Tôi khỏe, bạn thế nào?\"}) # Hello, how are you? / I'm fine, how about you?\n",
    "custom_memory.save_context({\"input\": \"Bạn tên gì?\"}, {\"output\": \"Tôi là một mô hình ngôn ngữ.\"}) # What's your name? / I am a language model.\n",
    "print(custom_memory.load_memory_variables({}))\n",
    "custom_memory.clear()\n",
    "print(custom_memory.load_memory_variables({}))\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. How to Access and Manipulate Data in Memory\n",
    "\n",
    "Once you've integrated Memory into a Chain or Agent, accessing and manipulating data in Memory is done through the Memory object's methods:\n",
    "\n",
    "* `memory.load_memory_variables({})`: To get the current content of the memory.\n",
    "* `memory.save_context(inputs, outputs)`: To add a new conversation turn to memory.\n",
    "* `memory.clear()`: To clear the memory.\n",
    "\n",
    "In the following practical examples, you'll see how Chains and Agents automatically call these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Practical Example: Building a Chatbot Capable of Maintaining Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a simple chatbot and test it with different Memory types to see the differences in how they maintain context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have `langchain-openai` installed.\n",
    "* Set the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    ConversationSummaryBufferMemory,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Define a function to run the chatbot with a specific Memory type\n",
    "def run_chatbot_with_memory(memory_instance, memory_name: str, num_turns: int = 5):\n",
    "    \"\"\"\n",
    "    Runs a chatbot with the specified Memory type.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Bắt đầu Chatbot với {memory_name} ---\") # Starting Chatbot with {memory_name}\n",
    "\n",
    "    # Basic prompt for the chatbot\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Bạn là một trợ lý hữu ích. Hãy trả lời các câu hỏi của người dùng. Duy trì ngữ cảnh cuộc trò chuyện.\"), # You are a helpful assistant. Answer user questions. Maintain conversation context.\n",
    "        MessagesPlaceholder(variable_name=\"history\"), # Placeholder for conversation history\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "    conversation_chain = ConversationChain(\n",
    "        llm=llm,\n",
    "        memory=memory_instance,\n",
    "        prompt=prompt_template,\n",
    "        verbose=False # Set False to avoid cluttered output, but you can set True for debugging\n",
    "    )\n",
    "\n",
    "    # Sample questions to test context\n",
    "    questions = [\n",
    "        \"Chào bạn, tôi là An. Tôi đang tìm hiểu về LangChain.\", # Hello, I'm An. I'm learning about LangChain.\n",
    "        \"Bạn có thể giải thích về RAG không?\", # Can you explain RAG?\n",
    "        \"Vậy RAG giúp ích gì?\", # So what does RAG help with?\n",
    "        \"Tên tôi là gì?\", # What's my name?\n",
    "        \"Bạn có thể nhắc lại các thành phần chính của LangChain không?\", # Can you reiterate the main components of LangChain?\n",
    "        \"Tôi có thể sử dụng RAG với loại cơ sở dữ liệu nào?\", # What kind of databases can I use RAG with?\n",
    "    ]\n",
    "\n",
    "    for i in range(min(num_turns, len(questions))):\n",
    "        user_input = questions[i]\n",
    "        print(f\"Người dùng ({i+1}): {user_input}\") # User ({i+1}):\n",
    "        response = conversation_chain.invoke({\"input\": user_input})\n",
    "        print(f\"AI ({i+1}): {response['response']}\") # AI ({i+1}):\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    print(f\"--- Kết thúc Chatbot với {memory_name} ---\") # Ending Chatbot with {memory_name}\n",
    "    print(f\"Nội dung cuối cùng của {memory_name}:\") # Final content of {memory_name}:\n",
    "    print(memory_instance.buffer if hasattr(memory_instance, 'buffer') else memory_instance.load_memory_variables({}))\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# 2.1. Use ConversationBufferMemory\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "run_chatbot_with_memory(buffer_memory, \"ConversationBufferMemory\")\n",
    "\n",
    "# 2.2. Use ConversationBufferWindowMemory (k=2)\n",
    "window_memory = ConversationBufferWindowMemory(k=2)\n",
    "run_chatbot_with_memory(window_memory, \"ConversationBufferWindowMemory (k=2)\")\n",
    "\n",
    "# 2.3. Use ConversationSummaryMemory\n",
    "# Requires an LLM for summarization\n",
    "summary_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "summary_memory = ConversationSummaryMemory(llm=summary_llm)\n",
    "run_chatbot_with_memory(summary_memory, \"ConversationSummaryMemory\")\n",
    "\n",
    "# 2.4. Use ConversationSummaryBufferMemory (max_token_limit=100)\n",
    "# Requires an LLM for summarization\n",
    "summary_buffer_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "summary_buffer_memory = ConversationSummaryBufferMemory(\n",
    "    llm=summary_buffer_llm,\n",
    "    max_token_limit=100\n",
    ")\n",
    "run_chatbot_with_memory(summary_buffer_memory, \"ConversationSummaryBufferMemory (max_token_limit=100)\")\n",
    "\n",
    "# 2.5. Use Custom Memory (SimpleListMemory)\n",
    "custom_memory_instance = SimpleListMemory()\n",
    "run_chatbot_with_memory(custom_memory_instance, \"SimpleListMemory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing the Effectiveness of Different Memory Types in Various Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code above, you will observe clear differences in how each Memory type handles context:\n",
    "\n",
    "* **`ConversationBufferMemory`**:\n",
    "    * **Effectiveness:** Maintains perfect context for short conversations.\n",
    "    * **Limitations:** As the conversation lengthens, the `history` in the prompt will grow, leading to increased token costs and a risk of exceeding the LLM's token limits. The LLM can also become \"overwhelmed\" with information.\n",
    "    * **When to Use:** Short interactive applications where you need the full history and are not concerned about token limits/costs.\n",
    "\n",
    "* **`ConversationBufferWindowMemory` (k=2)**:\n",
    "    * **Effectiveness:** Good control over prompt size and token costs by only retaining the last `k` turns.\n",
    "    * **Limitations:** Loses long-term context. You will notice the LLM forgets the name \"An\" in turn 4, as that information has been pushed out of the window.\n",
    "    * **When to Use:** Applications requiring short-term context, or when you know that information older than a certain threshold is no longer relevant.\n",
    "\n",
    "* **`ConversationSummaryMemory`**:\n",
    "    * **Effectiveness:** Retains long-term context as a summary, significantly reducing the number of tokens passed to the LLM in subsequent turns.\n",
    "    * **Limitations:** Incurs additional cost and latency due to LLM calls for summarization. The quality of the summary depends on the LLM. Some specific details not included in the summary might be lost.\n",
    "    * **When to Use:** Very long conversations where you need to maintain general context without overflowing the token buffer.\n",
    "\n",
    "* **`ConversationSummaryBufferMemory` (max_token_limit=100)**:\n",
    "    * **Effectiveness:** Provides a good balance between retaining recent details and summarizing older context. This is often the optimal choice for many practical chatbots.\n",
    "    * **Limitations:** Still has cost and latency associated with summarization, and quality depends on the LLM.\n",
    "    * **When to Use:** Most chatbots that need to maintain long-term context but still want to control costs and performance.\n",
    "\n",
    "* **`SimpleListMemory` (Custom Memory)**:\n",
    "    * **Effectiveness:** Illustrates how you can fully customize the storage logic. In this example, it behaves similarly to `ConversationBufferMemory`, but you could modify it to store in a database or apply more complex logic.\n",
    "    * **Limitations:** Depends on the logic you define. If not careful, you might reintroduce token/context issues similar to basic memory types.\n",
    "    * **When to Use:** When no built-in Memory type meets your specific data storage/processing requirements.\n",
    "\n",
    "Choosing the appropriate Memory type is a crucial decision in your chatbot's design. It depends on the average conversation length, context accuracy requirements, LLM token limits, and your budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "In this lesson, you learned how to create **Custom Memory** in LangChain by inheriting from `BaseMemory` and implementing the necessary methods, opening up possibilities for integration with custom storage systems and logic. You also practiced **building a chatbot** and testing it with different Memory types: `ConversationBufferMemory`, `ConversationBufferWindowMemory`, `ConversationSummaryMemory`, and `ConversationSummaryBufferMemory`. Finally, we **compared the effectiveness** of each Memory type in various scenarios, helping you understand their pros and cons and make an informed choice for your application. Mastering Memory types and the ability to customize them is key to building intelligent and effective conversational applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
