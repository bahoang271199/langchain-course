{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5.3: Advanced Memory Types\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lesson 5.2, we learned about **ConversationBufferMemory** and **ConversationBufferWindowMemory**, two basic Memory types for maintaining context. However, for very long conversations, storing the entire history or a fixed window of messages might be inefficient in terms of cost or exceed token limits. This lesson will introduce more advanced Memory types designed to address these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Concept and How it Works\n",
    "\n",
    "**ConversationSummaryMemory** addresses the problem of long conversations by not storing the entire raw messages. Instead, it uses a **Large Language Model (LLM)** to **periodically summarize the conversation**. Only this summary is stored and passed into the LLM's prompt in subsequent turns.\n",
    "\n",
    "* **Mechanism:**\n",
    "    1.  After each or a few conversation turns, an LLM is used to generate a concise summary of the conversation up to that point.\n",
    "    2.  This summary (instead of the raw messages) is stored in memory.\n",
    "    3.  In subsequent turns, the summary is passed into the prompt along with the most recent messages (if any).\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Pros and Cons\n",
    "\n",
    "* **Pros:**\n",
    "    * **Long-term Context Retention:** Capable of maintaining context for very long conversations without exceeding token limits, as only the summary is stored.\n",
    "    * **Reduced Token Cost:** The number of tokens passed to the LLM in each turn is often significantly less than passing the entire history.\n",
    "    * **Effective for Complex Conversations:** The LLM can focus on the main points of the conversation without being distracted by minor details.\n",
    "\n",
    "* **Cons:**\n",
    "    * **Additional Cost and Latency:** Calling an LLM for summarization incurs extra API costs and latency.\n",
    "    * **Summary Quality Depends on LLM:** If the LLM's summarization is poor, important context might be lost or misinterpreted.\n",
    "    * **Can Lose Specific Details:** The nature of summarization is to abstract away details, so if you need to query very specific details from older conversation turns, `ConversationSummaryMemory` might not be the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Practical Example Using ConversationSummaryMemory\n",
    "\n",
    "We will practice with `ConversationSummaryMemory` to see how it maintains context through summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 1. Initialize LLM (used for both ConversationChain and summarization)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# 2. Initialize ConversationSummaryMemory\n",
    "# Pass the LLM so it can perform summarization\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# 3. Define Prompt for ConversationChain\n",
    "prompt_summary = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer user questions. Maintain conversation context.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"), # Placeholder for conversation history (will contain summary)\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# 4. Initialize ConversationChain with Summary Memory\n",
    "conversation_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=summary_memory,\n",
    "    prompt=prompt_summary,\n",
    "    verbose=True # To see the summarization process\n",
    ")\n",
    "\n",
    "print(\"--- Practical Example with ConversationSummaryMemory ---\")\n",
    "\n",
    "# Conversation turn 1\n",
    "user_input_s1 = \"Hello, I'm An. I'm learning about artificial intelligence.\"\n",
    "print(f\"\\nYou: {user_input_s1}\")\n",
    "response_s1 = conversation_summary.invoke({\"input\": user_input_s1})\n",
    "print(f\"AI: {response_s1['response']}\")\n",
    "\n",
    "# Conversation turn 2\n",
    "user_input_s2 = \"Can you explain machine learning?\"\n",
    "print(f\"\\nYou: {user_input_s2}\")\n",
    "response_s2 = conversation_summary.invoke({\"input\": user_input_s2})\n",
    "print(f\"AI: {response_s2['response']}\")\n",
    "\n",
    "# Conversation turn 3\n",
    "user_input_s3 = \"So, how is deep learning different from machine learning?\"\n",
    "print(f\"\\nYou: {user_input_s3}\")\n",
    "response_s3 = conversation_summary.invoke({\"input\": user_input_s3})\n",
    "print(f\"AI: {response_s3['response']}\")\n",
    "\n",
    "# Conversation turn 4 (After a few turns, memory will automatically summarize)\n",
    "user_input_s4 = \"Summarize our conversation so far.\"\n",
    "print(f\"\\nYou: {user_input_s4}\")\n",
    "response_s4 = conversation_summary.invoke({\"input\": user_input_s4})\n",
    "print(f\"AI: {response_s4['response']}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Summary Content in Memory ---\")\n",
    "# You can check the stored summary content\n",
    "print(summary_memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "* You will see in the `verbose=True` output that after a certain number of conversation turns, the LLM will be invoked to generate a summary. This summary is then used in place of older raw messages.\n",
    "* When you ask to \"Summarize our conversation so far,\" the LLM will use the summary in memory (along with the most recent messages if any) to provide the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ConversationSummaryBufferMemory (Revisiting and Clarifying)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although briefly introduced in Lesson 5.1, **ConversationSummaryBufferMemory** is a very important advanced Memory type that combines the advantages of both **ConversationBufferWindowMemory** and **ConversationSummaryMemory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Concept and How it Works\n",
    "\n",
    "**ConversationSummaryBufferMemory** retains a certain number of the most recent messages in a buffer (like `ConversationBufferWindowMemory`) and summarizes older messages when the total token count exceeds a certain threshold (like `ConversationSummaryMemory`).\n",
    "\n",
    "* **`max_token_limit`**: The primary parameter, defining the maximum total tokens the memory will attempt to maintain. When this threshold is exceeded, the oldest messages will be summarized.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Pros and Cons\n",
    "\n",
    "* **Pros:**\n",
    "    * **Optimal Balance:** Provides detailed context for recent conversation turns and general context for the rest of the conversation.\n",
    "    * **Efficient Token Control:** Very good for managing LLM token limits while still retaining long-term context.\n",
    "    * **Good User Experience:** Maintains conversation fluidity without losing too much information.\n",
    "\n",
    "* **Cons:**\n",
    "    * **More Complex:** Requires an LLM to perform summarization, which can be more costly and slightly slower than simple buffer memory types.\n",
    "    * **Summary Quality:** Depends on the quality of the LLM used for summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Practical Example Using ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "llm_buffer_summary = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Initialize ConversationSummaryBufferMemory with max_token_limit\n",
    "# It will summarize when the total token count exceeds the limit\n",
    "summary_buffer_memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm_buffer_summary,\n",
    "    max_token_limit=150 # Token limit for the buffer\n",
    ")\n",
    "\n",
    "prompt_buffer_summary = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer user questions. Maintain conversation context.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "conversation_buffer_summary = ConversationChain(\n",
    "    llm=llm_buffer_summary,\n",
    "    memory=summary_buffer_memory,\n",
    "    prompt=prompt_buffer_summary,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- Practical Example with ConversationSummaryBufferMemory ---\")\n",
    "\n",
    "# Conversation turn 1\n",
    "user_input_bs1 = \"Hello, I'm Minh. I'm learning about wild animals.\"\n",
    "print(f\"\\nYou: {user_input_bs1}\")\n",
    "response_bs1 = conversation_buffer_summary.invoke({\"input\": user_input_bs1})\n",
    "print(f\"AI: {response_bs1['response']}\")\n",
    "\n",
    "# Conversation turn 2\n",
    "user_input_bs2 = \"Can you name some wild animals in Vietnam?\"\n",
    "print(f\"\\nYou: {user_input_bs2}\")\n",
    "response_bs2 = conversation_buffer_summary.invoke({\"input\": user_input_bs2})\n",
    "print(f\"AI: {response_bs2['response']}\")\n",
    "\n",
    "# Conversation turn 3 (might trigger summarization if max_token_limit is exceeded)\n",
    "user_input_bs3 = \"What are the prominent characteristics of the Indochinese tiger?\"\n",
    "print(f\"\\nYou: {user_input_bs3}\")\n",
    "response_bs3 = conversation_buffer_summary.invoke({\"input\": user_input_bs3})\n",
    "print(f\"AI: {response_bs3['response']}\")\n",
    "\n",
    "# Conversation turn 4 (check if memory remembers the name)\n",
    "user_input_bs4 = \"What's my name?\"\n",
    "print(f\"\\nYou: {user_input_bs4}\")\n",
    "response_bs4 = conversation_buffer_summary.invoke({\"input\": user_input_bs4})\n",
    "print(f\"AI: {response_bs4['response']}\")\n",
    "\n",
    "print(\"\\n--- Content in ConversationSummaryBufferMemory ---\")\n",
    "print(summary_buffer_memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "* You will see that `ConversationSummaryBufferMemory` will try to keep recent messages in raw form and summarize older messages when the total token count exceeds `max_token_limit`.\n",
    "* This helps maintain both short-term details and long-term context effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ConversationKGMemory (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Concept and How it Works\n",
    "\n",
    "**ConversationKGMemory** (Knowledge Graph Memory) is a more advanced Memory type that not only stores messages but also builds a **Knowledge Graph (KG)** from the conversation. This KG contains entities and relationships extracted from conversation turns.\n",
    "\n",
    "* **Mechanism:**\n",
    "    1.  An LLM is used to extract entities and relationships from each new conversation turn.\n",
    "    2.  These entities and relationships are added to an internal knowledge graph.\n",
    "    3.  When the LLM needs context, it can query the knowledge graph to retrieve relevant information, or the entire graph can be serialized and passed into the prompt.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Pros and Cons\n",
    "\n",
    "* **Pros:**\n",
    "    * **Specific Information Query:** Very effective when you need to query specific information or relationships from the conversation history (e.g., \"Who is An's friend?\", \"When was Company X founded?\").\n",
    "    * **Effective for Complex Context:** Can capture complex relationships between concepts in the conversation.\n",
    "    * **Reduces Hallucinations:** By relying on extracted facts, it can reduce the LLM's tendency to fabricate information.\n",
    "\n",
    "* **Cons:**\n",
    "    * **More Complex:** Requires an LLM for KG extraction, incurring additional cost and latency.\n",
    "    * **Requires Powerful LLM:** The quality of the KG depends on the LLM's extraction capabilities.\n",
    "    * **Not Always Necessary:** For simple conversations, the benefits might not outweigh the cost and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Practical Example Using ConversationKGMemory (Conceptual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up `ConversationKGMemory` is more complex and might require additional dependencies (like `networkx` for graph representation). Here's a conceptual example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed (might need networkx)\n",
    "# pip install langchain-openai openai networkx\n",
    "\n",
    "# import os\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.chains import ConversationChain\n",
    "# from langchain.memory import ConversationKGMemory\n",
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# # Set environment variable for OpenAI API key\n",
    "# # os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# llm_kg = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) # Often use low temperature for extraction\n",
    "\n",
    "# # Initialize ConversationKGMemory\n",
    "# # It will use the LLM to extract entities and relationships\n",
    "# kg_memory = ConversationKGMemory(llm=llm_kg, verbose=True)\n",
    "\n",
    "# # Define Prompt for ConversationChain\n",
    "# prompt_kg = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful assistant. Answer user questions. Based on known facts: {kg_triplets}\"), # kg_triplets is placeholder for knowledge graph\n",
    "#     MessagesPlaceholder(variable_name=\"history\"), # Still can have raw message history\n",
    "#     (\"human\", \"{input}\"),\n",
    "# ])\n",
    "\n",
    "# conversation_kg = ConversationChain(\n",
    "#     llm=llm_kg,\n",
    "#     memory=kg_memory,\n",
    "#     prompt=prompt_kg,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# print(\"\\n--- Practical Example with ConversationKGMemory (Conceptual) ---\")\n",
    "\n",
    "# # Conversation turn 1\n",
    "# # user_input_kg1 = \"My name is An. I live in Hanoi.\"\n",
    "# # print(f\"\\nUser: {user_input_kg1}\")\n",
    "# # response_kg1 = conversation_kg.invoke({\"input\": user_input_kg1})\n",
    "# # print(f\"AI: {response_kg1['response']}\")\n",
    "\n",
    "# # Conversation turn 2\n",
    "# # user_input_kg2 = \"My friend is Binh. He works at Google.\"\n",
    "# # print(f\"\\nUser: {user_input_kg2}\")\n",
    "# # response_kg2 = conversation_kg.invoke({\"input\": user_input_kg2})\n",
    "# # print(f\"AI: {response_kg2['response']}\")\n",
    "\n",
    "# # Conversation turn 3 (Ask about a relationship)\n",
    "# # user_input_kg3 = \"Where does Binh work?\"\n",
    "# # print(f\"\\nUser: {user_input_kg3}\")\n",
    "# # response_kg3 = conversation_kg.invoke({\"input\": user_input_kg3})\n",
    "# # print(f\"AI: {response_kg3['response']}\")\n",
    "\n",
    "# print(\"\\n--- Content in ConversationKGMemory (Triplets) ---\")\n",
    "# # print(kg_memory.get_graph_as_string()) # View the knowledge graph as a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VectorStoreRetrieverMemory (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Concept and How it Works\n",
    "\n",
    "**VectorStoreRetrieverMemory** is the most powerful Memory type for extremely long conversations. Instead of storing the entire history in memory or summarizing them, it stores **each conversation turn (or chunks of them)** in a **Vector Store**. When context is needed, it performs a similarity search within the Vector Store to retrieve the most relevant conversation turns to the current query.\n",
    "\n",
    "* **Mechanism:**\n",
    "    1.  Each (input, output) pair of the conversation is converted into an embedding and stored in a Vector Store.\n",
    "    2.  When a new query arrives, it is also converted into an embedding.\n",
    "    3.  The Vector Store is queried to find (input, output) pairs in the history that are semantically similar to the current query.\n",
    "    4.  These relevant conversation turns are then passed into the LLM's prompt.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Pros and Cons\n",
    "\n",
    "* **Pros:**\n",
    "    * **Handles Extremely Long Conversations:** Nearly unlimited scalability, as it leverages the capabilities of a Vector Store.\n",
    "    * **Precise Context Retrieval:** Can semantically retrieve the most relevant historical segments, even if they occurred a very long time ago.\n",
    "    * **Efficient Token Cost Reduction:** Only passes truly relevant historical segments to the LLM.\n",
    "\n",
    "* **Cons:**\n",
    "    * **More Complex to Set Up:** Requires a Vector Store (Chroma, FAISS, Pinecone, etc.) and an embedding model.\n",
    "    * **More Costly:** Incurs Vector Store storage costs and embedding model call costs for each conversation turn.\n",
    "    * **Latency:** Searching in the Vector Store can add latency to the response process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Practical Example Using VectorStoreRetrieverMemory (Conceptual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up `VectorStoreRetrieverMemory` requires a Vector Store and an embedding model. Here's a conceptual example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai chromadb\n",
    "\n",
    "# import os\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from langchain.chains import ConversationChain\n",
    "# from langchain.memory import VectorStoreRetrieverMemory\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# # Set environment variable for OpenAI API key\n",
    "# # os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# llm_vs = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "# embeddings_vs = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# # Initialize a Vector Store (e.g., Chroma)\n",
    "# # persist_directory_vs = \"./chroma_vs_memory_db\"\n",
    "# # if os.path.exists(persist_directory_vs):\n",
    "# #     shutil.rmtree(persist_directory_vs)\n",
    "# # vector_store_vs = Chroma(embedding_function=embeddings_vs, persist_directory=persist_directory_vs)\n",
    "\n",
    "# # # Create Retriever from Vector Store\n",
    "# # retriever_vs = vector_store_vs.as_retriever(search_kwargs={\"k\": 2}) # Retrieve 2 most relevant segments\n",
    "\n",
    "# # Initialize VectorStoreRetrieverMemory\n",
    "# # vs_memory = VectorStoreRetrieverMemory(retriever=retriever_vs)\n",
    "\n",
    "# # Define Prompt for ConversationChain\n",
    "# # prompt_vs = ChatPromptTemplate.from_messages([\n",
    "# #     (\"system\", \"You are a helpful assistant. Answer user questions. Maintain conversation context. Relevant historical segments: {history}\"),\n",
    "# #     (\"human\", \"{input}\"),\n",
    "# # ])\n",
    "\n",
    "# # conversation_vs = ConversationChain(\n",
    "# #     llm=llm_vs,\n",
    "# #     memory=vs_memory,\n",
    "# #     prompt=prompt_vs,\n",
    "# #     verbose=True\n",
    "# # )\n",
    "\n",
    "# print(\"\\n--- Practical Example with VectorStoreRetrieverMemory (Conceptual) ---\")\n",
    "\n",
    "# # Conversation turn 1\n",
    "# # user_input_vs1 = \"I like science fiction movies. Can you recommend any?\"\n",
    "# # print(f\"\\nUser: {user_input_vs1}\")\n",
    "# # response_vs1 = conversation_vs.invoke({\"input\": user_input_vs1})\n",
    "# # print(f\"AI: {response_vs1['response']}\")\n",
    "\n",
    "# # Conversation turn 2\n",
    "# # user_input_vs2 = \"I also like history books. Any good ones?\"\n",
    "# # print(f\"\\nUser: {user_input_vs2}\")\n",
    "# # response_vs2 = conversation_vs.invoke({\"input\": user_input_vs2})\n",
    "# # print(f\"AI: {response_vs2['response']}\")\n",
    "\n",
    "# # Conversation turn 3 (Ask about the first preference again)\n",
    "# # user_input_vs3 = \"Do you remember what movie genre I like?\"\n",
    "# # print(f\"\\nUser: {user_input_vs3}\")\n",
    "# # response_vs3 = conversation_vs.invoke({\"input\": user_vs3})\n",
    "# # print(f\"AI: {response_vs3['response']}\")\n",
    "\n",
    "# # if os.path.exists(persist_directory_vs):\n",
    "# #     shutil.rmtree(persist_directory_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson expanded your knowledge of **Memory** in LangChain by introducing advanced types. You learned about **ConversationSummaryMemory**, which helps maintain long-term context by summarizing the conversation, and **ConversationSummaryBufferMemory**, an effective combination of window and summarization. Additionally, we introduced the concepts of **ConversationKGMemory** (using a knowledge graph) and **VectorStoreRetrieverMemory** (storing history in a Vector Store), two powerful Memory types for complex and extremely long use cases. Choosing the right Memory type is crucial for balancing context retention, cost control, and performance in your LLM applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
