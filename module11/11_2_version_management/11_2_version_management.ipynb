{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 11.2: Version Management and Advanced Deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After optimizing the performance and cost of LLM applications, the next step is to ensure they can be effectively managed, updated, and deployed in a production environment. This lesson will focus on **versioning**, **Continuous Integration/Continuous Deployment (CI/CD)**, advanced deployment strategies like **Kubernetes** and **Serverless functions**, along with a practical exercise to prepare a LangChain application for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Versioning for Chains, Agents, and LLM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versioning is crucial for tracking changes, reverting to previous versions, and ensuring consistency in a production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Code:**\n",
    "    * Use a version control system (VCS) like **Git** to manage your LangChain/LangGraph application code.\n",
    "    * Apply clear branching and commit naming conventions (e.g., GitFlow, Semantic Versioning).\n",
    "* **Prompts:**\n",
    "    * Prompts are a critical part of LLM application logic. Any change to a prompt can significantly affect LLM behavior.\n",
    "    * Store prompts in separate files (e.g., `.txt`, `.yaml`, `.json`) and manage them with Git.\n",
    "    * Use tools like **LangSmith Prompt Hub** to version prompts, compare variations, and track the performance of each version.\n",
    "* **LLM Models:**\n",
    "    * Record the specific version of the LLM you are using (e.g., `gpt-3.5-turbo-0125`, `gpt-4o-2024-05-13`).\n",
    "    * If you are using fine-tuned models or local models, manage their versions (e.g., using **MLflow** or other model management platforms).\n",
    "* **Data (Vector Stores, Datasets):**\n",
    "    * If you are using RAG, the data in your Vector Store also needs to be versioned.\n",
    "    * Record the version of the source data and how it was preprocessed/embedded.\n",
    "    * Evaluation datasets should also be versioned to ensure consistency during testing.\n",
    "\n",
    "![A Git version control system diagram](https://placehold.co/600x400/aabbcc/ffffff?text=Git+Versioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CI/CD for LLM Applications: Automating Testing and Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CI/CD (Continuous Integration/Continuous Deployment)** is a set of practices aimed at automating software development stages, from code integration to deployment. For LLM applications, CI/CD helps ensure quality and deployment speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Continuous Integration (CI):**\n",
    "    * Every time code is committed to the repository, automated tests are run.\n",
    "    * **Unit Tests:** Test individual components (e.g., an information extraction function, a specific prompt).\n",
    "    * **Integration Tests:** Test the interaction between components (e.g., a complete LangChain chain).\n",
    "    * **LLM Regression Tests:** Run critical prompts on a small evaluation dataset and compare outputs with expected results or previous versions. **LangSmith** is very useful for this.\n",
    "    * **Code Quality Checks:** Linters, formatters.\n",
    "* **Continuous Deployment (CD):**\n",
    "    * After successful CI, code is automatically deployed to staging or production environments.\n",
    "    * **Staged Rollouts:** Deploy to a small group of users first (e.g., 5%) to monitor performance and detect issues early (similar to A/B testing).\n",
    "    * **Automated Rollback:** If monitoring metrics indicate performance degradation or errors, the system automatically reverts to the previous version.\n",
    "* **Common CI/CD Tools:**\n",
    "    * **GitHub Actions, GitLab CI/CD, Jenkins, CircleCI:** Platforms for automating CI/CD pipelines.\n",
    "\n",
    "![A CI/CD pipeline diagram](https://placehold.co/600x400/ccddeeff/ffffff?text=CI/CD+Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deployment on Kubernetes/Container Orchestration for Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LLM applications with high traffic or requiring dynamic scalability, **Kubernetes** or other container orchestration platforms are ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Containerization (Docker):**\n",
    "    * Package your LangChain application into a **Docker container**. This ensures the application runs consistently across any environment.\n",
    "    * Include all necessary dependencies and configurations.\n",
    "* **Kubernetes (K8s):**\n",
    "    * **Concept:** An open-source platform for automating the deployment, scaling, and management of containerized applications.\n",
    "    * **Benefits:**\n",
    "        * **Auto-scaling:** Automatically increases/decreases the number of application instances (pods) based on CPU load, memory, or custom metrics.\n",
    "        * **Self-healing:** Automatically restarts failed containers, replaces unresponsive containers.\n",
    "        * **Load Balancing:** Distributes traffic among instances.\n",
    "        * **Resource Management:** Ensures each container receives sufficient resources.\n",
    "        * **Flexible Deployment:** Supports Canary deployments, Blue/Green deployments.\n",
    "* **Similar Platforms:**\n",
    "    * **AWS ECS (Elastic Container Service), Google Kubernetes Engine (GKE), Azure Kubernetes Service (AKS):** Managed Kubernetes services on the cloud.\n",
    "\n",
    "![A Kubernetes cluster diagram](https://placehold.co/600x400/ddeeff/ffffff?text=Kubernetes+Cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Serverless Functions (AWS Lambda, Google Cloud Functions) for Asynchronous Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For asynchronous tasks, event processing, or irregular traffic, **Serverless functions** are a cost-effective and scalable option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Concept:** You only write code, and the cloud platform automatically manages the underlying infrastructure to run that code when triggered by an event (e.g., HTTP request, message in a queue, file upload).\n",
    "* **Benefits:**\n",
    "    * **Pay-per-use:** You only pay for the time your code actually runs.\n",
    "    * **Automatic Scaling:** Automatically scales from zero to thousands of instances.\n",
    "    * **Reduced Operational Overhead:** No servers to manage.\n",
    "* **Applications in LLM:**\n",
    "    * **Post-processing:** E.g., summarizing documents after upload.\n",
    "    * **Low-traffic Chatbots:** Handling chatbot requests when there isn't continuous traffic.\n",
    "    * **Asynchronous Processing:** Sending long LLM requests to a serverless function and receiving results later.\n",
    "* **Popular Platforms:**\n",
    "    * **AWS Lambda:** Amazon Web Services' serverless compute service.\n",
    "    * **Google Cloud Functions:** Google Cloud Platform's serverless compute service.\n",
    "    * **Azure Functions:** Microsoft Azure's serverless compute service.\n",
    "* **Challenges:** Runtime limits, memory limits, difficulty in managing long-lived state.\n",
    "\n",
    "![A serverless function icon triggering a cloud service](https://placehold.co/600x400/eeccaa/ffffff?text=Serverless+Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical: Preparing a LangChain Application for Deployment with Docker and Basic CI/CD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a simple LangChain application and prepare it for Docker packaging, and then outline the steps for a basic CI/CD pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:**\n",
    "* Create a `Dockerfile` for the LangChain application.\n",
    "* Create a `requirements.txt` file.\n",
    "* Outline a simple CI/CD pipeline (e.g., with GitHub Actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangChain Application (example from Lesson 11.1):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import time\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Initialize LLM\n",
    "llm_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def simple_qa_app(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Simple Q&A application.\n",
    "    \"\"\"\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a Q&A assistant. Answer the following question concisely and directly.\"),\n",
    "        (\"user\", question),\n",
    "    ])\n",
    "    chain = prompt_template | llm_model | StrOutputParser()\n",
    "    \n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Processing question: {question}\")\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Response: {response[:100]}...\")\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage of the application\n",
    "    print(\"Starting LangChain Q&A application...\")\n",
    "    simple_qa_app(\"What is the capital of Japan?\")\n",
    "    simple_qa_app(\"Who wrote 'War and Peace'?\")\n",
    "    print(\"LangChain application completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Create `requirements.txt` file**\n",
    "\n",
    "Create a file named `requirements.txt` in the same directory as `app.py` and add the necessary libraries:\n",
    "\n",
    "```\n",
    "langchain-openai==0.1.x # Use specific version\n",
    "langchain-core==0.1.x\n",
    "openai==1.x.x\n",
    "```\n",
    "*(Note: Replace `x` with the specific version numbers you are using for consistency.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Create `Dockerfile`**\n",
    "\n",
    "Create a file named `Dockerfile` (no extension) in the same directory as `app.py` and `requirements.txt`:\n",
    "\n",
    "```dockerfile\n",
    "# Use a basic Python image\n",
    "FROM python:3.9-slim-buster\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the requirements.txt file to the working directory\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy the application source code to the working directory\n",
    "COPY app.py .\n",
    "\n",
    "# Set environment variable for API key (DO NOT STORE KEYS DIRECTLY IN DOCKERFILE)\n",
    "# Instead, you will pass this variable when running the container or in the deployment environment\n",
    "# ENV OPENAI_API_KEY=\"your_api_key_here\" # For illustration only, not for production\n",
    "\n",
    "# Command to run the application when the container starts\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```\n",
    "\n",
    "**`Dockerfile` Explanation:**\n",
    "\n",
    "* `FROM python:3.9-slim-buster`: Selects a lightweight Python image to reduce container size.\n",
    "* `WORKDIR /app`: Sets the `/app` directory as the default working directory.\n",
    "* `COPY requirements.txt .`: Copies the `requirements.txt` file to the `/app` directory in the container.\n",
    "* `RUN pip install --no-cache-dir -r requirements.txt`: Installs Python libraries. `--no-cache-dir` helps reduce image size.\n",
    "* `COPY app.py .`: Copies your application's source code into the container.\n",
    "* `CMD [\"python\", \"app.py\"]`: The command that will be executed when the container starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Build and Run Docker Image**\n",
    "\n",
    "Open your terminal in the directory containing the above files and run:\n",
    "\n",
    "```bash\n",
    "# Build the Docker image (replace your-app-name with your application name)\n",
    "docker build -t your-app-name .\n",
    "\n",
    "# Run the Docker container\n",
    "# Pass the API key via environment variable when running the container\n",
    "docker run -e OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\" your-app-name\n",
    "```\n",
    "*(Replace `YOUR_OPENAI_API_KEY` with your actual API key.)*\n",
    "\n",
    "You will see your LangChain application running inside the Docker container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Outline Basic CI/CD Pipeline (e.g., GitHub Actions)**\n",
    "\n",
    "Create a file named `.github/workflows/main.yml` in your Git project:\n",
    "\n",
    "```yaml\n",
    "name: CI/CD for LangChain App\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches:\n",
    "      - main # Triggers on push to main branch\n",
    "  pull_request:\n",
    "    branches:\n",
    "      - main # Triggers on pull request to main branch\n",
    "\n",
    "jobs:\n",
    "  build-and-test:\n",
    "    runs-on: ubuntu-latest # Runs on the latest Ubuntu environment\n",
    "\n",
    "    steps:\n",
    "    - name: Checkout code\n",
    "      uses: actions/checkout@v3 # Checks out the code\n",
    "\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.9' # Uses Python version 3.9\n",
    "\n",
    "    - name: Install dependencies\n",
    "      run: pip install -r requirements.txt # Installs libraries\n",
    "\n",
    "    - name: Run unit tests (if any)\n",
    "      run: |\n",
    "        # Example: python -m unittest discover tests\n",
    "        echo \"Running unit tests...\" # Placeholder for your tests\n",
    "\n",
    "    - name: Build Docker image\n",
    "      run: docker build -t your-app-name . # Builds the Docker image\n",
    "\n",
    "    - name: Login to Docker Hub (optional, if you want to push the image to Docker Hub)\n",
    "      if: github.ref == 'refs/heads/main' # Only logs in on push to main\n",
    "      uses: docker/login-action@v2\n",
    "      with:\n",
    "        username: ${{ secrets.DOCKER_USERNAME }}\n",
    "        password: ${{ secrets.DOCKER_PASSWORD }}\n",
    "\n",
    "    - name: Push Docker image (optional)\n",
    "      if: github.ref == 'refs/heads/main'\n",
    "      run: docker push your-app-name # Pushes the image to Docker Hub\n",
    "\n",
    "  deploy:\n",
    "    needs: build-and-test # Ensures build-and-test job completes first\n",
    "    if: github.ref == 'refs/heads/main' # Only deploys on push to main branch\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    steps:\n",
    "    - name: Deploy to Cloud Platform (Placeholder)\n",
    "      run: |\n",
    "        echo \"Deploying application to production environment...\"\n",
    "        # Add your actual deployment commands here\n",
    "        # Example:\n",
    "        # - Deploy to AWS Lambda/Cloud Run: Use AWS CLI / gcloud CLI\n",
    "        # - Update deployment on Kubernetes: kubectl apply -f deployment.yaml\n",
    "        # Ensure secrets (API keys, credentials) are managed securely via GitHub Secrets\n",
    "        # Example: -e OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}\n",
    "```\n",
    "\n",
    "**CI/CD Pipeline Explanation:**\n",
    "\n",
    "* **`build-and-test` job:**\n",
    "    * Triggers on every `push` or `pull_request` to the `main` branch.\n",
    "    * Checks out the code, sets up Python, and installs dependencies.\n",
    "    * Runs tests (you'll need to write your actual tests).\n",
    "    * Builds the Docker image of the application.\n",
    "    * (Optional) Logs in and pushes the Docker image to Docker Hub or another registry.\n",
    "* **`deploy` job:**\n",
    "    * Only runs after the `build-and-test` job successfully completes.\n",
    "    * Only runs on direct pushes to the `main` branch (not from pull requests).\n",
    "    * This is where you'll add the actual commands to deploy your application to your cloud platform (e.g., AWS, GCP, Azure, Kubernetes).\n",
    "    * **Important:** API keys and other sensitive information must be securely stored in **GitHub Secrets** (or equivalent in other CI/CD platforms) and passed to deployment steps as environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson provided comprehensive knowledge on **version management and advanced deployment** for LLM applications. You learned about the importance of **versioning** for code, prompts, models, and data. We delved into **CI/CD** and how to automate testing and deployment to ensure quality and speed. You also explored powerful deployment strategies like **Kubernetes/Container Orchestration** for scalability and **Serverless functions** for asynchronous tasks. Through the practical exercise of preparing a LangChain application for Docker deployment and outlining a basic CI/CD pipeline, you gained practical experience in professionally bringing your LLM application to a production environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
