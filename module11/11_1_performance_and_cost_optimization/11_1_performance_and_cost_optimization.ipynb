{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 11.1: Performance and Cost Optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Large Language Model (LLM) applications move from development to production deployment, **performance** and **cost** become critically important factors. High latency can degrade user experience, while LLM API costs can skyrocket if not carefully managed. This lesson will delve into the factors affecting performance and cost, strategies to optimize them, and a practical exercise to apply these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Factors Affecting Performance (Latency) and Cost of LLM Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize, we first need to understand the main factors contributing to latency and cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Factors Affecting Latency\n",
    "\n",
    "* **Model size and architecture:** Larger LLMs (e.g., GPT-4) are generally slower than smaller models (e.g., GPT-3.5 Turbo, smaller fine-tuned models). Complex architectures also increase processing time.\n",
    "* **Prompt and response length:** The longer the prompt (more input tokens), the longer it takes for the LLM to process. The longer the response (more output tokens), the longer it takes to generate.\n",
    "* **Number of API calls:** Each LLM API call has network latency and server-side processing latency. Applications with many reasoning steps or multiple Tool calls (e.g., Agents, Prompt Chaining) will have higher overall latency.\n",
    "* **LLM server load:** When LLM servers are overloaded, response times can increase.\n",
    "* **Network latency:** The geographical distance between your application and the LLM API server.\n",
    "* **Application logic:** Processing time within other components of your application (e.g., RAG retrieval, data preprocessing, Agent logic).\n",
    "* **`temperature` parameter:** Higher `temperature` can slightly increase latency as the LLM needs to explore a wider response space.\n",
    "\n",
    "![A graph showing increasing latency and cost](https://placehold.co/600x400/ffccaa/ffffff?text=Latency+and+Cost+Factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2. Factors Affecting Cost\n",
    "\n",
    "* **Number of tokens used:** This is the most significant factor. Most LLM APIs charge based on the number of input tokens and output tokens.\n",
    "* **Model pricing:** More powerful and larger models generally have higher per-token prices.\n",
    "* **Number of API calls:** Although cost is primarily token-based, some providers might have a base cost per call.\n",
    "* **Error rate:** Failed API calls might still be charged, or at least waste resources.\n",
    "* **Usage of advanced features:** Some features like fine-tuning or specialized tools might have their own costs.\n",
    "\n",
    "![A graph showing increasing latency and cost](https://placehold.co/600x400/ffccaa/ffffff?text=Latency+and+Cost+Factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Strategies to Reduce Token Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the number of tokens processed by the LLM is the most effective way to cut costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Prompt Optimization:**\n",
    "    * **Conciseness:** Eliminate unnecessary words, provide clear but brief instructions.\n",
    "    * **Avoid repetition:** Ensure the prompt does not contain redundant information.\n",
    "    * **Context reuse:** If possible, design chains so that context only needs to be passed once or summarized before being fed into subsequent steps.\n",
    "    * **Effective Few-shot Prompting:** Provide just enough examples for the LLM to understand the task, not too many.\n",
    "* **Use Smaller Models When Appropriate:**\n",
    "    * For simpler tasks (e.g., basic classification, short summarization), a smaller and cheaper model (e.g., GPT-3.5 Turbo instead of GPT-4, or smaller open-source models) can provide sufficient performance at a significantly lower cost.\n",
    "    * Perform **evaluation** to determine the optimal model for each task.\n",
    "* **Data Compression:**\n",
    "    * **Context summarization:** Before feeding a long text to the LLM (especially in RAG), use a smaller LLM or a specialized summarization model to condense the context into key points. This reduces the number of input tokens.\n",
    "    * **Essential information extraction:** Instead of providing the entire document, extract only the truly necessary pieces of information for the LLM.\n",
    "* **Conversation History Management:**\n",
    "    * In chatbots, conversation history can grow very quickly, increasing costs.\n",
    "    * **Strategies:**\n",
    "        * **Sliding Window:** Keep only the N most recent messages.\n",
    "        * **History Summarization:** Periodically summarize older parts of the conversation history into a concise text and include it in the prompt instead of the full old messages.\n",
    "\n",
    "![A compressed data icon](https://placehold.co/600x400/ccffdd/ffffff?text=Data+Compression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Strategies to Reduce Latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing response time is key to improving user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Caching:**\n",
    "    * **Purpose:** Store LLM responses for previously encountered prompts. If the same prompt appears again, the response is returned from the cache instead of calling the LLM API.\n",
    "    * **Cache types:**\n",
    "        * **In-memory cache:** Simple, fast, but not persistent and cannot be shared across instances.\n",
    "        * **Distributed cache (Redis, Memcached):** Suitable for production environments, can be shared across multiple instances.\n",
    "    * **When to use:** For frequently repeated questions or prompts.\n",
    "    * **Challenges:** Cache invalidation management, handling highly stochastic prompts.\n",
    "* **Parallel Processing:**\n",
    "    * If your application needs to make multiple independent LLM or Tool calls, perform them concurrently instead of sequentially.\n",
    "    * **In Python:** Use `asyncio` and `await` for asynchronous API calls, or `ThreadPoolExecutor` for I/O-bound tasks.\n",
    "    * **Example:** In RAG, multiple documents can be retrieved in parallel. In Agents, if the LLM decides to call multiple Tools, they can be run concurrently.\n",
    "* **Optimize API calls:**\n",
    "    * **Batching:** If you have many small requests, group them into a larger request (if the API supports it) to reduce the overhead of multiple calls.\n",
    "    * **Streaming:** Instead of waiting for the entire response to be generated, LLM APIs can send responses in parts (token-by-token). This improves the perceived latency for users, although the total time might not change significantly.\n",
    "* **Model Selection:**\n",
    "    * As mentioned, smaller models are generally faster.\n",
    "    * LLM providers often have \"turbo\" or \"fast\" versions optimized for speed.\n",
    "* **Optimize Application Logic:**\n",
    "    * Ensure your code is efficient, especially parts that preprocess and post-process data around LLM calls.\n",
    "    * Minimize unnecessary steps in LangChain chains or LangGraph graphs.\n",
    "\n",
    "![A fast-loading bar with a cache icon](https://placehold.co/600x400/ddeeff/ffffff?text=Latency+Reduction+Strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Balancing and Resource Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large-scale LLM applications, infrastructure management is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Load Balancing:**\n",
    "    * **Purpose:** Distribute requests across multiple instances of your LLM application or multiple API endpoints to prevent overload and improve responsiveness.\n",
    "    * **Implementation:** Use cloud load balancing services (e.g., AWS ELB, GCP Load Balancer) or tools like Nginx.\n",
    "* **Resource Management:**\n",
    "    * **Auto-scaling:** Automatically increase or decrease the number of application instances based on load to meet demand without wasting resources.\n",
    "    * **Resource Monitoring:** Track CPU, RAM, network bandwidth to ensure instances are not overloaded.\n",
    "    * **GPU Usage (if self-hosting):** For large models, GPUs are necessary to achieve acceptable performance.\n",
    "\n",
    "![A load balancer distributing requests to multiple servers](https://placehold.co/600x400/aaccaa/ffffff?text=Load+Balancing+and+Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical: Applying Optimization Techniques to a LangChain Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a simple Q&A application and apply optimization techniques: **prompt optimization** to reduce tokens and **caching** to reduce latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation:**\n",
    "* Ensure you have the `langchain-openai` library installed.\n",
    "* Set the `OPENAI_API_KEY` environment variable.\n",
    "* To measure time, we will use the `time` library.\n",
    "* For caching, we will use a simple in-memory `dict` as a cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "# pip install langchain-openai openai\n",
    "\n",
    "import os\n",
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Set environment variable for OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# --- Initialize LLM ---\n",
    "# Use an LLM with low temperature for more consistent results for caching\n",
    "llm_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# --- 1. Basic Q&A Application (Baseline) ---\n",
    "print(\"--- 1. Basic Q&A Application (Baseline) ---\")\n",
    "\n",
    "def run_basic_qa(question: str) -> str:\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a Q&A assistant. Answer the following question fully and in detail.\"),\n",
    "        (\"user\", question),\n",
    "    ])\n",
    "    chain = prompt_template | llm_model | StrOutputParser()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"  Question: {question}\")\n",
    "    print(f\"  Response: {response[:100]}...\") # Print only part of the response\n",
    "    print(f\"  Response Time: {end_time - start_time:.4f} seconds\")\n",
    "    return response\n",
    "\n",
    "# Run Baseline tests\n",
    "print(\"Running Baseline 1:\")\n",
    "run_basic_qa(\"What is the capital of France?\")\n",
    "print(\"Running Baseline 2 (same question):\")\n",
    "run_basic_qa(\"What is the capital of France?\")\n",
    "print(\"Running Baseline 3 (new question):\")\n",
    "run_basic_qa(\"Who invented the light bulb?\")\n",
    "\n",
    "# --- 2. Prompt Optimization (Reduce input tokens) ---\n",
    "print(\"\\n--- 2. Prompt Optimization (Reduce input tokens) ---\")\n",
    "\n",
    "def run_optimized_prompt_qa(question: str) -> str:\n",
    "    # More concise prompt, focused on direct answering\n",
    "    optimized_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Answer the question directly.\"),\n",
    "        (\"user\", question),\n",
    "    ])\n",
    "    chain = optimized_prompt_template | llm_model | StrOutputParser()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"  Question: {question}\")\n",
    "    print(f\"  Response: {response[:100]}...\")\n",
    "    print(f\"  Response Time: {end_time - start_time:.4f} seconds\")\n",
    "    # To check token cost, you would need to integrate with LangSmith or the LLM provider's API\n",
    "    # LangChain has callbacks to measure token usage, but for simplicity, we're focusing on latency here.\n",
    "    return response\n",
    "\n",
    "print(\"Running Optimized Prompt 1:\")\n",
    "run_optimized_prompt_qa(\"What is the capital of France?\")\n",
    "print(\"Running Optimized Prompt 2:\")\n",
    "run_optimized_prompt_qa(\"Who invented the light bulb?\")\n",
    "\n",
    "# --- 3. Caching (Reduce latency for repeated questions) ---\n",
    "print(\"\\n--- 3. Caching (Reduce latency for repeated questions) ---\")\n",
    "\n",
    "# Simple in-memory cache\n",
    "qa_cache = {}\n",
    "\n",
    "def run_cached_qa(question: str) -> str:\n",
    "    if question in qa_cache:\n",
    "        print(f\"  Question: {question} (From cache)\")\n",
    "        start_time = time.time()\n",
    "        response = qa_cache[question]\n",
    "        end_time = time.time()\n",
    "        print(f\"  Response: {response[:100]}...\")\n",
    "        print(f\"  Response Time (from cache): {end_time - start_time:.4f} seconds\")\n",
    "        return response\n",
    "    else:\n",
    "        print(f\"  Question: {question} (Calling LLM)\")\n",
    "        # Use the optimized prompt for the LLM call\n",
    "        optimized_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"Answer the question directly.\"),\n",
    "            (\"user\", question),\n",
    "        ])\n",
    "        chain = optimized_prompt_template | llm_model | StrOutputParser()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = chain.invoke({\"question\": question})\n",
    "        end_time = time.time()\n",
    "        \n",
    "        qa_cache[question] = response # Store in cache\n",
    "        \n",
    "        print(f\"  Response: {response[:100]}...\")\n",
    "        print(f\"  Response Time (calling LLM): {end_time - start_time:.4f} seconds\")\n",
    "        return response\n",
    "\n",
    "print(\"Running Cached QA 1 (new question):\")\n",
    "run_cached_qa(\"What is the capital of Germany?\")\n",
    "print(\"Running Cached QA 2 (same question):\")\n",
    "run_cached_qa(\"What is the capital of Germany?\")\n",
    "print(\"Running Cached QA 3 (new question):\")\n",
    "run_cached_qa(\"How tall is Mount Everest?\")\n",
    "print(\"Running Cached QA 4 (same as question 1):\")\n",
    "run_cached_qa(\"What is the capital of Germany?\")\n",
    "\n",
    "print(\"--- End of Practical ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
